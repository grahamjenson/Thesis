
\chapter{Experiments, Results and Analysis}
\label{experiments}
\epigraph{Experiment is the sole judge of scientific ``truth''}
{\textit{The Feynman Lectures on Physics, Introduction, Richard Feynman, 1961.}}
Previously the models and the implementation used to simulate CSE have been described.
The \usermodel model is used to create CUDF* documents given the probabilities a user will request to upgrade and install components, 
and the criteria used to accomplish these requests.
These documents describe the evolution of an Ubuntu system for a year starting on October 30th 2009.
Given such a document, the implementation GJSolver can be used to resolve the exact evolution of the system.
This process of describing a user in \usermodel, generating CUDF* documents, then resolving the evolution is used to explore CSE.

This chapter presents experiments and results to study the effects of CSE.
The specific effects that are focused on are the change made to the system, and the out-of-dateness of the system during evolution.
This exploration is accomplished through trying to answer the questions:
\begin{enumerate}
  \item What consequences do a user's choices (their probabilities to upgrade $u$ and install $i$) have on their system?
  \item Can the out-of-dateness of a system be reduced?
  \item Can the change of a system be reduced?
  \item How do the systems of realistic users evolve?
\end{enumerate}
This chapter presents experiments intended to answer these questions. 

\section{Users Choices to Upgrade and Install}
\label{exp.q1}
How often a user decides to upgrade their system or install a component will effect how the system evolves.
This section attempts to quantify the effects on change and out-of-dateness that these actions have on the users systems.

These experiments simulates users altering the probabilities a user will install a component $i$ and upgrade their system $u$,.
The criteria to install $I$ and upgrade $U$ are defaulted to the ones used by \texttt{apt-get}. 
\texttt{apt-get}'s $U$ criteria is \texttt{-removed,-new,-uptodatedistance}, and $I$ criteria is \texttt{-ovpp,-removed,-changed,-uptodatedistance} as described in section \ref{impl.validation} 

\subsection{Boundary Cases}
The first experiments assign values to the variables in \usermodel that describe the boundary cases for $i$ and $u$:
\begin{table}[h!]
\centering
\begin{tabular}{|l | c | c | c |}
\hline
User Name 				 	& \# simulated 	& $u$ 		& $i$ 			\\ \hline
Always Upgrade				& 1 			& 1			& 0				 \\
Control						& 1 			& 0			& 0				\\
Always Install 				& 30 			& 0			& 1				 \\
Always Upgrade\&Install 	& 30 			& 1			& 1				\\ \hline
\end{tabular}
\caption{Users that are the boundary cases in the simulation.}
\label{exp.tblextremeusers}
\end{table}
This table also states the number of simulations that were run for each user, i.e. the ``Always Install'' user had 30 CUDF* documents created and resolved.
The number of simulations run for a user is a trade-off between time to complete the simulations and accuracy of results.
The number of simulations run is based on the randomness of the simulated users (as described in section \ref{sim.randomness}) 
as well as the experience gained when developing the simulation as to the variability of the results.

\subsubsection{Results and Analysis}
First, how out-of-date these simulated systems become will be explored.
This out-of-dateness is measured using the function up-to-date distance (UTTD) function, as described in section \ref{impl.criteria}.
This measurement is the number of components that are a greater version than a component currently installed.
A problem that exists with this measurement is that it does not take into account the size of a system.
As a system grow the UTTD will grow as there will be more components that become out of date.
To normalise this effect the measurement UTTD per component (UTTDpC) is defined as the UTTD per installed component.
The UTTDpC of each simulated system is presented in figure \ref{exp.q1auttdpc}.
\begin{figure}[htp]
\begin{center}
  \includegraphics[width=\textwidth]{plots/q1auttdperc}
  \caption{The Up-to-date distance per Component (UTTDpC) of the simulated systems.}
  \label{exp.q1auttdpc}
\end{center}
\end{figure}

This figure shows that all systems eventually become out-of-date likely due to the \texttt{apt-get} upgrade criteria and constraints in core components restricting change.
The \texttt{apt-get} criteria minimises new components installed into the system before minimising the UTTD.
Therefore, if a newer version of a component require a new component to be installed, this component will not be upgraded when using this criteria.
Section \ref{exp.prouttdsection} presents experiments where changing this criteria removes this barrier to new components. 

This figure also shows that users which do not upgrade, the ``Control'' and ``Always Install'' users, become nearly three times more out-of-date than users that upgrade.
The speed at which systems become out-of-date increases over the months between March and May 2010.
The reason for this is likely due to the increased development effort because of the Ubuntu 10.04 release in April 2010.
The users that upgrade are less affected by this release as they are continually upgrading to the new component versions.

The ``Control'' users UTTDpC curve can be used as a measure of the speed at which components evolve.
This is because the faster versions of a component are released, the more the ``Control'' user becomes out-of-date.

The effect of the normalisation can also be seen in this figure, where the install variable $i$ has little effect on the UTTDpC of a component system.

Second, how much change each simulated system went through during evolution is measured.
To measure change the ``change'' function as described in section \ref{impl.criteria} is used.
The total change, i.e. the sum of all change a system has to a date, is presented in figure \ref{exp.q1achange}
\begin{figure}[htp]
\begin{center}
  \includegraphics[width=\textwidth]{plots/q1achange}
  \caption{The total change of the simulated systems.}
  \label{exp.q1achange}
\end{center}
\end{figure}

As was predicted above, this figure shows that there is an increase in change during the release of Ubuntu 10.04 for users that upgrade their systems.
During the months of March, April and May of 2010 had a mean of 266 changes per month for the ``Always Upgrade'' user.
All other months had had a mean of 105 changes per month, showing a more than 250\% increase in change over the release months.
This is likely because the increased development of components leads to an increased release of versions that have to be upgraded.
This increased change cannot be seen in the ``Always Install'' users as they will not change in response to newer versions.

The ``Always Install'' users start by changing their systems quickly, then after a month this change slows to a constant rate.
This can be seen with the mean change per day during the first month it is 4.9, where the final 11 months it is 3.0.
After further investigation this reduction in change is explained due to the common reuse of individual components like \texttt{libaudio2} and \texttt{libqt3-mt}.
Many components require such components to be installed which increases their likely hood of being installed in the first month.
After this installation they are no longer required to be added so the future change is reduced.
With no reuse in this system the rate of change would remain at 4.9 per day, resulting in a total change of 1788.5.
This is an addition of more than 650 change over the year.

The ``Always Upgrade\&Install'' users changes more than the combined amount of ``Always Upgrade'' and ``Always Install'' changes.
The mean total change of the ``Always Upgrade'' user is 1655, the ``Always Install'' user is 1137 and the ``Always Upgrade\&Install'' user is 3482.
This shows that a user that upgrades and installs changes more than 700 components than the combined amount changed by just installing and just upgrading.
The additional change is due to the installed components increasing the amount of components to be upgraded.


\subsection{Failures}
These initial experiments are the most extreme users that can be simulated.
This means phenomena that occur rarely are most observable in these simulations.
For this reason, these simulations were chosen to be closely inspected for various failures.

Two types of failure were observed in the results of these simulations:
\begin{enumerate}
  \item \textbf{Hard Failure}: Where a request has no satisfactory solution, therefore no change is made to the system.
  \item \textbf{Soft Failure}: The request is satisfied, however the search was interrupted and the best solution found at that time was returned.
\end{enumerate}
Additionally, a notable type of hard failure was observed: the \textbf{Multi-component Failure}, where multiple components must be installed to satisfy a request. 

%%%Hard failures
Hard failures where observed to occur in 24 of the 60 ``Always Install'' and ``Always Upgrade\&Install'' users, and never in the ``Always Upgrade'' user.
Only 40 total failed requests occurred over these 24 simulated users.
All these hard failures, except those that are multi-component failures, were failed install requests.
Given each ``Always Install'' user has 365 requests and each ``Always Upgrade\&Install" user has 730 requests, 
and 30 users of each type were simulated, the chance for a request to fail is just over 0.12\%.
This means the chance for a hard failure to occur is extremely low in this simulation.

Due to the difficultly of finding the constraints that caused these failures \citep{quickxplain},
and the rarity of these failures, the specific reasons for hard failures are not further explored.

%%%Soft Failure TODO
Soft failures are called ``soft'' because the request that causes them succeeds, however the search failed to finish.
This does not mean that the returned solution is not optimal\footnote{In MISC GJSolver was interrupted many times and most times returned the optimal solution},
it just means that it is uncertain that it is optimal.
Of the 33215 requests during the simulation of the users defined in table \ref{exp.tblextremeusers} only 900 requests were interrupted.
This is less than 3\%.
After looking at the results returned by the 900 requests only 8 were determined to be detrimental to the simulation.
Each of these 8 requests caused more than 100 components in the system to be removed in order to be satisfied.
This is due to the criterion \texttt{-removed} not being fully optimised before being interrupted.
There are three interesting points that were observed with these 8 requests:
\begin{enumerate}
  \item seven of them occurred during the Ubuntu 10.04 release month of April 2010, where the remainder of the soft failures are evenly distributed.
  \item five times the component that was requested to be installed was removed by the following requests.
\end{enumerate}
The first point implies that these detrimental soft-failures occur because of the increased release of components during the Ubuntu release.
The second point implies that the components themselves are over constrained and thus removed in future systems.
The effect of these detrimental soft failures can be seen in figure \ref{exp.q1achange} 
where the standard deviation of change for both ``Always Install'' and ``Always Upgrade\&Install'' users increases after the Ubuntu 10.04 release.
Given that only 8 such failures occurred, this is seen as a minor impact on the validity of the simulation..
Although, in future experiments if soft failures are detected they will be mentioned. 
These failures may be reduced or removed by increasing the time before the algorithm is interrupted, or by using more powerful hardware to perform the simulations.

%%%The multi component fialure
An interesting failure occurred when the situation arose that installing the package \texttt{chromium-browser} required two versions of the package \texttt{libc6} to be installed. 
This lead to the following upgrade request to be unsatisfiable, and the following install request to remove \texttt{chromium-browser} and upgrade to the newest version of \texttt{libc6}.
This instance, although rare, shows the case that at some points the hard constraint that Debian enforces to have only one version of each component installed,
can restrict the user.

%%%Disclaimer
All these failures are directly caused by, or are a result of an install request.
The failures and/or the rates they occur may directly be a product of the selection of components being the least valid part of the simulation (discussed in section \ref{sim.modelvalidation}).

\subsection{Upgrade Probability Effects}
By altering only the probability a user upgrades their system, while leaving other variables static, the effect of the user upgrading can be measured.
The users for simulated are:
\begin{table}[h!]
\centering
\begin{tabular}{|l | c | c | c |}
\hline
User Name 				& \# simulated 	& $u$ 		& $i$ 			\\ \hline
Upgrade once a month	& 10 			& 0.03 (1/30)			& 0				 \\
Upgrade twice a month	& 10 			& 0.06 (1/15)		& 0				\\
Upgrade once a week		& 10 			& 0.14 (1/7)		& 0				 \\
Upgrade twice a week 	& 10 			& 0.29 (1/3.5)		& 0				\\ \hline
\end{tabular}
\caption{Users with different upgrade probabilities.}
\label{exp.tblextremeusers}
\end{table}
These users are compared to the ``Always Upgrade'' user described in table \ref{exp.tblextremeusers}.

The up-to-date distance per component for these users is compared in figure \ref{exp.q1buttdperc}.
\begin{figure}[htp]
\begin{center}
  \includegraphics[width=\textwidth]{plots/q1buttdperc}
  \caption{The Up-to-date distance per Component (UTTDpC) of the simulated systems.}
  \label{exp.q1buttdperc}
\end{center}
\end{figure}

This figure shows that upgrading less frequently has it's greatest effect over the release of the Ubuntu 10.04.
This is most noticeable in the ``Upgrade once a month'' users.
This is likely because the increased release of new versions during the April release.
If the user does not upgrade often during this time their system will become quickly out of date.

Another result from this experiment is showing the correlation between UTTDpC and the frequency of upgrading.
This can be seen in figure \ref{exp.q1bcorrolationuttdpc} that compares the mean UTTDpC to $1/u$.

\begin{figure}[htp]
\begin{center}
  \includegraphics[width=\textwidth]{plots/q1bcorrolationuttdpc}
  \caption{The mean up-to-date distance per component (UTTDpC) plotted against of the reciprocal of the users upgrade probability ($u$)}
  \label{exp.q1bcorrolationuttdpc}
\end{center}
\end{figure}

The figure shows the mean UTTDpC of ``Always Upgrade'' is 0.258 and the ``Upgrade twice a week'' users mean is 0.267.
This is a difference of 0.009 UTTDpC.
This difference can be illustrated in an example where given a system has 1000 components, 
if a user upgraded twice a week they will have on average 9 components more out-of-date than if they upgraded every day. 
This figure also shows the diminishing return when increasing the amount a user upgrades.

The total change of these users is presented in figure \ref{exp.q1bchange}.

\begin{figure}[htp]
\begin{center}
  \includegraphics[width=\textwidth]{plots/q1bchange}
  \caption{The mean total change plotted against of the simulated users.}
  \label{exp.q1bchange}
\end{center}
\end{figure}

This figure shows that upgrading is causes significantly more change in the system than installing.
For example, the total change of the ``Upgrade once a month" user has a greater total change than the ``Always Install'' user.

This figure also shows that upgrading less frequently requires less change. 
This is in part due to the fact that updating once a month means that the majority of the month there is no change.
Under further analysis another effect was noticed that reduced change of the users that upgraded less frequently.
This is where many versions of a component may be released in quick succession, making upgrading to each interim version not required.
For example, if over a week a user upgrades their system every day and upgrades 5 different components twice (a total of 10 change), 
then they will upgrade twice as much as a user that only upgraded at the end of the week.
This is because the user that upgrades every day changes to all versions of the components, where the other only changes to the final version.

An example of this rapid release of components happening in the Ubuntu repository is given in figures \ref{exp.apachelog} and \ref{exp.apachebug}.

\begin{figure}[htp]
\begin{center}
\begin{alltt}
apache2 (2.2.20-1) unstable; urgency=low

  * New upstream release.
  * Fix some regressions related to Range requests caused by the CVE-2011-3192
    fix. Closes: #639825
\ldots

 -- Stefan Fritsch <sf@debian.org>  Sun, 04 Sep 2011 21:50:22 +0200

apache2 (2.2.19-2) unstable; urgency=high

\ldots

 -- Stefan Fritsch <sf@debian.org>  Mon, 29 Aug 2011 17:08:17 +0200
\end{alltt}
\caption[Apache Changelog]{An extract from the apache changelog located on http://changelogs.ubuntu.com/}
\label{exp.apachelog}
\end{center}
\end{figure}

\begin{figure}[htp]
\begin{center}
\begin{alltt}

Reported by: Takis Issaris <takis.issaris@uhasselt.be>
Date: Tue, 30 Aug 2011 16:09:01 UTC
Severity: important
Found in versions 2.2.9-10+lenny10, 2.2.16-6+squeeze2, apache2/2.2.19-2

\ldots

Package: apache2.2-common
Version: 2.2.9-10+lenny10

Yesterday evenings update broke our Apache server setup,
\ldots
\end{alltt}
\caption[Apache Bug Report]{Extract from the bug report \#639825, filed with Debian}
\label{exp.apachebug}
\end{center}
\end{figure}

A summary of these events are:
\begin{enumerate}
  \item Apache developer Stefan Fritsch released a new version of their server, \texttt{apache2}/2.2.19-2, on 29 Aug 2011.
  \item Takis Issaris upgraded to this version which broke his system on 29 Aug 2011.
  \item Takis Issaris submited a bug report on 30 Aug 2011, where he and Stefan Fritsch discuss the causes.
  \item On 04 Sep 2011 (5 days after the initial bug report) Stefan Fritsch releases a new version 2.2.20-1 that fixes this bug.
\end{enumerate}
In this example, the maintenance of the component \texttt{apache2} caused the release of two versions within a week of one another.

The amount of change this effect causes can be found out by looking at the ``Always Upgrade'' user.
This user upgraded the same component within seven days on 23 different occasions.
This may not seem like an insignificant amount of change, especially since the ``Always Upgrade'' user changes over 1500 components during the year.
However, these changes may introduce bugs (as with the above example) and are an unnecessary risk to the users system.
Additionally, as a user installs more components this will likely increase the amount this type of change occurs.

By updating less frequently the change caused by rapid releasing the components is reduced.
However, this reduction is by chance alone.
For example, if a user upgrade once a month then they will likely miss many of the rapid releases that happened during the month.
Such a user may upgrade during such a release, and therefore have a component (potentially with bugs) until they upgrade again in a month.

To remove the reduction of this change without updating less frequently and resorting to chance a novel criterion is presented in section \ref{exp.q3}.
This criterion is explicitly defined take advantage of this phenomena to reduce change during CSE.


\subsection{Install Probability Effects}
By altering the probability a user installs a component into their system, the effect this has on 
change\footnote{As discussed before the probability to install will have little effect on UTTDpC} can be studied. 
\begin{table}[h!]
\centering
\begin{tabular}{|l | c | c | c |}
\hline
User Name 				& \# simulated 	& $u$ 		& $i$ 			\\ \hline
Install once a month	& 30 			& 0 & 0.03 (1/30)							 \\
Install twice a month	& 30 			& 0 & 0.06 (1/15)						\\
Install once a week		& 30 			& 0 & 0.14 (1/7)					 \\
Install twice a week 	& 30 			& 0 & 0.29 (1/3.5)						\\ \hline
\end{tabular}
\caption{Users with different upgrade probabilities.}
\label{exp.tblextremeusers}
\end{table}
These users are compared to the ``Always Install'' users described in table \ref{exp.tblextremeusers}.

The total change of these users is presented in figure \ref{exp.q1cchange}.
\begin{figure}[htp]
\begin{center}
  \includegraphics[width=\textwidth]{plots/q1cchange}
  \caption{The mean total change plotted against of the simulated users.}
  \label{exp.q1cchange}
\end{center}
\end{figure}
This figure shows the range of possible change given the probability to install a component.
It also shows the change is inversely proportional to the frequency of installing, 
i.e. ``Install twice a week'' users change twice as much as ``Install once a week'' users.

Further analysis is futile is that the selection of components is the least valid part of the simulation, as discussed in section \ref{sim.randomness}.
 

\section{Reduction of Out-of-dateness During Evolution}
\label{exp.q2}
\label{exp.prouttdsection}
In the previous section the criteria used during evolution were the criteria used by \texttt{apt-get}.
This criteria limits the up-to-dateness of the system by prioritising the minimisation of installing new components.
Altering this criteria from \texttt{-removed,-new,-uptodatedistance} to \texttt{-removed,-uptodatedistance,-new} 
will let progressive users, who prioritize up-to-dateness over change, upgrade their system.

\begin{table}[h!]
\centering
\begin{tabular}{|l | c | c | c |}
\hline
User Name 								& \# simulated 			& $u$ 		& $i$ 			\\ \hline
Progressive Always Upgrade				& 1 			 	& 1				& 0\\ \hline
\end{tabular}
\caption{Progressive Users with different upgrade probabilities.}
\label{exp.tblextremeusers}
\end{table}
This user has the upgrade criteria $U$ assigned to \texttt{-removed,-uptodatedistance,-new}.
It is compared to the ``Always Upgrade'' user which is the same except it's upgrade criteria $U$ is assigned \texttt{-removed,-new,-uptodatedistance}.

Figure \ref{exp.q4auttdperc} presents the UTTDpC of these users.
\begin{figure}[htp]
\begin{center}
  \includegraphics[width=\textwidth]{plots/q4auttdperc}
  \caption{The UTTDpC plotted for the simulated users.}
  \label{exp.q4auttdperc}
\end{center}
\end{figure}

This figure shows that the progressive criteria reduces the out-of-dateness of the systems.
The final UTTDpC for the ``Always Upgrade'' user 0.72 compared to the final UTTDpC for the ``Progressive Always Upgrade'' user 0.48.
That is, a user who upgrades with the progressive criteira will have a system that is 24\% less out-of-date.

The UTTDpC of these systems is most differentiated during the release of Ubuntu 10.04.
At this time new versions of components are being released quickly with   

These are promising results, showing that it is possible to reduce the out-of-dateness of a system.
However, this comes at the cost of increased change.
This additional change comes from the 90 new components that the progressive change installs.
These new components also are upgraded throughout the year which is 258 change.
The total additional change when using a progressive user is 348 over a year.  

When a user upgrades their system with the progressive upgrade criteria at the end of the year there system is 24\% less out-of-date at the cost of an additional 258 changes.
Users that prefer to have an up-to-date system may favour this criteria over the \texttt{apt-get} criteria. 
This criteria is further simulated in section \ref{exp.realuserssim}.

\section{Reduction of Change During Evolution}
\label{exp.q3}
The phenomenon where components are rapidly released causes unnecessary change when upgrading.
To reduce this change a criterion is defined that does not upgrade to a component until it has become \textit{stable}.
A component is stable if no better version is released within a certain amount of time after its release.
To express this the $stable$ function is defined:
\begin{defs}
The function $stable$ takes a component $c$, a number of days $d$, and a time $t$ and returns \texttt{true} iff:
\begin{enumerate}
  \item the component was released within $d$ days of time $t$.
  \item no component was released within $d$ days after $c$ was released that has the same name and a greater version than $c$.
\end{enumerate}
\end{defs}
The first part of the definition ensures that the function waits $d$ days before returning that the component is stable.
The second part ensures that if a better version is released within $d$ days, the component is returned as not stable.

A criterion using this function can then be defined:
\begin{defs}
	Given the set of components $\mathbb{C}_t$ and number of days $d$, the \textbf{stable version} criterion is defined as $crit_{sv} = \langle rank^{sv}_{\alpha}, \geq \rangle$,
	where \\$rank^{sv}_{\alpha}(\beta) = \sum_{c \in \beta} \begin{cases} 1000 + uptodatedistance(c,\mathbb{C}_t)& not stable(c,d,t) \\ uptodatedistance(c,\mathbb{C}_t) & \text{otherwise} \\  \end{cases}$
\end{defs}
That is the ranking function $rank^{sv}_{\alpha}(\beta)$ returns the UTTD of the component if it is stable,
else it returns 1000 added to the UTTD.
This additional weight of 1000 is added to encourage the system to only install stable components.

The mapping between this criterion, MOF and the PB criteria is presented in table \ref{exp.stablcritmapping}, with a full description in appendix \ref{apx.critmapping}.
\begin{table}[htp]
\begin{tabular}{c | c | c}
\textbf{MOF} 		& \textbf{\modelname criterion} & \textbf{PB criterion} \\
\texttt{-stableversion(}$d$\texttt{)} & $crit_{sv} = \langle rank^{sv}_{\alpha}, \geq \rangle$ & $\langle f_{sv}, <, I_{sv} \rangle$ \\
\end{tabular}
\caption{Mapping between these elements}
\label{exp.stablcritmapping}
\end{table}
In this mapping the amount of days $d$ is left blank, allowing a range of different values to be simulated.

The users simulated are described in table \ref{exp.tblsvusers}.
\begin{table}[h!]
\centering
\begin{tabular}{|l | c | c | c | }
\hline
User Name 				& \# simulated 	& $u$ 	& $U$ 			\\ \hline
7 Days SV Upgrade		& 1 			& 1 & 	\texttt{-removed,-new,-stableversion(7)}			 \\
14 Days SV Upgrade		& 1 			& 1 & 	\texttt{-removed,-new,-stableversion(14)}			\\
21 Days SV Upgrade		& 1 			& 1 & 	\texttt{-removed,-new,-stableversion(21)}			 \\
28 Days SV Upgrade 		& 1 			& 1 & 	\texttt{-removed,-new,-stableversion(28)}			\\ \hline
\end{tabular}
\caption{Users with different upgrade probabilities.}
\label{exp.tblsvusers}
\end{table}
These users are compared to the ``Always Upgrade'' user described in table \ref{exp.tblextremeusers}.

The UTTDpC of these simulated users are presented in figure \ref{exp.q5auttdperc}.
\begin{figure}[htp]
\begin{center}
  \includegraphics[width=\textwidth]{plots/q5auttdperc}
  \caption{The UTTDpC plotted for the simulated users.}
  \label{exp.q5auttdperc}
\end{center}
\end{figure}

This figure shows the cost of using this criteria which is that the system is $d$ days out-of-date while the criterion waits for the components to stabilise.
The difference between the mean UTTDpC of the ``Always Upgrade'' user and the user ``7 Days SV Upgrade" is 0.025 UTTDpC.
This increases linearly with the other users; ``14 Days SV Upgrade'' is 0.05, ``21 Days SV Upgrade'' is 0.078, and ``28 Days SV Upgrade'' is 0.104.
The cost of waiting for the components to become stable is 0.0036 UTTDpC per day.

The reduction in change for these users are described in table \ref{exp.tblsvchange}.
This is measured by subtracting the total change of the ``Always Upgrade'' user $d$ days before the end of the simulation
from the total change of the  ``$d$ Days SV Upgrade'' user.
This calculation takes into account the delay in upgrading caused when waiting for components to become stable.
This table also includes the estimate from the ``Always Upgrade'' user.
This is a total of the amount of occurrences the ``Always Upgrade" user upgraded the same component within $d$ days.
\begin{table}[h!]
\centering
\begin{tabular}{|l | c | c | }
\hline
User Name 				& Reduction in change 	& ``Always Upgrade'' estimate		\\ \hline
7 Days SV Upgrade		& 23 					& 23 			 \\
14 Days SV Upgrade		& 29	 				& 31			\\
21 Days SV Upgrade		& 41 					& 46 			 \\
28 Days SV Upgrade 		& 59 					& 68			\\ \hline
\end{tabular}
\caption{Users with different upgrade probabilities.}
\label{exp.tblsvusers}
\end{table}

This table shows that the reduction in change grows quickly.
However, increasing the amount of days that is waited for a component to become stable more than a month may start to effect regular patterns of component evolution.
This is a trade off when using this criteria; increasing the time to stabilise significantly reduces change, however it may be detrimental to the systems normal evolution.

Another aspect this table shows is the estimate of this change from the ``Always Upgrade'' user.
This estimate differs from the actual reduction due to the stable version criterion because of components waiting to become stable at the end of the simulation.
This estimate is later used to determine the amount of change that could be reduced for a simulated user.
This is preferred over further simulation with the stable version criterion as the change is to small to be measured against the background noise of the randomness of a simulation. 

\section{Realistic Evolution}
\label{exp.realuserssim}
During the previous experiments the probabilities a user upgrades and installs have been assigned values that may not be ``realistic''.
This section presents the experiment where these variables are assigned values extracted from submitted user's \texttt{apt-get} logs.
Four such users are defined, ``High Install'', ``High Upgrade'', ``Medium Change'' and ``Low change''.
These users are then assigned the \texttt{apt-get} criteria, and the ``Progressive Upgrade'' criteria and simulated.
Using the estimate described in the previous section, the potential reduction in change for these users if they used the stable version criterion is discussed.

This section first discusses the method used to extract the information from the \texttt{apt-get} logs and define the users.
The results from the simulation of these users is then discussed and analysed.

\subsection{Extracting Information from the User Submitted Logs}
As previously discussed in chapter \ref{simulation}, during the conducted survey users where asked to submit their \texttt{apt-get} logs.
Nineteen logs were submitted of users using \texttt{apt-get} on either Debian or Ubuntu systems over a period between 23 and 277 days long. 
In section \ref{impl.validation} these logs were used to validate the simulations output.
This section describes how these logs are parsed to measure the probability a user upgrades and installs a component.
Through using the k-means clustering algorithm, four general users are extracted and described.

To give an example of the information included in an \texttt{apt-get} log, an extract is shown in figure \ref{aptlog}.
\begin{figure}[htp]
\begin{center}
\begin{alltt}
\ldots
Start-Date: 2010-12-21 11:32:28
Install: libnet-daemon-perl (0.43-1), libhtml-template-perl (2.9-1), libdbi-perl (1.609-1build1), mysql-client-core-5.1 (5.1.41-3ubuntu12.8), libdbd-mysql-perl (4.012-1ubuntu1), mysql-server-5.1 (5.1.41-3ubuntu12.8), mysql-client-5.1 (5.1.41-3ubuntu12.8), libmysqlclient-dev (5.1.41-3ubuntu12.8), libplrpc-perl (0.2020-2), mysql-server-core-5.1 (5.1.41-3ubuntu12.8), mysql-server (5.1.41-3ubuntu12.8), libmysqlclient16-dev (5.1.41-3ubuntu12.8)
Upgrade: mysql-common (5.1.41-3ubuntu12.6, 5.1.41-3ubuntu12.8), libmysqlclient16 (5.1.41-3ubuntu12.6, 5.1.41-3ubuntu12.8)
End-Date: 2010-12-21 11:33:03
\ldots
\end{alltt}
\caption[APT log extract]{An extract of an APT log file}
\label{aptlog}
\end{center}
\end{figure}

These logs describe the changes made to the system by \texttt{apt-get}, and not necessarily what the user requested to cause the change.
However, using the constraints that \texttt{apt-get} employs to make changes, the user request can be extracted.
These constraints are:
\begin{itemize}
  \item \texttt{apt-get} will never install or remove a component if the system is upgraded.
  \item \texttt{apt-get} will only install a package if one has been selected to be installed.
\end{itemize}
Using these constraints each log is processed and the dates a user upgraded or installed a component are extracted.
With this information the probability that a user upgrades or installs a component on a given day is calculated.
A k-means algorithm is used to cluster and find the center points for four types of users.
The results of this process are presented in figure \ref{exp.figuserlogs} and table \ref{exp.tbluserlogs} .

\begin{figure}[htp]
\begin{center}
  \includegraphics[width=\textwidth]{plots/userlogAnalysis}
  \caption{The Extracted Upgrade and Install probabilities of the Submitted Logs}
  \label{exp.figuserlogs}
\end{center}
\end{figure}

\begin{table}[h!]
\centering
\begin{tabular}{|l | c | c | }
\hline
User Name 				& 	$u$ 		& $i$ 		\\ \hline
High Install (HI)			& 0.422			& 0.764 	\\
Medium Change (MC)			& 0.222			& 0.259 	\\
High Upgrade (HU)			& 0.561			& 0.192		\\
Low Change 	(LC)			& 0.186			& 0.086 	\\ \hline
\end{tabular}
\caption{Extracted Users from submitted \texttt{apt-get} Logs}
\label{exp.tbluserlogs}
\end{table}

\subsection{Simulation of Realistic Users}
\label{exp.q4}
Above some realistic users are created by extracting information from user submitted logs.
These users are simulated using the \texttt{apt-get} upgrade criteria \texttt{-removed,-new,-uptodatedistance} and the progressive upgrade criteria \texttt{-removed,-uptodatedistance,-new}.
The progressive users names are prefixed  with ``Pro. Upgrade'' where the users using the \texttt{apt-get} criteria are not changed.

The UTTDpC of these users are presented in figure \ref{exp.q6uttdpc}.
\begin{figure}[htp]
\begin{center}
  \includegraphics[width=\textwidth]{plots/q6usersuttd}
  \caption{The UTTDpC plotted for the simulated users.}
  \label{exp.q6uttdpc}
\end{center}
\end{figure}
This figure shows that using the \texttt{apt-get} upgrade criteria results in a system that after a year of simulation is about 0.8 UTTDpC out of date for all users. 
The only noticeable difference between these users is that ``High Install'' users systems take longer to become up-to-date after the Ubuntu 10.04 release.

The users ``Pro. Upgrade High Install'' and ``Pro. Upgrade High Update'' benefit the most from using the progressive upgrade criteria.
These two users are about 0.42 UTTDpC at the end of the simulations.
This is compared to the other two progressive users who are about 0.62 UTTDpC out-of-date.
These results show that using the progressive criteria has the most benefit for users that upgrade the most.      

The total changes of these users are presented in figure \ref{exp.q6userchange} and table \ref{exp.tblq6change}.
\begin{figure}[htp]
\begin{center}
  \includegraphics[width=\textwidth]{plots/q6userchange}
  \caption{The Total Change for the simulated users.}
  \label{exp.q6userchange}
\end{center}
\end{figure}

\begin{table}[h!]
\centering
\begin{tabular}{|l | c | c | c | }
\hline
User Name 				& \texttt{apt-get} criteria 		& Progressive criteria 	& Difference	\\ \hline
High Install (HI)			& 3176			& 4043 	& 867	\\
Medium Change (MC)			& 2291			& 2660 	& 369 	\\
High Upgrade (HU)			& 2123			& 2842	& 719   \\
Low Change 	(LC)			& 1903			& 2212 	& 309   \\ \hline
\end{tabular}
\caption{The mean total change when using different upgrade criteria}
\label{exp.tblq6change}
\end{table}

This figure shows the additional change required when using the progressive criteria.
This figure shows that the total change of  ``High Install'' users for both criteria are significantly more than the other users.
Additionally it shows that the ``High Upgrade'' user's total change is significantly increased when using the progressive criteria.
This can be seen in the final months of the simulation where the rate of change increases due to the upcoming release of Ubuntu 10.10.

The reduced change when using the stable version criteria can be estimated for each of these users.
This is accomplished by counting the amount of times each user upgrades the same component within $d$ days.
These results are presented in table \ref{exp.tblq6sv}.
\begin{table}[h!]
\centering
\begin{tabular}{|l | c | c | c | c |}
\hline
User Name 	& $d$ = 7 	&  $d$ = 14 	&	$d$ = 21 	&	$d$ = 28 	\\ \hline
HI 			&24.58 	&	43.24&	 67.22&	 93.88\\
HU 			&19.08 	&	31.32&	 48.4&	 70.52\\
MC 			&11.54 	&	26.98&	 45.14&	 65.1\\
LC 			&8.5 	&	22.54&	 38.48&	 53.56\\
Pro. HI 	&30.72 	&	57.0&	 85.52&	 117.52\\
Pro. HU 	&32.02 	&	56.2&	 79.62&	 110.1\\
Pro. MC 	&14.9 	&	30.28&	 48.28&	 67.66\\
Pro. LC 	&12.4 	&	24.84&	 41.3&	 59.4\\ \hline
\end{tabular}
\caption{The estimated mean reduced change when using the stable version criteria}
\label{exp.tblq6sv}
\end{table}

This table shows that users that upgrade often will have the most benefit using the stable version criterion.
These users may not wish to use the stable version criterion as they may upgrade frequently to decrease out-of-dateness of their systems
which would be increased when using the stable version criterion.
However, the reduction in change caused when using the stable version criterion may reduce the risk of upgrading 
and this may increase the probability a conservative user would upgrade.
This situation would increase the benefits of the using the stable version criterion for conservative users.

\section{Answers}
The above performed experiments were conducted in order to answer the questions:
\begin{enumerate}
  \item What consequences do a user's choices have on their system?
  \item Can the out-of-dateness of a system be reduced?
  \item Can the change of a system be reduced?
  \item How do the systems of realistic users evolve?
\end{enumerate}

Each of these questions will be addressed in turn here.
\subsection{User Requests}
The notable consequences a users requests to change their system were measured through the experiments in section \ref{exp.q1}.
These experiments simulated users using the criteria \texttt{apt-get} uses to alter systems.
These consequences are summarised as:
\begin{itemize}
	\item A system will always become more out of date due to:
  		\begin{itemize}
  			\item the internal constraints of the components hindering the upgrading of components.
  			\item the criteria of \texttt{apt-get} stopping the installation of new packages to enable the upgrading of an installed component.  
		\end{itemize}
	\item The majority of change during evolution is caused by a user upgrading.
	Installing new components increases the amount of change when upgrading.
	\item Systems become out-of-date at the rate at which components evolve. Components evolve at a higher rates during release cycles.
	\item Reuse decreases the rate of change during CSE.
	This is due to the two effects; 
	reuse decreases the installation rate of components and this decreases the amount of components necessary to be upgraded.   
	\item There were two types of failures observed to occur:
		\begin{itemize}
  			\item Hard failures: where the constraints of a request could not be satisfied. These occurred 0.12\% during the simulation.
  			\item Soft failures: where the constraints were satisfied, though the returned solution was possibly not optimal.
  				These occurred in 3\% of the requests, though only 1\% of these were determined to be detrimental to the system.
  				Such detrimental soft failures occur more frequently during release cycles when components are evolving at a higher rate.
		\end{itemize}
	\item Increasing the frequency of upgrading has depreciating returns on reducing a systems out-of-dateness. 
	It may also increase change due to components being repeatedly upgraded if they are quickly released. 
\end{itemize}

\subsection{Reduce Out-of-dateness}
As noted above, a system becomes out-of-date partially due to the \texttt{apt-get} criteria not allowing new components to be installed.
To remove this restriction the \texttt{apt-get} criteria was altered to the ``progressive'' criteria.
Using this criteria to upgrade a system every day was shown to decrease the out-of-dateness by 24\% at the cost of increasing change by 21\%.
This additional change was due to the new components being installed and upgraded during the simulation.

\subsection{Reduce Change}
Some of the change caused when upgrading a system was due to rapidly released components being repeatedly upgraded.
For a user that upgrades every day, there were 23 occasions were a component being upgraded twice within a week.
This is unnecessary change that introduces risk into a system. 
The stable version criterion was defined to reduce this change by waiting for a component to become stable over a period of days.
The cost of using the stable version criteria was the system remains out-of-date by the number of days waited for components to become stable.
However, through using the stable version criteria waiting a week for components to become stable all 23 instances of this unnecessary change were removed.

\subsection{Real Users}
To simulate realistic users the probability a user upgrades and installed were extracted from user's submitted logs.
Using the k-means algorithm four stereotypes of user were created; `High Install'', ``High Upgrade'', ``Medium Change'' and ``Low change''.
These users were simulated using the \texttt{apt-get} criteria and the progressive criteria, and the lowered change from using the stable version criterion was estimated.
This has shown that the greatest benefits for using the progressive criteria and the stable version criterion is for the ``High Install'' and ``High Upgrade'' users.
This is due to their higher frequency they upgrade their systems.

\section{Summary}
This chapter presented a series of experiments that simulated various users to study CSE.
These experiments explored the consequences of users changing their systems, and the novel proposed progressive and stable version criteria.
The next chapter describes the conclusions, the related work, and the potential future research of CSE.



