
\chapter{Experiments, Results and Analysis}
\label{experiments}
\epigraph{Experiment is the sole judge of scientific ``truth''}
{\textit{The Feynman Lectures on Physics, Introduction, Richard Feynman, 1961.}}
Previously the models and the implementation used to simulate CSE have been described.
The \usermodel model is used to create CUDF* documents given the probabilities a user will request to upgrade and install components, 
and the criteria used to accomplish these requests.
These documents describe the evolution of an Ubuntu system for a year starting on October 30th 2009.
Given such a document, the implementation GJSolver can be used to resolve the exact evolution of the system.
This process of describing a user in \usermodel, generating CUDF* documents, then resolving the evolution is used to explore CSE.

This chapter presents experiments and results to study the effects of CSE.
The specific effects that are focused on are the change made to the system, and the out-of-dateness of the system during evolution.
This exploration is accomplished through trying to answer the questions:
\begin{enumerate}
  \item What effects do the probabilities user upgrades per day ($u$) and a user installs a component per day ($i$) have during CSE?
  \item Can the upgrade criteria ($U$) be altered to decrease out-of-dateness?
  \item Can the upgrade criteria ($U$) be altered to reduce change?
  \item What are the effects of these new criteria when simulating ``real'' users?
\end{enumerate}
This chapter is organised to present these questions in this order. 

\section{Upgrade and Install Variable Analysis}
How often a user decides to upgrade their system or install a component will effect how the system evolves.
This section attempts to quantify the effects on change and out-of-dateness that these actions have on the users systems.

These experiments simulates users altering the probabilities a user will install a component $i$ and upgrade their system $u$,.
The criteria to install $I$ and upgrade $U$ are defaulted to the ones used by \texttt{apt-get}. 
\texttt{apt-get}'s $U$ criteria is \texttt{-removed,-new,-uptodatedistance}, and $I$ criteria is \texttt{-ovpp,-removed,-changed,-uptodatedistance} as described in section \ref{impl.validation} 

\subsection{Boundary Cases}
The first experiments assign values to the variables in \usermodel that describe the boundary cases for $i$ and $u$:
\begin{table}[h!]
\centering
\begin{tabular}{|l | c | c | c |}
\hline
User Name 				 	& \# simulated 	& $u$ 		& $i$ 			\\ \hline
Always Upgrade				& 1 			& 1			& 0				 \\
Control						& 1 			& 0			& 0				\\
Always Install 				& 30 			& 0			& 1				 \\
Always Upgrade\&Install 	& 30 			& 1			& 1				\\ \hline
\end{tabular}
\caption{Users that are the boundary cases in the simulation.}
\label{exp.tblextremeusers}
\end{table}
This table also states the number of simulations that were run for each user, i.e. the ``Always Install'' user had 30 CUDF* documents created and resolved.
The number of simulations run for a user is a trade-off between time to complete the simulations and accuracy of results.
The number of simulations run is based on the randomness of the simulated users (as described in section \ref{sim.randomness}) 
as well as the experience gained when developing the simulation as to the variability of the results.

\subsubsection{Results and Analysis}
First, how out-of-date these simulated systems become will be explored.
This out-of-dateness is measured using the function up-to-date distance (UTTD) function, as described in section \ref{impl.criteria}.
This measurement is the number of components that are a greater version than a component currently installed.
A problem that exists with this measurement is that it does not take into account the size of a system.
As a system grow the UTTD will grow as there will be more components that become out of date.
To normalise this effect the measurement UTTD per component (UTTDpC) is defined as the UTTD per installed component.
The UTTDpC of each simulated system is presented in figure \ref{exp.q1auttdpc}.
\begin{figure}[htp]
\begin{center}
  \includegraphics[width=\textwidth]{plots/q1auttdperc}
  \caption{The Up-to-date distance per Component (UTTDpC) of the simulated systems.}
  \label{exp.q1auttdpc}
\end{center}
\end{figure}

This figure shows that all systems eventually become out-of-date likely due to the \texttt{apt-get} upgrade criteria and constraints in core components restricting change.
The \texttt{apt-get} criteria minimises new components installed into the system before minimising the UTTD.
Therefore, if a newer version of a component require a new component to be installed, this component will not be upgraded when using this criteria.
Section \ref{exp.prouttdsection} presents experiments where changing this criteria removes this barrier to new components. 


This figure also shows that users which do not upgrade, the ``Control'' and ``Always Install'' users, become nearly three times more out-of-date than users that upgrade.
The speed at which systems become out-of-date increases over the months between March and May 2010.
The reason for this is likely due to the increased development effort because of the Ubuntu 10.04 release in April 2010.
The users that upgrade are less affected by this release as they are continually upgrading to the new component versions.

The effect of the normalisation can also be seen in this figure, where the install variable $i$ has little effect on the UTTDpC of a component system.

Second, how much change each simulated system went through during evolution is measured.
To measure change the ``change'' function as described in section \ref{impl.criteria} is used.
The total change, i.e. the sum of all change a system has to a date, is presented in figure \ref{exp.q1achange}
\begin{figure}[htp]
\begin{center}
  \includegraphics[width=\textwidth]{plots/q1achange}
  \caption{The total change of the simulated systems.}
  \label{exp.q1achange}
\end{center}
\end{figure}

As was predicted above, this figure shows that there is an increase in change during the release of Ubuntu 10.04 for users that upgrade their systems.
During the months of March, April and May of 2010 had a mean of 266 changes per month for the ``Always Upgrade'' user.
All other months had had a mean of 105 changes per month, showing a more than 250\% increase in change over the release months.
This is likely because the increased development of components leads to an increased release of versions that have to be upgraded.
This increased change cannot be seen in the ``Always Install'' users as they will not change in response to newer versions.

The ``Always Install'' users start by changing their systems quickly, then after a month this change slows to a constant rate.
This can be seen with the mean change per day, during the first month it is 4.9, where the final 11 months it is 3.0.
After a small investigation this reduction in change can be explained due to the common reuse of individual components e.g. \texttt{libaudio2} and \texttt{libqt3-mt}.
Many components require such components to be installed, they are then required to be installed in the first month.
After this installation they are no longer required to be added, so the future change is reduced. 
These effects described here may be due to the way in which \usermodel selects components to be installed.

The ``Always Upgrade\&Install'' users changes more than the combined amount of ``Always Upgrade'' and ``Always Install'' changes.
The mean total change of the ``Always Upgrade'' user is 1655, the ``Always Install'' user is 1137 and the ``Always Upgrade\&Install'' user is 3482.
This shows that a user that always upgrades and installs changes more than 700 components more than the sum of just installing and just upgrading.
The additional change is due to the installed components increasing the amount of components to be upgraded.
This also shows that upgrading a system over the course of a year causes more change than installing a component everyday.
Though, this may be related to how packages are selected for installation.

\subsection{Failures}
These initial experiments are the most extreme users that can be simulated.
This means phenomena that occur rarely are most observable in these simulations.
For this reason, these simulations were chosen to be closely inspected for various failures.

Three types of failure were observed in the results of these simulations:
\begin{enumerate}
  \item \textbf{Hard Failure}: Where a request has no satisfactory solution, therefore no change is made to the system.
  \item \textbf{Soft Failure}: The request is satisfied, however the search was interrupted and the best solution found at that time was returned.
  \item \textbf{Multi-component Failure}: A type of hard failure where multiple components must be installed to satisfy a request. 
  This causes the next upgrade request to fail and the next install request to remove the offending components.
\end{enumerate}

%%%Hard failures
Hard failures where observed to occur in 24 of the 60 ``Always Install'' and ``Always Upgrade\&Install'' users, and never in the ``Always Upgrade'' user.
Only 40 total failed requests occurred over these 24 simulated users.
All these hard failures, except those that are multi-component failures, were failed install requests.
Given each ``Always Install'' user has 365 requests and each ``Always Upgrade\&Install" user has 730 requests, 
and 30 users of each type were simulated, the chance for a request to fail is just over 0.12\%.
This means the chance for a hard failure to occur is extremely low in this simulation.

Due to the difficultly of finding the constraints that caused these failures \citep{quickxplain},
and the rarity of these failures, the specific reasons for hard failures are not further explored.

%%%Soft Failure TODO
Soft failures are called ``soft'' because the request that causes them succeeds, however the search failed to finish.
This does not mean that the returned solution is not optimal\footnote{In MISC GJSolver was interrupted many times and most times returned the optimal solution},
it just means that it is uncertain that it is optimal.
Of the 33215 requests during the simulation of the users defined in table \ref{exp.tblextremeusers} only 900 requests were interrupted.
This is less than 3\%.
After looking at the results returned by the 900 requests only 8 were determined to be detrimental to the simulation.
Each of these 8 requests caused more than 100 components in the system to be removed in order to be satisfied.
This is due to the criterion \texttt{-removed} not being fully optimised before being interrupted.
There are three interesting points that were observed with these 8 requests:
\begin{enumerate}
  \item seven of them occurred during the Ubuntu 10.04 release month of April 2010, where the remainder of the soft failures are evenly distributed.
  \item five times the component that was requested to be installed was removed by the following requests.
\end{enumerate}
The first point implies that these detrimental soft-failures occur because of the increased release of components during the Ubuntu release.
The second point implies that the components themselves are over constrained and thus removed in future systems.
The effect of these detrimental soft failures can be seen in figure \ref{exp.q1achange} 
where the standard deviation of change for both ``Always Install'' and ``Always Upgrade\&Install'' users increases after the Ubuntu 10.04 release.
Given that only 8 such failures occurred, this is seen as a minor impact on the validity of the simulation..
Although, in future experiments if soft failures are detected they will be mentioned. 
These failures may be reduced or removed by increasing the time before the algorithm is interrupted, or by using more powerful hardware to perform the simulations.

%%%The multi component fialure
An interesting failure occurred when the situation arose that installing the package \texttt{chromium-browser} required two versions of the package \texttt{libc6} to be installed. 
This lead to the following upgrade request to be unsatisfiable, and the following install request to remove \texttt{chromium-browser} and upgrade to the newest version of \texttt{libc6}.
This instance, although rare, shows the case that at some points the hard constraint that Debian enforces to have only one version of each component installed,
can restrict the user.

%%%Disclaimer
All these failures are directly caused by, or are a result of an install request.
The failures and/or the rates they occur may directly be a product of the selection of components being the least valid part of the simulation (discussed in section \ref{sim.modelvalidation}).

\subsection{Upgrade Probability Effects}
By altering only the probability a user upgrades their system, while leaving other variables static, the effect of the user upgrading can be measured.
The users for simulated are:
\begin{table}[h!]
\centering
\begin{tabular}{|l | c | c | c |}
\hline
User Name 				& \# simulated 	& $u$ 		& $i$ 			\\ \hline
Upgrade once a month	& 10 			& 0.03 (1/30)			& 0				 \\
Upgrade twice a month	& 10 			& 0.06 (1/15)		& 0				\\
Upgrade once a week		& 10 			& 0.14 (1/7)		& 0				 \\
Upgrade twice a week 	& 10 			& 0.29 (1/3.5)		& 0				\\ \hline
\end{tabular}
\caption{Users with different upgrade probabilities.}
\label{exp.tblextremeusers}
\end{table}
These users are compared to the ``Always Upgrade'' user described in table \ref{exp.tblextremeusers}.

The up-to-date distance per component for these users is compared in figure \ref{exp.q1buttdperc}.
\begin{figure}[htp]
\begin{center}
  \includegraphics[width=\textwidth]{plots/q1buttdperc}
  \caption{The Up-to-date distance per Component (UTTDpC) of the simulated systems.}
  \label{exp.q1buttdperc}
\end{center}
\end{figure}

This figure shows that upgrading less frequently has it's greatest effect over the release of the Ubuntu 10.04.
This is most noticeable in the ``Upgrade once a month'' users.
This is likely because the increased release of new versions during the April release.
If the user does not upgrade often during this time their system will become quickly out of date.

Another result from this experiment is showing the correlation between UTTDpC and the reciprocal of the upgrade probability.
This can be seen in figure \ref{exp.q1bcorrolationuttdpc} that compares the mean UTTDpC to $1/u$.

\begin{figure}[htp]
\begin{center}
  \includegraphics[width=\textwidth]{plots/q1bcorrolationuttdpc}
  \caption{The mean up-to-date distance per component (UTTDpC) plotted against of the reciprocal of the users update probability ($u$)}
  \label{exp.q1bcorrolationuttdpc}
\end{center}
\end{figure}

The figure shows the mean UTTDpC of ``Always Upgrade'' is 0.258 and the ``Upgrade twice a week'' users mean is 0.267.
This is a difference of 0.009 UTTDpC.
This difference can be illustrated in an example where given a system has 1000 components, 
if a user upgraded twice a week they will have on average 9 components more out-of-date than if they upgraded every day. 
This figure also shows the diminishing return when increasing the amount a user upgrades.

The total change of these users is presented in figure \ref{exp.q1bchange}.

\begin{figure}[htp]
\begin{center}
  \includegraphics[width=\textwidth]{plots/q1bchange}
  \caption{The mean total change plotted against of the simulated users.}
  \label{exp.q1bchange}
\end{center}
\end{figure}

This figure shows that upgrading less frequently requires less change. 
This is in part due to the fact that updating once a month means that the majority of the month there is no change.

Another effect was noticed that reduced change of the users that upgraded less frequently.
This is where many versions of a component may be released in quick succession, making upgrading to each interim version not required.
For example, if over a week a user upgrades their system every day and upgrades 5 different components twice (a total of 10 change), 
then they will upgrade twice as much as a user that only upgraded at the end of the week.
This is because the user that upgrades every day changes to all versions of the components, where the other only changes to the final version.

An example of this rapid release of components happening in the Ubuntu repository is given in figures \ref{exp.apachelog} and \ref{exp.apachebug}.

\begin{figure}[htp]
\begin{center}
\begin{alltt}
apache2 (2.2.20-1) unstable; urgency=low

  * New upstream release.
  * Fix some regressions related to Range requests caused by the CVE-2011-3192
    fix. Closes: #639825
\ldots

 -- Stefan Fritsch <sf@debian.org>  Sun, 04 Sep 2011 21:50:22 +0200

apache2 (2.2.19-2) unstable; urgency=high

\ldots

 -- Stefan Fritsch <sf@debian.org>  Mon, 29 Aug 2011 17:08:17 +0200
\end{alltt}
\caption[Apache Changelog]{An extract from the apache changelog located on http://changelogs.ubuntu.com/}
\label{exp.apachelog}
\end{center}
\end{figure}

\begin{figure}[htp]
\begin{center}
\begin{alltt}

Reported by: Takis Issaris <takis.issaris@uhasselt.be>
Date: Tue, 30 Aug 2011 16:09:01 UTC
Severity: important
Found in versions 2.2.9-10+lenny10, 2.2.16-6+squeeze2, apache2/2.2.19-2

\ldots

Package: apache2.2-common
Version: 2.2.9-10+lenny10

Yesterday evenings update broke our Apache server setup,
\ldots
\end{alltt}
\caption[Apache Bug Report]{Extract from the bug report \#639825, filed with Debian}
\label{exp.apachebug}
\end{center}
\end{figure}

A summary of these events are:
\begin{enumerate}
  \item Apache developer Stefan Fritsch released a new version of their server, \texttt{apache2}/2.2.19-2, on 29 Aug 2011.
  \item Takis Issaris upgraded to this version which broke his system on 29 Aug 2011.
  \item Takis Issaris submited a bug report on 30 Aug 2011, where he and Stefan Fritsch discuss the causes.
  \item On 04 Sep 2011 (5 days after the initial bug report) Stefan Fritsch releases a new version 2.2.20-1 that fixes this bug.
\end{enumerate}
In this example, the maintenance of the component \texttt{apache2} caused the release of two versions within a week of one another.

The amount of change this effect causes can be found out by looking at the ``Always Upgrade'' user.
The ``Always Upgrade'' user upgrades the same component within a week on 23 different occasions, and within a month on 68 difference occasions.
By updating less frequently these instances may be avoided.
This may not seem like an insignificant amount of change, especially since the ``Always Upgrade'' user changes over 1500 components during the simulated year.
However, these changes may introduce bugs (as with the above example) and are an unnecessary risk to the users system.
Additionally, as a user installs more components these situations will likely increase.

In section \ref{exp.stable} a criteria is created to explicitly take advantage of this phenomena to reduce change.


\subsection{Install Probability Effects}
By altering the probability a user installs a component into their system, the effect this has on 
change\footnote{As discussed before the probability to install will have little effect on UTTDpC} can be studied. 
\begin{table}[h!]
\centering
\begin{tabular}{|l | c | c | c |}
\hline
User Name 				& \# simulated 	& $u$ 		& $i$ 			\\ \hline
Install once a month	& 30 			& 0 & 0.03 (1/30)							 \\
Install twice a month	& 30 			& 0 & 0.06 (1/15)						\\
Install once a week		& 30 			& 0 & 0.14 (1/7)					 \\
Install twice a week 	& 30 			& 0 & 0.29 (1/3.5)						\\ \hline
\end{tabular}
\caption{Users with different upgrade probabilities.}
\label{exp.tblextremeusers}
\end{table}
These users are compared to the ``Always Install'' users described in table \ref{exp.tblextremeusers}.

The total change of these users is presented in figure \ref{exp.q1cchange}.
\begin{figure}[htp]
\begin{center}
  \includegraphics[width=\textwidth]{plots/q1cchange}
  \caption{The mean total change plotted against of the simulated users.}
  \label{exp.q1cchange}
\end{center}
\end{figure}
This figure shows the range of possible change given the probability to install a component.
It also shows the change is inversely proportional to the reciprocal of $i$, 
i.e. ``Install twice a week'' users change twice as much as ``Install once a week'' users.
The reason that further analysis is futile is that the selection of components is the least valid part of the simulation, as discussed in section \ref{sim.randomness}.
 

\section{Decreased Out-of-dateness During Evolution}
\label{exp.prouttdsection}
In the previous section the criteria used during evolution were the criteria used by \texttt{apt-get}.
This criteria limits the up-to-dateness of the system by prioritising the minimisation of installing new components.
Altering this criteria from \texttt{-removed,-new,-uptodatedistance} to \texttt{-removed,-uptodatedistance,-new} 
will let progressive users, who prioritize up-to-dateness over change, upgrade their system.

\begin{table}[h!]
\centering
\begin{tabular}{|l | c | c | c |}
\hline
User Name 								& \# simulated 			& $u$ 		& $i$ 			\\ \hline
Progressive Always Upgrade				& 1 			 	& 1				& 0\\ \hline
\end{tabular}
\caption{Progressive Users with different upgrade probabilities.}
\label{exp.tblextremeusers}
\end{table}
This user has the upgrade criteria $U$ assigned to \texttt{-removed,-uptodatedistance,-new}.
It is compared to the ``Always Upgrade'' user which is the same except it's upgrade criteria $U$ is assigned \texttt{-removed,-new,-uptodatedistance}.

Figure \ref{exp.q4auttdperc} presents the UTTDpC of these users.
\begin{figure}[htp]
\begin{center}
  \includegraphics[width=\textwidth]{plots/q4auttdperc}
  \caption{The UTTDpC plotted for the simulated users.}
  \label{exp.q4auttdperc}
\end{center}
\end{figure}

This figure shows that the progressive criteria reduces the out-of-dateness of the systems.
The final UTTDpC for the ``Always Upgrade'' user 0.72 compared to the final UTTDpC for the ``Progressive Always Upgrade'' user 0.48
That is, a user who upgrades with the progressive criteira will have a system that is 24\% less out-of-date.

The UTTDpC of these systems is most differentiated during the release of Ubuntu 10.04.
At this time new versions of components are being released quickly with   

These are promising results, showing that it is possible to reduce the out-of-dateness of a system.
However, this comes at the cost of increased change.
This additional change comes from the 90 new components that the progressive change installs.
These new components also are upgraded throughout the year which is 258 change.
The total additional change when using a progressive user is 348 over a year.  

When a user upgrades their system with the progressive upgrade criteria at the end of the year there system is 24\% less out-of-date at the cost of an additional 258 changes.
Users that prefer to have an up-to-date system may favour this criteria over the \texttt{apt-get} criteria. 
This criteria is further simulated in section \ref{exp.realuserssim}.

\section{Reduction of Change During Evolution}
\label{exp.stable}
The phenomenon where components are rapidly released causes unnecessary change when upgrading.
This change can be reduced by changing the criterion UTTD criterion to a new criterion called \textit{stable version}.
To define this criteria first the $stable$ function must be defined:
\begin{defs}
The function $stable$ takes a component $c$ an integer $d$ and a set of components $\mathbb{C}_t$ and returns a true iff:
%boolean sincerelease = (releasetime + this.timespan) < this.currentdate ;
%boolean versionrelease = (releasetime + this.timespan) < nextrelease;
%boolean stupidrepo = releasetime > nextrelease;
\begin{enumerate}
  \item  the component was released within $d$ days of time $t$.
  \item there exists no component with the same name, a greater version, that was released within $d$ days after the release of it. 
\end{enumerate}
\end{defs}
This function takes a component and a $d$ and attempts to determine if the component has not been changed in $d$ days.
The first part of the definition ensures that the function waits $d$ days before stating a component is stable.
The second part ensures that if a better version is released within $d$ days, the component is stated to be unstable.

A criterion using this measurement can then be defined:
\begin{defs}
	Given the set of components $\mathbb{C}_t$ and number of days $d$, the \textbf{stable version} criterion is defined as $crit_{sv} = \langle rank^{sv}_{\alpha}, \geq \rangle$,
	where \\$rank^{sv}_{\alpha}(\beta) = \sum_{c \in \beta} \begin{cases} 1000 + uptodatedistance(c,\mathbb{C}_t)& not stable(c,d,\mathbb{C}_t) \\ uptodatedistance(c,\mathbb{C}_t) & \text{otherwise} \\  \end{cases}$
\end{defs}
That is the ranking function $rank^{sv}_{\alpha}(\beta)$ returns the UTTD of the component if it is stable,
else it returns 1000 added to the UTTD.
This extra weight of 1000 that is added if the component is seen as unstable will encourage the system to wait $d$ days to upgrade this component.

The mapping between this criterion, MOF and the PB criteria is presented in table \ref{exp.stablcritmapping}, with a full description in appendix \ref{apx.critmapping}.
\begin{table}[htp]
\begin{tabular}{c | c | c}
\textbf{MOF} 		& \textbf{\modelname criterion} & \textbf{PB criterion} \\
\texttt{-stableversion(}$d$\texttt{)} & $crit_{sv} = \langle rank^{sv}_{\alpha}, \geq \rangle$ & $\langle f_{sv}, <, I_{sv} \rangle$ \\
\end{tabular}
\caption{Mapping between these elements}
\label{exp.stablcritmapping}
\end{table}
In this mapping the amount of days $d$ is left blank, allowing a range of different values to be simulated.

The users simulated are described in table \ref{exp.tblsvusers}.
\begin{table}[h!]
\centering
\begin{tabular}{|l | c | c | c | }
\hline
User Name 				& \# simulated 	& $u$ 	& $U$ 			\\ \hline
7 Days SV Upgrade		& 1 			& 1 & 	\texttt{-removed,-new,-stableversion(7)}			 \\
14 Days SV Upgrade		& 1 			& 1 & 	\texttt{-removed,-new,-stableversion(14)}			\\
21 Days SV Upgrade		& 1 			& 1 & 	\texttt{-removed,-new,-stableversion(21)}			 \\
28 Days SV Upgrade 		& 1 			& 1 & 	\texttt{-removed,-new,-stableversion(28)}			\\ \hline
\end{tabular}
\caption{Users with different upgrade probabilities.}
\label{exp.tblsvusers}
\end{table}
These users are compared to the ``Always Upgrade'' user described in table \ref{exp.tblextremeusers}.

The UTTDpC of these simulated users are presented in figure \ref{exp.q5auttdperc}.
\begin{figure}[htp]
\begin{center}
  \includegraphics[width=\textwidth]{plots/q5auttdperc}
  \caption{The UTTDpC plotted for the simulated users.}
  \label{exp.q5auttdperc}
\end{center}
\end{figure}

This figure shows the cost of using this criteria, the system is $d$ days out-of-date while the criterion waits for the components to ``stabilise''.
The difference between the ``Always Upgrade'' user and the ``7 Days SV Upgrade'' user is on average 0.025 UTTDpC.

The reduction in change for the ``7 Days SV Upgrade'' user is 28 changes over the year.
This is measured by subtracting the total change of the ``Always Upgrade'' user 7 days before the end of the simulation
from the total change of the  ``7 Days SV Upgrade'' user.
This calculation takes into account the lag caused by the 7 day delay in upgrading.
Above, it was calculated the total number of occurrences where a component is released

\section{Real Evolution}
\label{exp.realuserssim}
During the previous experiments how often the user upgrades or installs a component have been assigned values that may not be ``realistic''.
This section presents the final experiments where the variables are assigned values calculated from real user's systems.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Extracting Information from the User Submitted Logs}
These variables of update probability and user install probability can be extracted from the user submitted logs from the survey.
Initially, 31 logs where submitted, these logs were filtered to be APT logs, of more than 15 days long.
This resulted in 19 logs of between 23 and 277 days long to be processed.

An extract from one of these logs is shown in figure \ref{aptlog}.
\begin{figure}[htp]
\begin{center}
\begin{alltt}
\ldots
Start-Date: 2010-12-21 11:32:28
Install: libnet-daemon-perl (0.43-1), libhtml-template-perl (2.9-1), libdbi-perl (1.609-1build1), mysql-client-core-5.1 (5.1.41-3ubuntu12.8), libdbd-mysql-perl (4.012-1ubuntu1), mysql-server-5.1 (5.1.41-3ubuntu12.8), mysql-client-5.1 (5.1.41-3ubuntu12.8), libmysqlclient-dev (5.1.41-3ubuntu12.8), libplrpc-perl (0.2020-2), mysql-server-core-5.1 (5.1.41-3ubuntu12.8), mysql-server (5.1.41-3ubuntu12.8), libmysqlclient16-dev (5.1.41-3ubuntu12.8)
Upgrade: mysql-common (5.1.41-3ubuntu12.6, 5.1.41-3ubuntu12.8), libmysqlclient16 (5.1.41-3ubuntu12.6, 5.1.41-3ubuntu12.8)
End-Date: 2010-12-21 11:33:03
\ldots
\end{alltt}
\caption[APT log extract]{An extract of an APT log file}
\label{aptlog}
\end{center}
\end{figure}

These logs mainly describe the changes made to the system by APT, not necessarily what the user requested.
This is because APT may be used through another application, like semantic, to install or update the system.
As the criteria of APT is known, some information about the action the user requested can be inferred.

Given APT will never install or remove a component if the system is updated; 
if a package is upgraded but none are removed or installed then the user probably selected to update.
Also, if a package is installed, then the user requested a single package to be installed.
This second rule is assuming that only one package was selected, and not two at the same time.

Using these rules, each log is processed, and the variables of update probability and install distribution are measured.

\section{Summary}
%%%A list of the answers gained from the questions
