
\chapter{Experiments, Results and Analysis}
\label{experiments}
\epigraph{Experiment is the sole judge of scientific ``truth''}
{\textit{The Feynman Lectures on Physics, Introduction, Richard Feynman, 1961.}}
Previously the models and the implementation used to simulate CSE have been described.
The \usermodel model is used to create CUDF* documents given the probabilities a user will request to upgrade and install components, 
and the criteria used to accomplish these requests.
These documents describe the evolution of an Ubuntu system for a year starting on October 30th 2009.
Given such a document, the implementation GJSolver can be used to resolve the exact evolution of the system.
This process of describing a user in \usermodel, generating CUDF* documents, then resolving the evolution is used to explore CSE.

This chapter presents experiments and results to study the effects of CSE.
The specific effects that are focused on are the change made to the system, and the out-of-dateness of the system during evolution.
This exploration is accomplished through trying to answer the questions:
\begin{enumerate}
  \item What consequences do a user's choices (their probabilities to upgrade $u$ and install $i$) have on their system?
  \item Can the out-of-dateness of a system be reduced?
  \item Can the change of a system be reduced?
  \item How do the systems of realistic users evolve?
\end{enumerate}
This chapter presents experiments intended to answer these questions. 

\section{Users Choices to Upgrade and Install}
\label{exp.q1}
How often a user decides to upgrade their system or install a component will effect how the system evolves.
This section attempts to quantify the effects on change and out-of-dateness that these actions have on the users systems.

These experiments simulate users by altering the probabilities a user will install a component $i$ and upgrade their system $u$.
The criteria to install $I$ and upgrade $U$ are defaulted to the ones used by \texttt{apt-get}. 
\texttt{apt-get}'s $U$ criteria is \texttt{-removed,-new,-uptodatedistance}, and $I$ criteria is \texttt{-ovpp,-removed,-changed,-uptodatedistance} as described in section \ref{impl.validation} 

\subsection{Boundary Cases}
The first experiments assign values to $i$ and $u$ that describe the boundary cases.
The experimented on users are described in table \ref{exp.tblextremeusers}.
\begin{table}[h!]
\centering
\begin{tabular}{|l | c | c | c |}
\hline
User Name 				 	& \# simulated 	& $u$ 		& $i$ 			\\ \hline
Control						& 1 			& 0			& 0				\\
Always Upgrade				& 1 			& 1			& 0				 \\
Always Install 				& 30 			& 0			& 1				 \\
Always Upgrade\&Install 	& 30 			& 1			& 1				\\ \hline
\end{tabular}
\caption{Users that are the boundary cases in the simulation.}
\label{exp.tblextremeusers}
\end{table}
In addition to the values for $u$ and $i$, this table also states the number of simulations that were run for each user.
For example, the ``Always Install'' user had 30 simulations run, that is 30 CUDF* documents were created and resolved.
The number of simulations run for each user is a trade-off between time to complete the simulations and accuracy of results.
This number was chosen based on the randomness of each user (as described in section \ref{sim.randomness}) 
as well as the experience gained when developing the simulation as to the variability of the results.
The ``Always Upgrade'' and ``Control'' users have no randomness therefore only one user is required to be simulated.

\subsubsection{Results and Analysis}
The first of the effects that will be explored is the out-of-dateness for each of the simulated users.
This is measured using the up-to-date distance (UTTD) function, as described in section \ref{impl.criteria}.
This measurement is the number of components that are a greater version than a component currently installed.
A problem that exists with this measurement is that it does not take into account the size of a system.
As a system's size increases, the UTTD will grow as there will be more components that become out-of-date.
To normalise this effect the measurement UTTD per component (UTTDpC) is defined as the UTTD per installed component.
The UTTDpC of each simulated user is presented in figure \ref{exp.q1auttdpc}.
\begin{figure}[htp]
\begin{center}
  \includegraphics[width=\textwidth]{plots/q1auttdperc}
  \caption{The Up-to-date distance per Component (UTTDpC) of the simulated systems.}
  \label{exp.q1auttdpc}
\end{center}
\end{figure}

This figure shows that all systems eventually become out-of-date.
This is due to constraints in components that cannot be removed that restrict change. 
It is also partially due to the \texttt{apt-get} upgrade criteria minimising new components to be installed.
This stops a component upgrading if it requires a component that is not installed in the system.
Section \ref{exp.prouttdsection} presents experiments where changing this criteria removes this barrier to new components. 

This figure also shows that users which do not upgrade, the ``Control'' and ``Always Install'' users, become nearly three times more out-of-date than users that upgrade.
The speed at which systems become out-of-date increases over the months between March and May 2010.
The reason for this is likely due to the increased development effort because of the Ubuntu 10.04 release in April 2010.
The users that upgrade are less affected by this release as they are installing newer component versions.

The rate of change of the ``Control'' users UTTDpC is the rate at which components evolve.
This is because the faster versions of a component are released, the more the ``Control'' user becomes out-of-date.
As this user will never upgrade it's UTTD is increased by one for every new component release.

The effect of the normalisation of UTTD can also be seen in this figure.
The ``Control'' and ``Always Install'' users, and the ``Always Upgrade'' and ``Always Upgrade\&Install'' users have similar UTTDpC over the year.
This normalisation means that the install variable $i$ has little effect on the UTTDpC of a component system.


The next effect measured is how much change each simulated system went through during evolution.
To measure change the ``change'' function as described in section \ref{impl.criteria} is used.
The total change, i.e. the sum of all change a system has to a date, is presented in figure \ref{exp.q1achange}.
\begin{figure}[htp]
\begin{center}
  \includegraphics[width=\textwidth]{plots/q1achange}
  \caption{The total change of the simulated systems.}
  \label{exp.q1achange}
\end{center}
\end{figure}

As was predicted above, this figure shows that there is an increase in change during the release of Ubuntu 10.04 for users that upgrade their systems.
During the months of March, April and May of 2010 the ``Always Upgrade'' user had a mean of 266 changes per month.
All other months they had a mean of 105 changes per month.
This is a more than 250\% increase in change over the months of the release.
This increased change cannot be seen in the ``Always Install'' users as they will not change in response to newer versions.

The effect of reuse of components can be most seen when examining the ``Always Install'' users.
Initially, these user's systems change quickly, then after a month change rate decreases.
This can be seen with the mean change per day during the first month it is 4.9, where the final 11 months it is 3.0.
After further investigation this reduction in change is explained due to the common reuse of a few components like \texttt{libaudio2} and \texttt{libqt3-mt}.
Many components depend on these components, and are therefore often installed in the first month.
After this installation they are no longer required to be added so the future change is reduced.
If no component reused such components the rate of change would remain near 4.9 components per day, resulting in an estimated total change of 1788.5 over a year.
Reuse then saves an estimated 650 changes over the year.

The ``Always Upgrade\&Install'' users changes more than the combined amount of ``Always Upgrade'' and ``Always Install'' changes.
The mean total change of the ``Always Upgrade'' user is 1655, the ``Always Install'' user is 1137 and the ``Always Upgrade\&Install'' user is 3482.
This shows that a user that upgrades and installs changes more than 700 components than the combined amount changed by just installing and just upgrading.
The additional change is due to the installed components increasing the amount of components to be upgraded.


\subsection{Failures}
These initial experiments are the most extreme users that can be simulated.
This means phenomena that occur rarely are most observable in these simulations.
For this reason, these simulations were chosen to be closely inspected for various failures.

Two types of failure were observed in the results of these simulations:
\begin{enumerate}
  \item \textbf{Hard Failure}: Where a request has no satisfactory solution, therefore no change is made to the system.
  \item \textbf{Soft Failure}: The request is satisfied, however the search was interrupted and the best solution found at that time was returned.
\end{enumerate}
Additionally, a notable hard failure was observed, the \textbf{Multi-component Failure} occurs when multiple components must be installed to satisfy a request. 

%%%Hard failures
Hard failures where observed to occur in 24 of the 60 ``Always Install'' and ``Always Upgrade\&Install'' users, and never in the ``Always Upgrade'' user.
Only 40 total failed requests occurred over these 24 simulated users.
All hard failures, except those that are multi-component failures, were failed install requests.
Given each ``Always Install'' user has 365 requests and each ``Always Upgrade\&Install" user has 730 requests, 
and 30 users of each type were simulated, the chance for a request to fail is just over 0.12\%.
The chance for a hard failure to occur is extremely low in this simulation.

Due to the difficultly of finding the constraints that caused these failures \citep{quickxplain},
and the rarity of these failures, the specific reasons for hard failures are not further explored.

%%%Soft Failure TODO
Soft failures are called ``soft'' because the request that causes them succeeds, however the search failed to finish.
This occurs when the time-limit of two minutes is reached when searching for an optimal solution, and the anytime algorithm employed by GJSolver is interrupted.
This does not mean that the returned solution is not optimal\footnote{In MISC GJSolver was interrupted many times and most times returned the optimal solution},
it just means that it is uncertain whether or not the returned solution is optimal.
Of the 33215 requests during the simulation of the users defined in table \ref{exp.tblextremeusers} only 900 requests suffered soft failures.
This is less than 3\%.
After looking at the results returned by these 900 requests only 8 were determined to be detrimental to the simulation.
Each of these 8 requests caused more than 100 components in the system to be removed in order to be satisfied.
This is due to the criterion \texttt{-removed} not being fully optimised before being interrupted.
There are three interesting points that were observed with these 8 requests:
\begin{enumerate}
  \item seven of them occurred during the Ubuntu 10.04 release month of April 2010, where the remainder of the soft failures are evenly distributed.
  \item five times the component that was requested to be installed was removed by the following requests.
\end{enumerate}
The first point implies that these detrimental soft-failures occur because of the increased release of components during the Ubuntu release.
The second point implies that the components themselves are over constrained and thus removed in future systems.
The effect of these detrimental soft failures can be seen in figure \ref{exp.q1achange} 
where the standard deviation of change for both ``Always Install'' and ``Always Upgrade\&Install'' users increases after the Ubuntu 10.04 release.
Given that only 8 such failures occurred, this is seen as a minor impact on the validity of the simulation.
These failures may be reduced or removed by increasing the time before the algorithm is interrupted, or by using more powerful hardware to perform the simulations.
In future experiments detrimental soft failures are searched for, however none are found.

%%%The multi component fialure
An interesting failure occurred when the situation arose that installing the package \texttt{chromium-browser} required two versions of the package \texttt{libc6} to be installed. 
This lead to the following upgrade request to be unsatisfiable, and the following install request to remove \texttt{chromium-browser} and upgrade to the newest version of \texttt{libc6}.
This instance, although rare, shows the case that at some points the hard constraint that Debian enforces to have only one version of each component installed,
can restrict the user.

%%%Disclaimer
All these failures are directly caused by, or are a result of an install request.
The failures and the rates they occur are therefore a product of the selection of components to be installed.
As this is the least valid part of the simulation (discussed in section \ref{sim.modelvalidation}), the results in this section should be tempered by this knowledge. 


\subsection{Upgrade Probability Effects}
By altering only the probability a user upgrades their system the effect of the user upgrading can be measured.
The users simulated are described in table \ref{exp.tblq1busers}.
\begin{table}[h!]
\centering
\begin{tabular}{|l | c | c | c |}
\hline
User Name 				& \# simulated 	& $u$ 		& $i$ 			\\ \hline
Upgrade once a month	& 10 			& 0.03 (1/30)			& 0				 \\
Upgrade twice a month	& 10 			& 0.06 (1/15)		& 0				\\
Upgrade once a week		& 10 			& 0.14 (1/7)		& 0				 \\
Upgrade twice a week 	& 10 			& 0.29 (1/3.5)		& 0				\\ \hline
\end{tabular}
\caption{Users with different upgrade probabilities.}
\label{exp.tblq1busers}
\end{table}
These users are compared to the ``Always Upgrade'' user described in table \ref{exp.tblextremeusers}.

The up-to-date distance per component for these users is compared in figure \ref{exp.q1buttdperc}.
\begin{figure}[htp]
\begin{center}
  \includegraphics[width=\textwidth]{plots/q1buttdperc}
  \caption{The Up-to-date distance per Component (UTTDpC) of the simulated systems.}
  \label{exp.q1buttdperc}
\end{center}
\end{figure}

This figure shows that upgrading less frequently has it's greatest effect over the release of the Ubuntu 10.04.
This is most noticeable in the ``Upgrade once a month'' users.
The increased release of newer versions of components makes systems that do not frequently upgrade quickly become out of date.

The mean UTTDpC of the simulated users plotted against their frequency ($1/u$) of upgrading is presented in figure \ref{exp.q1bcorrolationuttdpc}.
\begin{figure}[htp]
\begin{center}
  \includegraphics[width=\textwidth]{plots/q1bcorrolationuttdpc}
  \caption{The mean up-to-date distance per component (UTTDpC) plotted against of the frequency a user upgrades ($1/u$)}
  \label{exp.q1bcorrolationuttdpc}
\end{center}
\end{figure}

The figure shows the mean UTTDpC of ``Always Upgrade'' is 0.258 and the ``Upgrade twice a week'' users mean is 0.267.
This is a difference of 0.009 UTTDpC.
This difference can be illustrated with an example; given a system of 1000 components, 
a user that upgrades twice a week they will have on average 9 components more out-of-date than if they upgraded every day. 
This figure also shows the diminishing return when increasing the amount a user upgrades.
As a user upgrades more frequently, the amount of out-of-dateness their system is reduces.
So the difference in UTTDpC between upgrading monthly compared to twice a month is far greater than upgrading weekly compared to twice a week.

The total change of these users is presented in figure \ref{exp.q1bchange}.
\begin{figure}[htp]
\begin{center}
  \includegraphics[width=\textwidth]{plots/q1bchange}
  \caption{The mean total change plotted against of the simulated users.}
  \label{exp.q1bchange}
\end{center}
\end{figure}
This figure shows that upgrading is causes significantly more change in the system than installing.
The total change of a ``Upgrade once a month" user has a greater total change than a ``Always Install'' user.

This figure also shows that upgrading less frequently requires less change. 
This is in part due to the fact that updating once a month means that the majority of the month there is no change.
However, under further analysis another effect was noticed that reduced change of the users that upgraded less frequently.
This occurs when many versions of a component is released in quick succession, making upgrading to each interim version not required.
For example, if a component releases two versions within a week, a user who upgrades every day will upgrade twice and a user that upgrades at the end of the week will only upgrade once.
An example of this rapid release of components happening in the Ubuntu repository is given in figures \ref{exp.apachelog} and \ref{exp.apachebug}.

\begin{figure}[h!]
\begin{center}
\begin{alltt}
apache2 (2.2.20-1) unstable; urgency=low

  * New upstream release.
  * Fix some regressions related to Range requests caused by the CVE-2011-3192
    fix. Closes: #639825
\ldots

 -- Stefan Fritsch <sf@debian.org>  Sun, 04 Sep 2011 21:50:22 +0200

apache2 (2.2.19-2) unstable; urgency=high

\ldots

 -- Stefan Fritsch <sf@debian.org>  Mon, 29 Aug 2011 17:08:17 +0200
\end{alltt}
\caption[Apache Changelog]{An extract from the apache changelog located on http://changelogs.ubuntu.com/}
\label{exp.apachelog}
\end{center}
\end{figure}

\begin{figure}[h!]
\begin{center}
\begin{alltt}

Reported by: Takis Issaris <takis.issaris@uhasselt.be>
Date: Tue, 30 Aug 2011 16:09:01 UTC
Severity: important
Found in versions 2.2.9-10+lenny10, 2.2.16-6+squeeze2, apache2/2.2.19-2

\ldots

Package: apache2.2-common
Version: 2.2.9-10+lenny10

Yesterday evenings update broke our Apache server setup,
\ldots
\end{alltt}
\caption[Apache Bug Report]{Extract from the bug report \#639825, filed with Debian}
\label{exp.apachebug}
\end{center}
\end{figure}

A summary of these events are:
\begin{enumerate}
  \item Apache developer Stefan Fritsch released a new version of their server, \texttt{apache2}/2.2.19-2, on 29 Aug 2011.
  \item Takis Issaris upgraded to this version which broke his system on 29 Aug 2011.
  \item Takis Issaris submited a bug report on 30 Aug 2011, where he and Stefan Fritsch discuss the causes.
  \item On 04 Sep 2011 (5 days after the initial bug report) Stefan Fritsch releases a new version 2.2.20-1 that fixes this bug.
\end{enumerate}
In this example, the maintenance of the component \texttt{apache2} caused the release of two versions within a week of one another.

The amount of change this effect causes can be found out by looking at the ``Always Upgrade'' user.
This user upgraded the same component within seven days on 23 different occasions.
This may not seem like an insignificant amount of change, especially since the ``Always Upgrade'' user changes over 1500 components during the year.
However, these changes may introduce bugs (as with the above example) and are an unnecessary risk to the users system.
Additionally, as a user installs more components this will likely increase the amount this type of change occurs.

By updating less frequently the change caused by rapid releasing the components is reduced.
However, this reduction is by chance alone.
For example, if a user upgrade once a month then they will likely miss many of the rapid releases that happened during the month.
Such a user may upgrade during such a release, and therefore have a component (potentially with bugs) until they upgrade again in a month.

To actively reduce this type of change a novel criterion is presented in section \ref{exp.q3}.

\subsection{Install Probability Effects}
By altering the probability a user installs a component into their system, the effect this has on 
change can be studied\footnote{As discussed before, the probability to install will have little effect on UTTDpC. Therefore UTTDpC is not studied here.}.
The users studied are described in table \ref{exp.tblq1cusers}.
\begin{table}[h!]
\centering
\begin{tabular}{|l | c | c | c |}
\hline
User Name 				& \# simulated 	& $u$ 		& $i$ 			\\ \hline
Install once a month	& 30 			& 0 & 0.03 (1/30)							 \\
Install twice a month	& 30 			& 0 & 0.06 (1/15)						\\
Install once a week		& 30 			& 0 & 0.14 (1/7)					 \\
Install twice a week 	& 30 			& 0 & 0.29 (1/3.5)						\\ \hline
\end{tabular}
\caption{Users with different upgrade probabilities.}
\label{exp.tblq1cusers}
\end{table}
These users are compared to the ``Always Install'' users described in table \ref{exp.tblextremeusers}.

The total change of these users is presented in figure \ref{exp.q1cchange}.
\begin{figure}[htp]
\begin{center}
  \includegraphics[width=\textwidth]{plots/q1cchange}
  \caption{The mean total change plotted against of the simulated users.}
  \label{exp.q1cchange}
\end{center}
\end{figure}
This figure shows the range of possible change given the probability to install a component.
It also shows the change is inversely proportional to the frequency of installing, 
i.e. ``Install twice a week'' users change twice as much as ``Install once a week'' users.

Further analysis is not necessary as these users are entirely influenced by the selection of components to install, which is the least valid part of the simulation (as discussed in section \ref{sim.randomness}).

\section{Reduction of Out-of-dateness During Evolution}
\label{exp.q2}
\label{exp.prouttdsection}
In the previous section the criteria used during evolution were the criteria used by \texttt{apt-get}.
This criteria limits the up-to-dateness of the system by prioritising the minimisation of installing new components.
Altering this criteria from \texttt{-removed,-new,-uptodatedistance} to \texttt{-removed,-uptodatedistance,-new} 
will let progressive users, who prioritize up-to-dateness over change upgrade their system.
The simulated user is described in table \ref{exp.tblq4ausers}.
\begin{table}[h!]
\centering
\begin{tabular}{|l | c | c | c |}
\hline
User Name 								& \# simulated 			& $u$ 		& $i$ 			\\ \hline
Progressive Always Upgrade				& 1 			 	& 1				& 0\\ \hline
\end{tabular}
\caption{Progressive Users with different upgrade probabilities.}
\label{exp.tblq4ausers}
\end{table}
This user has the upgrade criteria $U$ assigned to \texttt{-removed,-uptodatedistance,-new}.
It is compared to the ``Always Upgrade'' user that is the same except it's upgrade criteria $U$ is assigned \texttt{-removed,-new,-uptodatedistance}.

Figure \ref{exp.q4auttdperc} presents the UTTDpC of these users.
\begin{figure}[htp]
\begin{center}
  \includegraphics[width=\textwidth]{plots/q4auttdperc}
  \caption{The UTTDpC plotted for the simulated users.}
  \label{exp.q4auttdperc}
\end{center}
\end{figure}

This figure shows that the progressive criteria reduces the out-of-dateness of the systems.
The final UTTDpC for the ``Always Upgrade'' user 0.72 compared to the final UTTDpC for the ``Progressive Always Upgrade'' user 0.48.
That is, a user who upgrades with the progressive criteira will have a system that is 24\% less out-of-date.

The UTTDpC of these systems is most differentiated during the release of Ubuntu 10.04.
At this time new versions of components are being released quickly.
The ``Progressive Always Upgrade" user can upgrade these components if they require new components to be installed where the ``Always Upgrade'' user cannot.

These are promising results, showing that it is possible to reduce the out-of-dateness of a system.
However, this comes at the cost of increased change.
This additional change comes from the 90 new components that the progressive change installs.
These new components are also upgraded 258 times throughout the year.
The total additional change when using a progressive user is therefore 348 changes over a year.  

When a user upgrades their system with the progressive upgrade criteria at the end of the year there system is 24\% less out-of-date at the cost of an additional 348 changes.
Users that prefer to have an up-to-date system may favour this criteria over the \texttt{apt-get} criteria. 
This criteria is further simulated in section \ref{exp.realuserssim}.

\section{Reduction of Change During Evolution}
\label{exp.q3}
When versions of a component are released in a small amount of time can cause unnecessary change when upgrading.
To reduce this change a criterion is defined that does not upgrade to a component until it has become \textit{stable}.
A component is stable if no better version is released within a certain amount of time after its release.
The $stable$ function is defined to return \texttt{true} if a component is stable.
\begin{defs}
The function $stable$ takes a component $c$, a number of days $d$, and the current time $t$ and returns \texttt{true} iff:
\begin{enumerate}
  \item the component was not released within $d$ days of time $t$.
  \item no component was released within $d$ days after $c$ was released that has the same name and a greater version than $c$.
\end{enumerate}
\end{defs}
The first part of the definition ensures that the function waits $d$ days before returning that the component is stable.
The second part ensures that if a better version is released within $d$ days, the component is returned as not stable.

A criterion using this function can then be defined:
\begin{defs}
	Given the set of components $\mathbb{C}_t$ and number of days $d$, the \textbf{stable version} criterion is defined as $crit_{sv} = \langle rank^{sv}_{\alpha}, \geq \rangle$,
	where \\$rank^{sv}_{\alpha}(\beta) = \sum_{c \in \beta} \begin{cases} 1000 + uptodatedistance(c,\mathbb{C}_t)& \text{not }stable(c,d,t) \\ uptodatedistance(c,\mathbb{C}_t) & \text{otherwise} \\  \end{cases}$
\end{defs}
That is the ranking function $rank^{sv}_{\alpha}(\beta)$ returns the UTTD of the component if it is stable,
else it returns 1000 added to the UTTD.
This additional weight of 1000 is added to encourage the system to only install stable components.

The mapping between this criterion, MOF and the PB criteria is presented in table \ref{exp.stablcritmapping}, with a full description in appendix \ref{apx.critmapping}.
\begin{table}[htp]
\begin{tabular}{c | c | c}
\textbf{MOF} 		& \textbf{\modelname criterion} & \textbf{PB criterion} \\
\texttt{-stableversion(}$d$\texttt{)} & $crit_{sv} = \langle rank^{sv}_{\alpha}, \geq \rangle$ & $\langle f_{sv}, <, I_{sv} \rangle$ \\
\end{tabular}
\caption{Mapping between these elements}
\label{exp.stablcritmapping}
\end{table}
In this mapping the amount of days $d$ is left blank, allowing a range of different values to be simulated.

The users simulated are described in table \ref{exp.tblsvusers}.
\begin{table}[h!]
\centering
\begin{tabular}{|l | c | c | c | }
\hline
User Name 				& \# simulated 	& $u$ 	& $U$ 			\\ \hline
7 Days SV Upgrade		& 1 			& 1 & 	\texttt{-removed,-new,-stableversion(7)}			 \\
14 Days SV Upgrade		& 1 			& 1 & 	\texttt{-removed,-new,-stableversion(14)}			\\
21 Days SV Upgrade		& 1 			& 1 & 	\texttt{-removed,-new,-stableversion(21)}			 \\
28 Days SV Upgrade 		& 1 			& 1 & 	\texttt{-removed,-new,-stableversion(28)}			\\ \hline
\end{tabular}
\caption{Users with different upgrade probabilities.}
\label{exp.tblsvusers}
\end{table}
These users are compared to the ``Always Upgrade'' user described in table \ref{exp.tblextremeusers}.

The UTTDpC of these simulated users are presented in figure \ref{exp.q5auttdperc}.
\begin{figure}[htp]
\begin{center}
  \includegraphics[width=\textwidth]{plots/q5auttdperc}
  \caption{The UTTDpC plotted for the simulated users.}
  \label{exp.q5auttdperc}
\end{center}
\end{figure}

This figure shows the cost of using this criteria which is that the system is $d$ days out-of-date while the criterion waits for the components to stabilise.
The difference between the mean UTTDpC of the ``Always Upgrade'' user and the user ``7 Days SV Upgrade" is 0.025 UTTDpC.
This increases linearly with the other users; ``14 Days SV Upgrade'' is 0.05, ``21 Days SV Upgrade'' is 0.078, and ``28 Days SV Upgrade'' is 0.104.
The cost of waiting for the components to become stable is 0.0036 UTTDpC per day.

The reduction in change for these users are described in table \ref{exp.tblsvchange}.
This is measured by subtracting the total change of the ``Always Upgrade'' user $d$ days before the end of the simulation
from the total change of the  ``$d$ Days SV Upgrade'' user.
This calculation takes into account the delay in upgrading caused when waiting for components to become stable.
This table also includes the estimate from the ``Always Upgrade'' user.
This is a total of the amount of occurrences the ``Always Upgrade" user upgraded the same component within $d$ days.
\begin{table}[h!]
\centering
\begin{tabular}{|l | c | c | }
\hline
User Name 				& Reduction in change 	& ``Always Upgrade'' estimate		\\ \hline
7 Days SV Upgrade		& 23 					& 23 			 \\
14 Days SV Upgrade		& 29	 				& 31			\\
21 Days SV Upgrade		& 41 					& 46 			 \\
28 Days SV Upgrade 		& 59 					& 68			\\ \hline
\end{tabular}
\caption{Users with different upgrade probabilities.}
\label{exp.tblsvchange}
\end{table}

This table shows that the reduction in change grows quickly.
However, increasing the amount of days that is waited for a component to become stable more than a month may start to detrimentally effect a component system.

Another aspect this table shows is the estimate of this change from the ``Always Upgrade'' user.
This estimate differs from the actual reduction due to the stable version criterion because of components waiting to become stable at the end of the simulation.
This estimate is later used to determine the amount of change that could be reduced for a simulated user.
Using this estimate to calculate the potential saved change when using the stable version criterion is preferred 
as simulation is an expensive task and with such an accurate estimator it is deemed unnecessary. 

\section{Realistic Evolution}
\label{exp.realuserssim}
During the previous experiments the probabilities a user upgrades and installs have been assigned values that may not be ``realistic''.
This section presents the experiment where these variables are assigned values extracted from submitted user's \texttt{apt-get} logs.
Four such users are defined, ``High Install'', ``High Upgrade'', ``Medium Change'' and ``Low change''.
These users are then assigned the \texttt{apt-get} criteria, and the ``Progressive Upgrade'' criteria and simulated.
Using the estimate described in the previous section, the potential reduction in change for these users if they used the stable version criterion is measured.

This section first describes the method used to extract the information from the \texttt{apt-get} logs and define the users variables.
The results from the simulation of these users is then discussed and analysed.

\subsection{Extracting Information from the User Submitted Logs}
As previously discussed in chapter \ref{simulation}, during the conducted survey users where asked to submit their \texttt{apt-get} logs.
Nineteen logs were submitted of users using \texttt{apt-get} on either Debian or Ubuntu systems over a period between 23 and 277 days long. 
In section \ref{impl.validation} these logs were used to validate the simulations output.
This section describes how these logs are parsed to measure the probability a user upgrades and installs a component.
Through using the k-means clustering algorithm, four general users are extracted and described.

To give an example of the information included in an \texttt{apt-get} log, an extract is shown in figure \ref{aptlog}.
\begin{figure}[htp]
\begin{center}
\begin{alltt}
\ldots
Start-Date: 2010-12-21 11:32:28
Install: libnet-daemon-perl (0.43-1), libhtml-template-perl (2.9-1), libdbi-perl (1.609-1build1), mysql-client-core-5.1 (5.1.41-3ubuntu12.8), libdbd-mysql-perl (4.012-1ubuntu1), mysql-server-5.1 (5.1.41-3ubuntu12.8), mysql-client-5.1 (5.1.41-3ubuntu12.8), libmysqlclient-dev (5.1.41-3ubuntu12.8), libplrpc-perl (0.2020-2), mysql-server-core-5.1 (5.1.41-3ubuntu12.8), mysql-server (5.1.41-3ubuntu12.8), libmysqlclient16-dev (5.1.41-3ubuntu12.8)
Upgrade: mysql-common (5.1.41-3ubuntu12.6, 5.1.41-3ubuntu12.8), libmysqlclient16 (5.1.41-3ubuntu12.6, 5.1.41-3ubuntu12.8)
End-Date: 2010-12-21 11:33:03
\ldots
\end{alltt}
\caption[APT log extract]{An extract of an APT log file}
\label{aptlog}
\end{center}
\end{figure}

These logs describe the changes made to the system by \texttt{apt-get}, and not necessarily what the user requested to cause the change.
However, using the constraints that \texttt{apt-get} employs to make changes, the user request can be extracted.
These constraints are:
\begin{itemize}
  \item \texttt{apt-get} will never install or remove a component if the system is upgraded.
  \item \texttt{apt-get} will only install a package if one has been selected to be installed.
\end{itemize}
Using these constraints each log is processed and the dates a user upgraded or installed a component are extracted.
With this information the probability that a user upgrades or installs a component on a given day is calculated.
A k-means algorithm is used to cluster and find the center points for four types of users.
The results of this process are presented in figure \ref{exp.figuserlogs} and table \ref{exp.tbluserlogs} .

\begin{figure}[htp]
\begin{center}
  \includegraphics[width=\textwidth]{plots/userlogAnalysis}
  \caption{The Extracted Upgrade and Install probabilities of the Submitted Logs}
  \label{exp.figuserlogs}
\end{center}
\end{figure}

\begin{table}[h!]
\centering
\begin{tabular}{|l | c | c | }
\hline
User Name 				& 	$u$ 		& $i$ 		\\ \hline
High Install (HI)			& 0.422			& 0.764 	\\
Medium Change (MC)			& 0.222			& 0.259 	\\
High Upgrade (HU)			& 0.561			& 0.192		\\
Low Change 	(LC)			& 0.186			& 0.086 	\\ \hline
\end{tabular}
\caption{Extracted Users from submitted \texttt{apt-get} Logs}
\label{exp.tbluserlogs}
\end{table}

\subsection{Simulation of Realistic Users}
\label{exp.q4}
Above some realistic users are created by extracting information from user submitted logs.
These users are simulated using the \texttt{apt-get} upgrade criteria \texttt{-removed,-new,-uptodatedistance} and the progressive upgrade criteria \texttt{-removed,-uptodatedistance,-new}.
The progressive users names are prefixed  with ``Pro. Upgrade'' where the users using the \texttt{apt-get} criteria are not changed.

The UTTDpC of these users are presented in figure \ref{exp.q6uttdpc} and table \ref{exp.tblq6uttd}.
\begin{figure}[htp]
\begin{center}
  \includegraphics[width=\textwidth]{plots/q6usersuttd}
  \caption{The UTTDpC plotted for the simulated users.}
  \label{exp.q6uttdpc}
\end{center}
\end{figure}

\begin{table}[h!]
\centering
\begin{tabular}{|l | c | c | c | }
\hline
User Name 				& \texttt{apt-get} criteria & Progressive criteria 	& \% Less UTTDpC	\\ \hline
High Install (HI)			& 0.798				& 0.433 			& 46\%	\\
Medium Change (MC)			& 0.795				& 0.633 			& 20\% 	\\
High Upgrade (HU)			& 0.826				& 0.418				& 49\%  \\
Low Change 	(LC)			& 0.773				& 0.620 			& 24\%   \\ \hline
\end{tabular}
\caption{The final UTTDpC when using different upgrade criteria}
\label{exp.tblq6uttd}
\end{table}

This figure shows that using the \texttt{apt-get} upgrade criteria results in a system that after a year of simulation is about 0.8 UTTDpC out of date for all users. 
The only noticeable difference between these users is that ``High Install'' users systems take longer to become up-to-date after the Ubuntu 10.04 release.


The users ``Pro. Upgrade High Install'' and ``Pro. Upgrade High Update'' benefit the most from using the progressive upgrade criteria.
These two users are about 0.42 UTTDpC at the end of the simulations.
This is compared to the other two progressive users who are about 0.62 UTTDpC out-of-date.
These results show that using the progressive criteria has the most benefit for users that upgrade the most.      

The total changes of these users are presented in figure \ref{exp.q6userchange} and table \ref{exp.tblq6change}.
\begin{figure}[htp]
\begin{center}
  \includegraphics[width=\textwidth]{plots/q6userchange}
  \caption{The Total Change for the simulated users.}
  \label{exp.q6userchange}
\end{center}
\end{figure}

\begin{table}[h!]
\centering
\begin{tabular}{|l | c | c | c | }
\hline
User Name 				& \texttt{apt-get} criteria 		& Progressive criteria 	& \% Added Change	\\ \hline
High Install (HI)			& 3176			& 4043 	& 27\% 	\\
Medium Change (MC)			& 2291			& 2660 	& 16\% 	\\
High Upgrade (HU)			& 2123			& 2842	& 34\%   \\
Low Change 	(LC)			& 1903			& 2212 	& 16\%   \\ \hline
\end{tabular}
\caption{The mean total change when using different upgrade criteria}
\label{exp.tblq6change}
\end{table}

This figure shows the additional change required when using the progressive criteria.
This figure shows that the total change of  ``High Install'' users for both criteria are significantly more than the other users.
Additionally it shows that the ``High Upgrade'' user's total change is significantly increased when using the progressive criteria.
This can be seen in the final months of the simulation where the rate of change increases due to the upcoming release of Ubuntu 10.10.

The reduced change when using the stable version criteria can be estimated for each of these users.
This is accomplished by counting the amount of times each user upgrades the same component within $d$ days.
These results are presented in table \ref{exp.tblq6sv}.
\begin{table}[h!]
\centering
\begin{tabular}{|l | c | c | c | c |}
\hline
User Name 	& $d$ = 7 	&  $d$ = 14 	&	$d$ = 21 	&	$d$ = 28 	\\ \hline
HI 			&24.58 	&	43.24&	 67.22&	 93.88\\
HU 			&19.08 	&	31.32&	 48.4&	 70.52\\
MC 			&11.54 	&	26.98&	 45.14&	 65.1\\
LC 			&8.5 	&	22.54&	 38.48&	 53.56\\
Pro. HI 	&30.72 	&	57.0&	 85.52&	 117.52\\
Pro. HU 	&32.02 	&	56.2&	 79.62&	 110.1\\
Pro. MC 	&14.9 	&	30.28&	 48.28&	 67.66\\
Pro. LC 	&12.4 	&	24.84&	 41.3&	 59.4\\ \hline
\end{tabular}
\caption{The estimated mean reduced change when using the stable version criteria}
\label{exp.tblq6sv}
\end{table}

This table shows that the ``High Install'' and ``High Upgrade'' users will have the most benefit using the stable version criterion.
This is likely due to their high probability to upgrade their systems.
However, users that upgrade less frequently, e.g. the ``Low Change'' users, are less likely to be concerned with their systems out-of-dateness and may prefer a higher value for $d$.
This would increase the benefit for these user's as well.

\section{Answers}
The above performed experiments were conducted in order to answer the questions:
\begin{enumerate}
  \item What consequences do a user's choices have on their system?
  \item Can the out-of-dateness of a system be reduced?
  \item Can the change of a system be reduced?
  \item How do the systems of realistic users evolve?
\end{enumerate}
These questions are addressed here.

\subsection{User Requests}
The notable consequences a users requests to change their system were measured through the experiments in section \ref{exp.q1}.
These experiments simulated users using the criteria \texttt{apt-get} uses to alter systems.
These consequences are summarised as:
\begin{itemize}
	\item A system will always become more out of date due to:
  		\begin{itemize}
  			\item the internal constraints of the components hindering the upgrading of components.
  			\item the criteria of \texttt{apt-get} stopping the installation of new packages to enable the upgrading of an installed component.  
		\end{itemize}
	\item The majority of change during evolution is caused by a user upgrading.
	Installing new components increases the amount of change when upgrading.
	\item Systems become out-of-date at the rate at which components evolve. Components evolve at a higher rates during release cycles.
	\item Reuse decreases the rate of change during CSE.
	This is due to the two effects; 
	reuse decreases the installation rate of components and this decreases the amount of components necessary to be upgraded.   
	\item There were two types of failures observed to occur:
		\begin{itemize}
  			\item Hard failures: where the constraints of a request could not be satisfied. These occurred 0.12\% during the simulation.
  			\item Soft failures: where the constraints were satisfied, though the returned solution was possibly not optimal.
  				These occurred in 3\% of the requests, though only 1\% of these were determined to be detrimental to the system.
  				Such detrimental soft failures occur more frequently during release cycles when components are evolving at a higher rate.
		\end{itemize}
	\item Increasing the frequency of upgrading has depreciating returns on reducing a systems out-of-dateness. 
	It may also increase change due to components being repeatedly upgraded if they are quickly released. 
\end{itemize}

\subsection{Reduce Out-of-dateness}
As noted above, a system becomes out-of-date partially due to the \texttt{apt-get} criteria not allowing new components to be installed.
To remove this restriction the \texttt{apt-get} criteria was altered to the ``progressive'' criteria.
Using this criteria to upgrade a system every day was shown to decrease the out-of-dateness by 24\% at the cost of increasing change by 21\%.
This additional change was due to the new components being installed and upgraded during the simulation.

\subsection{Reduce Change}
Some of the change caused when upgrading a system was due to rapidly released components being repeatedly upgraded.
For a user that upgrades every day, there were 23 occasions were a component being upgraded twice within a week.
This is unnecessary change that introduces risk into a system. 
The stable version criterion was defined to reduce this change by waiting for a component to become stable over a period of days.
The cost of using the stable version criteria was the system remains out-of-date by the number of days waited for components to become stable.
However, through using the stable version criteria waiting a week for components to become stable all 23 instances of this unnecessary change were removed.

\subsection{Real Users}
To simulate realistic users the probability a user upgrades and installed were extracted from user's submitted logs.
Using the k-means algorithm four stereotypes of user were created; `High Install'', ``High Upgrade'', ``Medium Change'' and ``Low change''.
These users were simulated using the \texttt{apt-get} criteria and the progressive criteria, and the lowered change from using the stable version criterion was estimated.
This has shown that the greatest benefits for using the progressive criteria and the stable version criterion is for the ``High Install'' and ``High Upgrade'' users.
This is due to their higher frequency they upgrade their systems.

\section{Summary}
This chapter presented a series of experiments that simulated various users to study CSE.
These experiments explored the consequences of users changing their systems, and the novel proposed progressive and stable version criteria.
The next chapter describes the conclusions, the related work, and the potential future research of CSE.



