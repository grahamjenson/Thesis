\chapter{Component Dependency Resolution}
\label{implementation}
\epigraph{What I cannot create, I do not understand.}
{\textit{Richard Feynman, 1988}}

%%%CDR Has been shown to be NP-Complete to find a solution and NP-Hard to find an optimial one.
Finding any solution to an evolution problem is NP-Complete.
Given this difficulty of finding an optimal solution, any CDR implementation which attempts to automate the process must be efficient.

%%%Formally it resembles the SAT problem, which already has efficient implementations, and has been used to solve this problem
To create an efficient CDR implementation a common solution, in both academia \citep{abate2011} and industry \citep{leBerre2010}, 
is to map the evolution problem to a Boolean Satisfiability Problem (SAT),
and represent the criteria as a set of  pseudo-Boolean (PB) constraints \citep{dixon2004automating}.
Many fast and robust solvers exist that solve SAT and PB constraints.
Through using such solvers to create CDR implementations, validated and tested solutions can be reused.  

%%%In this chapter\ldots
In this chapter the algorithms that CDR uses to find optimal solutions are defined and discussed.  
The Davis-Putnam-Logemann-Loveland (DPLL) algorithm \citep{Davis1960, davis1962machine} is described as it is the basis for many current SAT solvers.
More recent advancements in SAT solver technology are discussed, including its extension to handle pseudo-Boolean constraints.
These discussions are intended to give a basic understanding of the internal workings of the SAT solver used at the core of many CDR implementations.

The CDR implementation, called GJSolver, created through this research is presented in section \ref{impl.gjsolver}.
The specific requirements that GJSolver was developed towards, and the implementation decisions and compromises that were made are also described.

This chapter ends with a discussion of the process of verification of GJSolver through the Mancoosi International Solver Competition (MISC).  
This competition involved the comparison of GJSolver against other current solvers over a series of evolution problems.

\section{Boolean Satisfiability Solvers}
\label{impl.SAT}
Boolean satisfiability (SAT) is the problem of determining if the variables in a Boolean equation can be assigned in such a way that the equation returns true.
SAT was the first identified NP-Complete problem, meaning there is no known algorithm that efficiently solves all instances of SAT problems.
The fundamental difficulty of the SAT problems, 
combined with the ability to map many problems to SAT has spawned a community\footnote{http://www.satcompetition.org/ accessed 6/3/2012} 
dedicated to creating and testing various SAT solver implementations. 
Such SAT solvers have been used in various domains to tackle problems such as electronic design automation \citep{Marques-Silva2000}, 
model verification \citep{dennis2006}, and, of course, component system evolution \citep{leBerre2010}.

%%%In this section a common implementation of SAT solvers is discussed, the DPLL algorithm
In this section, a SAT problem is defined and the terminology used in this domain is described. 
A description of the DPLL algorithm, which are the core of many current SAT solvers, is given.
The extension of DPLL with pseudo-Boolean constraints is discussed, as it allows for more complicated optimisation problems to be represented.
Finally, the specific implementations of SAT solvers MiniSAT \citep{een2003} and SAT4J\citep{le2010sat4j}, are briefly described.

\subsection{The SAT Problem}
As described above, the goal of a SAT algorithm is to find whether a set of variables in a Boolean equation can be assigned values in such a way as to make the equation equal true.
A common representation of such an equation is in Conjunctive Normal Form (CNF).
CNF is defined as a conjunction of clauses, 
where each clause is a disjunction of literals, e.g. $(a \vee b) \wedge (\neg b \vee c)$.

This CNF representation of a SAT problem can be defined as such:
\begin{defs}
.
\begin{itemize}
\item An instance of a SAT problem is a set of Boolean variables $V$ and a formula $F$, where $F$ is is a set of clauses.
\item Each clause in $F$ is a set of literals, where a literal is a variable $v$ or its negation $\neg v$.
\item A set of literals is said to be consistent if for any variable $v$, the set does not contain both $v$ and its negation $\neg v$.
\item A clause $c$ is said to be satisfied by a set of literals $P$ if there exists a literal in $c$ that is also in $P$.
\item A solution to a SAT formula $F$ is a consistent set of literals $P$, such that for every clause $c$ in $F$, $c$ is satisfied by $P$.
\item A partial solution is a subset of any solution, e.g. given $P$ is a solution $P'$ is a partial solution iff $P' \subseteq P$.
\item If there exists a $P$ that is a solution of a SAT formula $F$ the problem is said to be satisfiable, otherwise it is unsatisfiable. 
\end{itemize}
\end{defs}

That is, a SAT problem can have the variable set $\{a,b,c\}$ and the formula $\{c_1,c_2\}$, where clauses $c_1 = {a,b}$ and $c_2 = {\neg b, c}$.
A solution for this problem could be $\{a,\neg b,c\}$ as $a \in c_1$ and $\neg b \in c_2$.
However, $\{a, \neg b, b\}$ is not a solution because it is not consistent, and $\{a, b, \neg c\}$ is not a solution because it does not contain a literal in $c_2$.  

This definition also follows some classical logic rules, such as $\neg \neg v = v$.
This can be extended to sets of literals such that for a set of literals $P$, $\neg P = [\neg v \mid v \in P]$.

\subsection{Davis-Putnam-Logemann-Loveland algorithm for SAT Solvers}
%%%A successful algorithm for solving SAT problems is the DPLL algorithm, here we describe it in overview
The Davis-Putnam-Logemann-Loveland (DPLL) algorithm \citep{Davis1960, davis1962machine} for solving SAT problems is a complete (meaning it will find a solution if one exists), 
backtracking-based search algorithm for SAT problems represented in conjunctive normal form (CNF).

It is defined to take a formula $F$ and a set of literals $P$ (described as a partial assignment), and return \verb+SATISFIABLE+ if $P$ is a partial solution, 
otherwise returning \verb+UNSATISIFABLE+.
By first calling DPLL with $P$ being the empty set, by adding literals to $P$ and recursively  calling itself, the DPLL function searches for whether a solution to the formula exists, 
i.e the formula is satisfiable.
The DPLL algorithm in defined in figure \ref{impl.DPLL} as presented in \citep{dixon2004automating}:
\begin{figure}[h]
\begin{center}
\begin{alltt}
function DPLL(\(F, P\))
   P = unit-propagate(\(F, P\))
   if \(P\) is not consistent:
       then return UNSATISIFABLE;
   if \(P\) is a solution to \(F\):
       then return SATISFIABLE;
   \(l\) = decide\((P)\);
   if DPLL\((F, P \cup \{l\})\)
       return SATISFIABLE
   else
       return DPLL\((F, P \cup \{\neg l\})\);
\end{alltt}
  \caption{Recursive DPLL algorithm}
  \label{impl.DPLL}
\end{center}
\end{figure}

The first \verb+if+ branch returns \verb+UNSATISIFABLE+ if the partial assignment $P$ is inconsistent.
The second \verb+if+ branch determines if the partial assignment $P$ is a solution to $F$, this would end the search by returning \verb+SATISFIABLE+.
The \verb+unit-propagation+ function finds literals that must be in $P$ for $P$ to be a partial solution,
and the \verb+decide+ function returns a literal to be added to $P$.
These functions are described in the following sections. 

\subsubsection{Unit Propagation}
The first line in the  DPLL algorithm calls the \verb+unit-propagation+ function.
This function uses the clauses in the formula and the partial assignment and adds literals to $P$ that must be included if $P$ is to be a partial solution.

A clause $c$ is called unit given a partial assignment $P$, if $c$ is not satisfied by $P$, and $P$ contains all but one of its literals negations.
For example, a clause $\{a,b,c\}$ is unit if the partial assignment contains $\neg b$ and $\neg c$ but neither $a$ or $\neg a$.
A unit literal is then defined as the literal in a unit clause whose negation is not in the partial assignment.
In the above example, the literal $a$ is the unit literal.

For a formula to be satisfiable given partial assignment, each unit literal must be included in the partial assignment,
because if their negation is included the clause is not satisfied by the partial assignment.
For example, given a formula $\{c\}$, where $c = \{a,b\}$;
given the assignment $\{\neg a\}$ the clause $c$ is unit and unit literal is $b$.
If $\neg b$ where in the partial assignment, $c$ would not be satisfied by the partial assignment,
therefore $b$ must be in the partial assignment for it to be a partial solution.

The process of unit propagation is defined in figure \ref{impl.propagation}.
\begin{figure}[htp]
\begin{center}
\begin{alltt}
unit-propagate(\(F, P\)):
  while P is consistent and there exists a \(c \in F\) that given \(P\) is unit:
    \(l\) = unit literal in \(c\)
    \(P\) = \(P \cup \{l\}\)
\end{alltt}
  \caption{Pseudo code of Unit Propagation}
  \label{impl.propagation}
\end{center}
\end{figure}

\subsubsection{Literal Order}
The function \verb+decide+ returns a literal who is not in the partial assignment, nor whose negation is in the partial assignment.
That is, if $l =$ \verb+decide+$(P)$, then $\{l\} \not \in P$ and $\{\neg l\} \not \in P$.
This literal is then added to the partial assignment as an assumption that it might be in the final solution.
This assumption is checked to be correct or not by recursively calling the DPLL function again with the new partial assignment.

If the \verb+decide+ function selects literals that are in the final solution then the search for whether the formula is satisfiable is quickly resolved.
However, if it returns literals that are not in the final solution, the search can be extensive and costly.
Therefore, the efficacy of this selection has a great impact on the overall efficiency of the DPLL algorithm.

\subsection{Advancements in SAT Solvers}
Though the DPLL algorithm is the basis of most modern SAT solvers, the actual implementations have been significantly altered to increase efficiency.
Some changes, including the use of conflict learning, backjumping, and watched literals, are briefly described here.
This section should give a broad overview of the techniques used in current SAT solvers, 
in order to show that their application to problems like component evolution is justified. 

\subsubsection{Conflict Learning and Backjumping}
Conflict learning \citep{stallman1976} is a technique to cache previously tried sets of assignments in order to stop re-solving the same sub-problems.
This is accomplished by remembering what unit clauses, also known as reasons, caused literals to be added to the partial assignment. 
This process works by identifying a variable both inferred to be true and false,
then creating a new clause, known as the learnt clause, which stops that inconsistency being reached again.
This learnt clause is derived by disjoining the two reason clauses after removing both the references to the inconsistent variable.
This process is shown in figure \ref{impl.clauselearning}. 

\begin{figure}[htp]
\begin{center}
$\begin{array}{c}
\{a_1,\ldots,a_k, l\} \\
 \{b_1,\ldots,b_m,\neg l\}\\
\hline
\{a_1,\ldots,a_k, b_1,\ldots,b_m \}
\end{array}$
  \caption{Clause Learning. Where $\{a_1,\ldots,a_k, l\}$ is the reason for $l$, and $\{b_1,\ldots,b_m,\neg l\}$ is the reason for $\neg l$
   are used to create the learnt clause $\{a_1,\ldots,a_k, b_1,\ldots,b_m \}$.}
  \label{impl.clauselearning}
\end{center}
\end{figure}

For example, if the reason for the inferred literal $a$ is clause $\{a, b\}$, and the reason for $\neg a$ is clause $\{\neg a, c\}$,
then the learnt clause derived is $\{b,c\}$, and added to the formula.

Backjumping \citep{Gaschnig1979} is the technique which determines how far to up the search tree to backtrack when a conflict is found.
The higher up the tree the technique ``jumps'' to, the greater reduction of the search space.
The level at which the algorithm backjumps is typically the point at which the learnt clause becomes unit. 

More advanced methods of conflict learning occur by minimising the size of the learnt clauses, as presented in \citep{sorensson2009}.
This research describes search methods that use other reason clauses to find smaller more succinct learnt clauses.
The smaller the clause, the more of the search tree is pruned and the more levels are backjumped through the search.

\subsubsection{Watched Literals}
As noted by studies into the efficiency of DPLL-based SAT solvers \citep{dixon2004automating}, unit propagation is where the bulk of the computation occurs.
Attempts to increase the efficiency of this task was initially to find better heuristics \citep{JamesMCrawford1996} for the literal order, to encourage cascades of unit propagation.
These attempts were shown to work well on random SAT problems but be less efficient for large structured problems \citep{dixon2004automating}.

It was noted that within unit propagation most of the time was spent on identifying the unit clauses.
The naive approach to unit propagation was to examine every clause, and then every literal in the clause to find if it is unit or not.
A more efficient approach was proposed using watched literals \citep{Madigan2001}, where instead of having the clauses examined, 
the clauses maintain an index of the necessary literals and notify the algorithm when they become unit.
This ``don't call us, we will call you'' concept makes the efficiency of the unit propagation function less dependent on the amount of clauses in the formula.

Advances on watched literals have occurred through algorithms to maintain the index of literals, like that presented in \citep{Moskewicz2001}.
Such algorithms enable larger formulae to be solved without necessarily increasing the time to solve them.

\subsection{Pseudo-Boolean Extension of SAT Solvers}
%%%Optimisation of SAT solvers is typically done through extending their possible constraints to include Psuedo Boolean inequalities
A typical extension of the DPLL algorithm is through adding the ability to handle pseudo-Boolean (PB) constraints \citep{dixon2004automating}.
PB constraints consist of a linear inequality over Boolean variables.
Through this extension, the method of pseudo-Boolean optimisation can be used to define criteria and optimise the evolution of component systems.

\subsubsection{Pseudo-Boolean Representation}
A pseudo-Boolean function takes a set of Boolean literals and returns a real number, i.e. $f:B^n \rightarrow \mathbb{R}$ where $B = \{0,1\}$.
A pseudo-Boolean constraint is then an inequality relating the function to a number, e.g. $f(x) \leq 3$.

This is a broad definition of these constraints.
To be efficiently implemented the pseudo-Boolean representation has been restricted to integers and a linear equation.
That is,
\begin{defs}
A pseudo-Boolean function is a linear function over Boolean literals $x$ of the form

$f(x) = a_1x_1 + b_1\neg x_1 + a_2x_2 + b_2\neg x_2 \ldots +  a_nx_n + b_n\neg x_n$

where $x_i \in \{0,1\}$ and fixed integers $a_i$ and $b_i$.
\end{defs}
For example, the pseudo-Boolean function $f(x) = 1x_1 + 0\neg x_1 + 0x_2 + 3\neg x_2$, when taking the input $\langle x_1, \neg x_2 \rangle$ will return $4$.
This function can be more concisely described by removing all members whose coefficient is $0$, e.g. $f(x) = 1x_1 + 3\neg x_2$.

This function can become a constraint such that:
\begin{defs}
A pseudo-Boolean constraint is a linear inequality over Boolean literals $x$ and pseudo-Boolean function $f$ of the form

$f(x) \geq k$ , $f(x) > k$, $f(x) \leq k$ or $f(x) < k$

where $k$ is a fixed integer
\end{defs}

For example, the pseudo-Boolean function $f(x) = 1x_1 + 3\neg x_2$ can be used to create the constraint $f(x) > 2$.
This constraint will be satisfied by the literals $\langle x_1, \neg x_2 \rangle$ as $f(x) = 4$, but not by the literals $\langle x_1,  x_2 \rangle$ as $f(x) = 2$.
A pseudo-Boolean constraint can be therefore be represented by a pseudo-Boolean function $f$, an inequality and an integer.

Such constraints, can be translated into the standard CNF, but the original pseudo-Boolean representation has been shown to be exponentially more concise \citep{dixon2004automating}.
Also given the proper amendments to unit propagation and other algorithms (some of which is described in \citep{Sheini2006}), 
it can be faster to find solutions to problems represented in pseudo-Boolean constraints rather than their translated SAT constraints \citep{dixon2004automating}.
Both of these reasons give ample justification to use the pseudo-Boolean extension to DPLL when mapping the component evolution problem to a set of constraints. 

\subsection{MiniSat and SAT4J}
MiniSAT presented in \citep{een2003}, is a simple SAT solver implementation written in C, and designed for speed and extensibility.
It uses the DPLL based conflict driven algorithm as discussed above.
This solver has become popular and is the basis of many other SAT solvers due to its open source distribution.
This has also lead to a track in the 2011 SAT competitions\footnote{http://www.satcompetition.org/2011/ accessed 6/3/2012} that deals with only altering MiniSAT to increase performance.
This means that MiniSAT has been repeatedly validated for performance by third parties across many difference SAT problems. 

SAT4J \citep{le2010sat4j} is a Java re-implementation, and extension, of MiniSAT in the Java programming language.
The extensions SAT4J makes to MniSAT include the ability to find resolve pseudo-Boolean constraints.
SAT4J was developed in order to quickly test combinations of advancements in SAT solving technology.
This goal has created an easily modifiable and transparent implementation, able to be adapted to be used in various domains.

\section{GJSolver}
\label{impl.gjsolver}
%%%My Implementation, cut the fat, straight forward
Through the course of this research an implementation grew out of the need to have a modifiable base to experiment with component dependency resolution.
Other implementations of CDR where often component model specific, or involved difficult (or impossible) to modify code.
The CDR implementation created through this research is known as GJSolver, 
and is designed to solve CDR problems represented in CUDF with optimisation descriptions in Mancoosi format.

\subsection{Requirements of GJSolver}

Given the context under which GJSolver was developed, the set of requirements that it was designed according to are listed here:
\begin{enumerate}
  \item \textbf{Mancoosi International Competition Ready}: The MISC gives a set of standards to solve CUDF problems with criteria defined using the Mancoosi optimisation format.
  A goal of the GJSolver is to be entered in the MISC in order to be compared against other solvers, and be validated to show the implementation is correct.
  Following these standards ensures that the solver can be entered, and then competing can show the relative speed and correctness of the GJSolver implementation.
  This will ensure that GJSolver is a valid and efficient implementation. 
  \item \textbf{Anytime Algorithm}: Return a solution, even if it is not the optimal solution, within a predefined amount of time. 
  CDR problems can be large and complex, finding the ``best'' solution can be difficult. 
  This requirement ensures that the algorithm will return a solution in a practical time frame.
  \item \textbf{Easily Creatable Criteria}: The ability to quickly implement and test criteria in a manner that enables experimentation, will increase the speed of research.
\end{enumerate}

\subsubsection{Mancoosi International Solver Competition}
Given a goal of the GJSolver implementation is to compete in the MISC, the interface and standards defined for this competition must be followed.
How the entered solvers are executed, what environment they are executed in, and the output required are all important aspects to the development of GJSolver.

%%%They are executed on the command line
The way in which the entered solvers are executed is standardised to allow the automation of the competition.
This standard requires the entered solvers should be able to be executed on the command line with three arguments, \verb+cudfin+, \verb+cudfout+ and \verb+criteria+.
These arguments are defined as:
\begin{itemize}
  \item \verb+cudfin+: is a relative path to a CUDF document (as specified in section \ref{formal.cudf}) that describes the problem to be solver.
  \item \verb+cudfout+: is a relative path to a non-existent file which is created by the solver to output the solution.
  \item \verb+criteria+: is a Mancoosi optimisation format (as described in section \ref{formal.mancoosioptimisationformat}) that describes the criteria to select a solution. 
\end{itemize}
The format of the output file, whose path is the \verb+cudfout+ argument, is a sub set of CUDF.
This output only requires package stanzas with the package name, version and installed properties.
This lowers the size of the output file required to be written by the solver.

%%%The environment POSIX, with 5minutes 1GB of memory
The environment in which the solver is executed is a virtual machine running a GNU/Linux system in a x86 architecture with 1GB of memory (RAM).
It contains a Java runtime environment, allowing the use of Java as a primary language.
The time in which the solver is allowed to run is 5 minutes, after this is will return a result of ABORT at which time the solver will be forcibly executed.
This time limit either ensures that your algorithm returns a result quickly or is an anytime algorithm, where it can be return a result when interrupted during the search.
The latter is the option that has been selected for GJSolver, and will be discussed in the next section.

\subsubsection{Anytime Algorithm}
The time required to find an optimal solution to a component evolution problem can be impractical.
For this reason, GJSolver will be implemented with an anytime algorithm at its core.
An anytime algorithm can return a valid solution to a problem if it is interrupted before it ends.
This interruption could be from a user or some other stimuli.
In the case of GJSolver, the interruption will be caused by an internal timer dedicated to ensure that the algorithm does not run in time exceeding the 5 minute deadline.

Such anytime algorithms therefore create a trade-off between time and optimality, where the more time that is set aside for the algorithm to run the more optimal the solution. 
They can also create inconsistent results, as any input may, on different runs, return many different possible solutions.
This means that if it is necessary to interrupt the algorithm, it is possible to return non-optimal solutions.
However, this is only if interuption is necessary, as GJSolver will encounter a wide range of difficult problems, 
it is difficult, or impossible, to judge beforehand if a problem will require the algorithm to be interrupted. 

\subsubsection{Expendable Criteria}
The ability to test and experiment with a wide range of possible criteria to use in CDR is a goal of this research.
To satisfy this requirement, GJSolver was developed in a modular manner, in which the criteria can be extended, modified, and tested when solving problems quickly.

\subsection{Implementation Decisions}
%%%First decision we base our solver on Eclipse P2
The first decision that was made during development, was to model GJSolver on another successful CDR implementation, Eclipse P2 \citep{le_berre_dependency_2009,leBerre2010}.

%%%The advantages to this decision
There are many advantages to basing GJSolver on P2; firstly P2 had already been modified to enter MISC, so had various tools (like a CUDF parser) that reduced the work significantly.
The internal SAT solver used by P2, SAT4J, is separated into an OSGi bundle making the reuse of this solver trivial.
Most importantly of all, both SAT4J and P2 are open source making internal investigation of issues, and modification possible reducing risk.

GJSolver, however, is not just a modification of Eclipse P2, as core problems would make such an effort impractical.
Firstly, Eclipse P2 is a solver for the OSGi/Eclipse component models, and has much platform specific details.
The way in which P2 solves CUDF problems, is to first change them into OSGi problems through an internal data structure that does not resemble CUDF.
This additional layer of abstraction makes error checking, and solution checking a difficult process as the internal representation no longer directly resembles the original problem.
The implementation of criteria in Eclipse P2 is also difficult to alter, as again the system is built for optimising OSGi specific CDR problems.

Therefore, GJSolver is modeled after Eclipse P2, but not a modification of it.
This decision constrained GJSolver to be Java based, 
using SAT4J as a core, and implementing a pseudo-Boolean optimisation algorithm.

\subsubsection{CUDF Mapping}
In chapter \ref{formal},
the mapping of the component relationships described in a CUDF document to the set of four different types of constraints (keep, dependency, conflict and request) was described.
Here, the mapping from these four constraints to the SAT and pseudo-Boolean constraints is described.
This gives a complete path, from CUDF document, through the abstract constraints to SAT and PB constraints ready for a solver to satisfy.

Given a formula $F$ is a set of clauses, a clause is a set of literals, and a literal is either a variable or its negation;
The set of component is defined as a subset of the variables, i.e. $C \subseteq V$.
Literals, which make up the clauses, can therefore be a component or its negation.

The mapping to this formalism from the keep, dependency, conflict constraints are: 
\begin{itemize}
  \item keep constraints of the form $a_1\vee \ldots \vee a_n$  can be represented as a clause $\{a_1,\ldots,a_n\}$
  \item dependency constraints of the form $a \rightarrow c_1 \vee \ldots \vee c_n$ can be represented as the clause $\{\neg a, c_1, \ldots, c_n\}$
  \item conflict constraints of the form $a \rightarrow \neg c$ can be represented as a clause $\{\neg a, \neg c\}$
\end{itemize}

The request constraints can be mapped as:
\begin{itemize}
      \item install constraints of the form $a_1 \vee \ldots \vee a_n$  can be represented as a clause $\{a_1,\ldots,a_n\}$
      \item remove constraints of the form $\neg a$ can be represented as a clause $\{\neg a\}$
      \item update constraints have two clause types; one $\neg a$ can be represented as a clause $\{\neg a\}$;
      the second $a_1 + \ldots + a_i = 1$ can be represented by the clause $\{a_1,\ldots,a_i\}$ 
      and the pseudo-Boolean constraint $f(x) <= 1$, where $f(x) = 1.a_1 + \ldots + 1.a_i$.
\end{itemize}

The update constraints of the form $a_1 + \ldots + a_i = 1$ accomplish only having one component installed by stating with the clause $\{a_1,\ldots,a_i\}$, at least one must be installed,
and with the constraint $f(x) <=1$ stating that at most one is installed.

%%%TODO up to here
\subsubsection{Pseudo-Boolean Optimisation}
%%%Map psuedo boolean optimisation to formal optimisation definition
The method that GJSolver uses to optimise the component evolution problem is through a method called pseudo-Boolean optimisation (PBO).
PBO involves trying to assign values to the Boolean variables in such a way to either minimise or maximise a pseudo-Boolean function.

This optimisation method can be shown to be consistent and mapped to the framework defined in chapter \ref{formal}.
Two complete lattices are defined to either minimise or maximise natural numbers; $L_{min} = \langle \mathbb{N}, \geq \rangle$ and $L_{max} = \langle \mathbb{N}, \leq \rangle$.
Both these lattices are over the totally ordered natural numbers, so trivially shown to have a join, meet and be partially ordered.
Given the set of variables $V$, these lattices are used to form either a minimising or maximising  ranking system;
$RS_{min} = \langle L_{min}, \{F,T\}, V \rangle$ and $RS_{max} = \langle L_{max}, \{F,T\}, V \rangle$. 

A variable in solution $\gamma$ of a ranking system can be mapped to a literal $l$,
where given a variable $v$ iff $\gamma(v) = T$ then a literal will equal $v$, and iff $\gamma(v) = F$ then the literal will equal $\neg v$.
This can be extended to map the set of variables in solution $\gamma$ to the set of literals $P$, i.e. $P = [v \mid v \in V \wedge \gamma(v) = T] \cup [\neg v \mid v \in V \wedge \gamma(v) = F]$

The ranking function ($rank$), for either ranking system, can be defined to be a pseudo-Boolean function.
However, as a single pseudo-Boolean function does not compare two solutions, the current and proposed systems, but only measures one solution.
As the current system does not change for a single evolution problem, a pseudo-Boolean function must be defined for each evolution problem.
That is a ranking function, $rank(l_1,l_2)$, can be used across many evolution problems, but a  pseudo-Boolean function, $f_{l_1}(l_2)$, will need to be defined given the current previous system.

To demonstrate this PBO, the criteria to minimise the amount of components added to the system is defined.
The function $rank^{added}(l_1,l_2)$ is defined to return the amount of components in the proposed system that are not in the previous system (the added components),
i.e. $rank^{added}(l_1,l_2) = |[x \mid x \in \mathbb{C} \wedge \neg x \in l_1 \wedge x \in l_2]|$.
Given pseudo-Boolean functions only measure the proposed system,
the function $f_{l_1}(l_2)$ is defined by summing all literals that are negative in the current system and positive in the proposed system;
i.e. given the set of variables $\{x_1,\ldots,x_n\} = [x \mid x \in \mathbb{C} \wedge \neg x \in l_1]$, the pseudo-Boolean function is defined $f_{l_1}(l_2) =  1.x_1 + \ldots +  1.x_n$.

\subsection{Iterative Strengthening}
The algorithm known as iterative strengthening, presented in \citep{calistri1994iterative} and \citep{le2010sat4j}, 
describes an anytime algorithm using constraint satisfaction that iteratively finds better solutions.
This algorithm can be used for PBO, to find optimal solutions to an evolution problem.
This is done by first finding a solution, then iteratively adding constraints to the formula that ensure the next solution found will be better than the previous.
This continues until either the strengthened formula is found to be unsatisfiable, or the algorithm is interrupted, at which point the best solution currently found is returned. 
This algorithm is defined in figure \ref{impl.strength}.

\begin{figure}[htp]
\begin{center}
\begin{alltt}
find-solution(\(F\),strengthen):
    \(answer\) = DPLL(\(F\))
    if \(answer\) = UNSAT:
        return UNSAT
    do:
        \(model\) = \(answer\)
        \(constraints\) = strengthen(\(model\))
        \(F\) = \(F \cup constraints\)
        \(answer\) = DPLL(\(F\))
    while not interrupted() and \(answer\) != UNSAT
    return \(model\) 
\end{alltt}
  \caption{Pseudo code of Iterative Strengthening Algorithm}
  \label{impl.strength}
\end{center}
\end{figure}

The first aspect to note in this algorithm is that the \verb+DPLL+ function has been altered slightly.
This function now takes the formula $F$ as a parameter and returns a solution, $answer$, that satisfies $F$. 
If the formula is unsatisfiable, however, \verb+DPLL+ returns \verb+UNSAT+.

The first action in this algorithm is to check if the formula is satisfiable, by assigning the output of the \verb+DPLL+ function to the variable $answer$.
If $answer$ is \verb+UNSAT+, then the algorithm stops and returns that the formula is unsatisfiable.

The main loop of this function first assigns the contents of the variable $answer$ to the variable $model$.

The function \verb+strengthen+ is then called on $model$, to generate a set of constraints that when added to the formula ensure that only solutions more optimal are satisfiable.
Therefore, given some function $d$ measures how optimal a solution is, $model_1$ satisfies $F$, and $constraints = $ \verb+strengthen+$(model_1)$;
any $model_2$ that satisfies $F \cup constraints$ must be of the relation $d(model_1) < d(model_2)$.
That is, that given some measurement of optimal, adding the constraints ensure that only more optimal solutions satisfy the formula with the strengthened constraints. 

The constraints from the \verb+strengthen+ function are then added to the formula, a more optimal solution is searched for with \verb+DPLL+.

To ensure that this algorithm will end, and not just have \verb+DPLL+ repeatedly return the same answer to the formula, 
the found $model$ must not satisfy the formula with the strengthened constraints.
If this is the case, the number of answers that satisfy $F$ will decrease by at least one per iteration.

This main loop will continue until the function \verb+interrupted+ returns true, which may happen given some external cause (like a user stopping the algorithm or a timer running out),
or the strengthened formula is found to be unsatisfiable.
At this point the previously best solution discovered stored in the variable $model$ will be returned.

\subsubsection{Lexicographic Optimisation}
%%%Basic iterative strengthing can be modified towards handling lexicographically ordered crtiera, and allowing this criteria to effect DPLL's efficiency
The basic iterative strengthening algorithm, presented in figure \ref{impl.strength}, can be modified to handle lexicographically related criteria, so as to solve component evolution problems.
By adding an additional outer loop, including other necessary alterations, the original algorithm has been modified to handle lexicographical criteria.

The original iterative strengthening algorithm with the lexicographical extension has been implemented in GJSolver. 
This altered algorithm is presented in figure \ref{impl.lexstrength}.

\begin{figure}[htp]
\begin{center}
\begin{alltt}
find-lex-solution(\(F\),criteria):
    \(F\) = \(F \cup \) criteria.initial-constraints()
    \(answer\) = DPLL(\(F\))
    if \(answer\) = UNSAT:
        return UNSAT
    do:
        current-criterion = criteria.pop()
        do:
            \(model\) = \(answer\)
            \(constraints\) = current-criterion.strengthen(\(model\))
            \(F\) = \(F \cup constraints\)
            \(answer\) = DPLL(\(F\))
        while not interrupted() and \(answer\) != UNSAT
        \(F\) = \(F \backslash constraints\)
        \(F\) = \(F \cup \) current-criterion.lock(\(model\))
        \(answer\) = \(model\)
    while not interrupted() and criteria.size() != 0
    return \(model\) 
\end{alltt}
  \caption{Pseudo code of the Lexicographic Iterative Strengthening Algorithm}
  \label{impl.lexstrength}
\end{center}
\end{figure}

The first alteration to the original algorithm is that this version takes two parameters, firstly the formula denoted as $F$;
secondly a stack of criteria, ordered so the top of the stack is the criteria most lexicographically important.

The first action of this algorithm is to include initial criteria constraints,
these constraints may define new literals that can be used in other criteria functions.
These constraints are just a union of sets of initial constraints given by individual criteria, and they should not effect a solutions satisfiability. 

The next action of this algorithm is to check whether the formula is satisfiable,
and if it is not, return unsatisfied.

A new outer loop is defined, that loops over the criteria until either there is none left or it is interrupted.
This loop find optimal solution for each criteria, then moves to the next including the constraints stopping a less optimal solution to be found.
The first action is to $\verb+pop+$ off the (current) most important criteria and store it in the variable \verb+current-criterion+.

Then the inner loop based from original iterative strengthening algorithms main loop is then defined.
This main difference between this and the originals inner loop is the \verb+strengthen+ function is now defined per criteria from the function \verb+current-criterion.strengthen+.
This means the \verb+strengthen+ function changes with the outer loop, as it optimises for different criteria.
As this inner loop iterates more optimal solutions are found, until there exist no more to be found.

When the \verb+current-criterion+ has strengthened the formula to make it unsatisfiable, the constraints that made it unsatisfiable are removed.
This is necessary to continue the search for more optimal solutions judged by the next criteria.
The formula then has constraints from the function \verb+current-criterion.lock+ added to it, this ensures no worse solution will be found on the next iteration.

These loops will iterate until either all the criteria have been optimised, or the function is interrupted.
The result is a set of literals that satisfy the constraints in the variable $model$.
These literals can be easily mapped to a set of components that are non-negative literals in the model.
That is, the resulting system of components is equal to $[c \mid c \in \mathbb{C}$ and $c \in model]$.

\subsubsection{Criteria}
%%%This algorithm shows the use of some methods that criteria must implement
This new algorithm allows us to describe the necessary requirements a criteria must define in order to be used.
Each criteria must define the functions \verb+strengthen+ and \verb+lock+ to be used in this algorithm.
Further more, through altering the \verb+decide+, or literal order, function in the DPLL algorithm, each criteria could make the search for optimal solutions more efficient.
This will help guide the search to near optimal solutions, which will lower the upper bound of the number of loops necessary to find the most optimal solution.

%%%Initial constraints
Some criteria also require the ability to add constraints to the formula before the outer loop starts,
this is implemented in the function \verb+criteria.initial-constraints+.
These constraints are typically used to measure quantities that have no direct variable available.
A typical case for this is when a criteria needs to measure component by their names.
For instance, if component $\langle a,2\rangle$ was removed and component $\langle a,3 \rangle$ was added. 
Although a component was removed and a new one added, the component name $a$ still exists in the system, so no change to component names were made.

Further description of how initial constraints are used is given in chapter \ref{strategies}. 

\subsubsection{Formal Criteria}
Each criterion in the stack \verb+criteria+ contains a pseudo-Boolean function that either requires minimisation or maximisation.
This equation, represented by a list of literals and coefficients, is used to define the \verb+strengthen+ function.

The ranking function \verb+criteria+.$f$ is the pseudo-Boolean function used by the criteria.
Given a criterion's pseudo-Boolean function $f$,
the function \verb+strengthen+ the criterion is defined to return a pseudo-Boolean constraint such that:
\begin{itemize}
  \item if the function is being minimised then the returned constraint is $\langle f,<,f(l_2) \rangle$
  \item if the function is being maximised then the returned constraint is $\langle f,>,f(l_2) \rangle$
\end{itemize}
That is, the strengthen function for a criteria is created from its pseudo-Boolean function.
The function \verb+lock+ is similarly defined except the inequalities $>$ and $<$ are replaced respectively with $\geq$ and $\leq$.

\subsection{Drawbacks of this Optimisation approach}
%%%There are a few drawbacks to this mapping;
GJSolver's implementation tries to address the initial set of requirements by using an SAT solver extended with pseudo-Boolean constraints as its core,
and the lexicographic iterative strengthening algorithm for optimisation.
This has created some drawbacks that will be discussed here.

%%%The optimisation must be represented linearly
As GJSolver uses a SAT solver at its core, 
and it has been noted in \citep{le_berre_dependency_2009} and \citep{leBerre2010} there is no easy solution to extending a SAT solver to handle non-linear constraints.
There is no practical way of creating criteria that can optimise have non-linear functions within GJSolver.
This will stop many criteria from being defined, though the trade-off is that GJSolver will remain relatively simple and efficient.

%%%Real numbers must be truncated to fit the integer representation. 
The pseudo-Boolean constraints use only integer and not real coefficients.
This stops the definition of criteria that optimise values like percentages.
A practical solution to this is to multiply any real number by a large value and truncate the product to an integer representation.
Scaling a value in such a manner, may remove a small amount of resolution of the real value, though it does allow for criteria to use real values numbers.

\subsection{Verification}
\label{impl.verif}
%%%We entered this solver into two Mancoosi MISC competitions
The GJSolver implementation was verified by entering it into the Mancoosi International Solver Competition, whose requirements are described above.
This process was taken twice, firstly in a MISC Live event, which is an interim competition held during the year;
secondly at the MISC 2011 event, 
which is the main competition, whose results are announced at the Workshop on Logics for Component Configuration\footnote{http://www.pps.jussieu.fr/~treinen/lococo/2011/ accessed 6/3/2012}.

\subsubsection{Tracks and Scoring}
As MISC Live and MISC are competitions to compare solvers entered by different developers and researchers,
criteria to select the ``best'' solver must be defined through a scoring system.

Each competition is broken down into three possible tracks, each defined by the criteria used to solve the set of problems.
The first basic track, is 'paranoid', the second more advanced track is 'trendy', and third track is 'user'.
Both 'paranoid' and 'trendy' have pre-defined criteria in set lexicographic order for the solvers to use, 
and the 'user' track uses pre-defined set of criteria whose configurations are unknown to the solvers before the competition.
This means that 'paranoid' and 'trendy' can have solutions tailored to their specific criteria that is required, where the 'user' track cannot.
The exact criteria and how they are defined, are discussed in the next chapter.

For each track, a set of solvers is entered.
Each track has a set of problems to solve, and all participating solvers are executed to return solutions to these problems.
Each solution is scored and all are summed to get a final score of a solver for a given track, where the solver with the lowest amount of points is victor.

The way in which a solution is scored is by first giving it one of three classes; a real solution; no solution; a incorrect solution.
A real solution is any solution that is correct; no solution occurs when a solver finished without output, this can happen because of error, timeout, or there not being a satisfiable solution;
an incorrect solution is the worst class, as it can cause an incorrect system to be created.

If $m$ is the number of solvers entered into the competition,
a real solution is given $1$ point if it is the best solution returned by any solver, and $1$ plus how ever many solvers found better solutions.
For example, if solver $s1$ found a solution where solvers $s2,s3,s4$ found a more optimal solution, then $s1$ gets $4$ points for that solution.
If no solution is returned then $2\times m$ are the points given, and if an incorrect solution is returned then $3 \times m$ points are given.

If more than one solver has the same amount of points at the end of a track, then the time it took for them to find each solution is summed and the solver that took the least time wins.
This is also a lexicographical order of points, where it is infinitely better to return good solutions, than to return solutions quickly.

\subsubsection{MISC Live}
%%%In the first competition we had only partially implemented much of the functionality, so we did not expect great results.
The first competitions, MISC Live, was entered without GJSolver being fully implemented.
Therefore, the only track that was possible to enter was the basic track 'paranoid'.
The results for this track\footnote{http://mancoosi.org/misc-live/20101126/paranoid/ accessed 6/3/2012} where promising, though there where some clearly necessary improvements.
Due to the competition openly distributing the solutions and the output from the entered solvers, solution to problems from other solvers, 
as well as GJSolver, were able to be analysed to search for improvement opportunities.

\subsubsection{MISC}
The main verification of GJSolver was through the MISC 2011 event.
In this event GJSolver was entered into all tracks, where the 'paranoid' track had a total of 5 solvers, the 'trendy' track had a total of 6 solvers, 
and the 'user' track had a total of 4 solvers.
Each track was also entered by the solver which GJSolver is based on, Eclipse P2, and another very efficient solver aspuncud.

The scores and the times for each of the track compared to that from Eclipse P2 and aspuncud in table \ref{impl.misc2011}.
\begin{table}
\begin{tabular}{| l | c | c | c | c |}\hline
Track & \# of Problems & GJSolver & P2 & aspuncud\\ \hline
paranoid & 129 & (190 : 5,294) & (181 : 4,646) & (147 : 1,035) \\ \hline
trendy & 129 & (197 : 13,073) & (232 : 13,435) & (151 : 1,767) \\ \hline
user & 400 & (656 : 73,522) & (1392 : 87,956) & (1215 : 39,905) \\ \hline
\end{tabular}
\caption{Results from MISC 2011, results are (score:time in seconds)}
\label{impl.misc2011}
\end{table}

The total winners for each track where for both 'paranoid' and 'trendy' aspuncud won, and for the 'user' track GJSolver won.

\subsubsection{Analysis}
The main analysis of the results from the MISC competition is accomplished through comparing them to the previously stated requirements that
GJSolver should be \textbf{Mancoosi International Competition Ready}, should implement an \textbf{Anytime Algorithm} and should be \textbf{Easily Creatable Criteria}.

The results from MISC show that GJSolver was ready for its entrance, during the competition it had very consistent results.
These results allowed it to compete with the other solvers, and even win the 'user' track.

The implementation of the anytime algorithm was shown through the many hard problems required to be solved.
This can be seen in many of the problems in the 'trendy' track, these required a lengthy search and many required interruption of the algorithm to return the currently best found solution.
Looking at many of these timed out problems, reveals that many of them returned optimal solutions regardless of the fact they were stopped before finishing the search.
This leads to the presumption that the finding of the optimal solution is not necessarily the time consuming part of the search, but the proof that there are no better solution remaining.

The criteria that were defined in to enter this competition were wide ranging, and will be specifically discussed in chapter \ref{strategies}.
It may be said that the reason for GJSolvers win of the 'user' track could be put down to the easily defined and tuning criteria that allowed the quick testing of different heuristics.


%\section{Other Methods}
%This \citep{Stuckenholz2007} study looks at using boolean optimisation with branch and bound as a solution, as does \citep{Jenson2010a}.
%\subsection{Integer Programming}
%Discussion of this method as the best MISC solver uses this, it has a very complex implementation
%\subsection{SMT Solvers}
%SMT Solver, a slightly higher logic than SAT uses; it has to broad a definition when SAT suffices
%\subsection{Constraint Solver}
%We could just use Prolog, like SMT I think it is too broad when there are good SAT solvers


\section{Summary}
In this chapter first Boolean satisfiability solvers were introduced, 
describing the DPLL algorithm and its extensions through pseudo-Boolean constraints as a means to solve component evolution problems.
Then the implementation produced through this research, GJSolver, was introduced with the modified iterative strengthening algorithm to find optimal solutions.
Finally, the description of the MISC competition and GJSolver's verification through it were given to show the requirements were satisfied.
