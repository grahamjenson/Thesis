\chapter{Component Dependency Resolver}
\label{implementation}
\epigraph{What I cannot create, I do not understand.}
{\textit{Richard Feynman, 1988}}

%%%CDR Has been shown to be NP-Complete to find a solution and NP-Hard to find an optimial one.
Finding any solution to an evolution problem is NP-Complete, and finding an optimal solution is NP-Hard.
Therefore, automating component system evolution will require an efficient algorithm and implementation.
The function that finds and returns a optimal solution to an evolution problem in this research is called a Component Dependency Resolver (CDR). 

%%%The efficiency of the implementation is important as the problem can combinatorially grow into a hard problem 
The efficiency of CDR is a significant requirement.
Many components may exist in evolution problems, each with multiple versions, and each version with multiple relationship constraints.
This problems complexity grows  in combinatorial manner with the addition of components, making any ad-hoc, or overly simple, CDR implementation easily overwhelmed.

%%%Formally it resembles the SAT problem, which already has efficient implementations, and has been used to solve this problem
To find a solution to an evolution problem the set of relationship and user request constraints must be satisfied.
These constraints create a problem that closely resembles the Boolean Satisfiability Problem (SAT), which has fast, robust solver implementations that can be used.
This has been noticed by other researchers \citep{Berre2008,Mancinelli2006} 
and the use of SAT solvers to implement CDR in both academia \citep{abate2011} and industry \citep{leBerre2010} has become common.

%%%Finding the optimial solutions requires extensions to SAT solvers
The finding of an optimal solution to an evolution problem is impractical when representing the problem in only SAT constraints. 
Through extending a SAT solver implementation to also handle other types of constraints, specifically pseudo-Boolean constraints, optimisation becomes practical.
Such extensions have been used to solve other problems \citep{dixon2004automating}, as well as within other CDR implementations \citep{le_berre_dependency_2009}.

%%%In this chapter\ldots
In this chapter the algorithms that CDR uses to find optimal solutions are defined and discussed.  
The Davis-Putnam-Logemann-Loveland (DPLL) algorithm \citep{Davis1960, davis1962machine} is described as it is the basis for many current SAT solvers.
Along with DPLL, more recent advancements in SAT solver technology are discussed, including its extension to handle pseudo-Boolean constraints.
This description is intended to give a basic understanding of the internal workings of the SAT solver used at the core of the presented CDR implementation.
The description of this research's CDR implementation, GJSolver, is then given.
The specific requirements that GJSolver was developed towards, and the implementation decisions and compromises that were made are also described.
This chapter ends with a discussion of the process of verification of GJSolver through the Mancoosi International Solver competition.  
This competition involved the comparison of GJSolver against other current solvers where solutions and time where measured and points were assigned.

\section{Boolean Satisfiability Solvers}
\label{impl.SAT}
Boolean satisfiability (SAT) is the problem of determining if the variables in a Boolean equation can be assigned in such a way that the equation returns true.
This problem was the first identified NP-Complete problem, meaning that no known algorithm exists that efficiently solves all instances of SAT problems.
Many difficult problems have been expressed through reduction to SAT to show their NP-Completeness, this includes the evolution problem from chapter \ref{formal}.
As many difficult problems can be expressed in SAT, 
algorithms and implementations that solve SAT problems (SAT solvers) have received significant attention.
Taking advantage of this research will enable a robust CDR implementation to be created.

%%%SAT solvers are used to solve many problems, creating a community dedicated to creating fast implenmenations
SAT solvers have been used in various domains to tackle problems such as electronic design automation \citep{Marques-Silva2000}, 
model verification \citep{dennis2006}, and, of course, component system evolution \citep{leBerre2010}.
The need for efficient solvers has spawned a community dedicated to creating and testing various SAT solver implementations.
As a means of validating SAT solvers, this community regularly compare them against one another in a series of competitions\footnote{http://www.satcompetition.org/}.
Using competitions encourages progress of SAT solvers for both specific and general instances of SAT problems.

%%%In this section a common implementation of SAT solvers is discussed, the DPLL algorithm
In this section, a SAT problem is defined, giving the terminology used in this domain. 
Then a description of the DPLL algorithm is given, and current improvements are described..
The extension of this algorithm with pseudo-Boolean constraints is discussed, as it allows for more complicated optimisation problems to be represented.
Finally, the specific implementations of SAT solvers MiniSAT \citep{een2003} and SAT4J\citep{le2010sat4j}, are briefly described.

\subsection{The SAT Problem}
As described above, the goal of a SAT algorithm is to find whether a set of variables in a Boolean equation can be assigned values in such a way as to make the equation equal true.
A common representation of such a formula is in Conjunctive Normal Form (CNF), this is defined as a conjunction of clauses, 
where each clause is a disjunction of literals, e.g. $(a \vee b) \wedge (\neg b \vee c)$.

This CNF representation of a SAT problem can be defined as such:
\begin{defs}
An instance of a SAT problem is a set of Boolean variables $V$ and a formula which is a set of clauses $F$.
Each clause in $F$ is a set of literals, where a literal is a variable $v$ or its negation $\neg v$.
A set of literals is said to be consistent if for any variable $v$, the set does not contain both $v$ and its negation $\neg v$.
A clause $c$ is said to be satisfied by a set of literals $P$ if there exists a literal in $c$ that is also in $P$.
A solution to a SAT formula $F$ is a consistent set of literals $P$, such that for every clause $c$ in $F$, $c$ is satisfied by $P$.
A partial solution is any subset of any solution, e.g. given $P$ is a solution $P'$ is a partial solution iff $P' \subseteq P$.
If there exists a $P$ that is a solution of a SAT formula $F$ the problem is said to be satisfiable, otherwise it is unsatisfiable. 
\end{defs}

That is, a SAT problem can have the variable set $\{a,b,c\}$ and the formula $\{c_1,c_2\}$, where clauses $c_1 = {a,b}$ and $c_2 = {\neg b, c}$.
A solution for this problem could be $\{a,\neg b,c\}$ as $a \in c_1$ and $\neg b \in c_2$.
However, $\{a, \neg b, b\}$ is not a solution because it is not consistent, and $\{a, b, \neg c\}$ is not a solution because it does not contain a literal in $c_2$.  

This definition also follows some classical logic rules, such as literals and their negation can be defined as $\neg \neg v = v$,
and the negation of sets of literals $P$ is the negation of each literal in $P$, i.e. $\neg P = [\neg v \mid v \in P]$.

\subsection{Davis-Putnam-Logemann-Loveland algorithm for SAT Solvers}
%%%A successful algorithm for solving SAT problems is the DPLL algorithm, here we describe it in overview
The Davis-Putnam-Logemann-Loveland (DPLL) algorithm \citep{Davis1960, davis1962machine} for solving SAT problems is a complete (meaning it will find a solution if one exists), 
backtracking-based search algorithm for SAT problems represented in conjunctive normal form (CNF).

It is defined to take a formula $F$ and a set of literals $P$ (described as a partial assignment), and return \verb+SATISFIABLE+ if $P$ is a partial solution, 
otherwise returning \verb+UNSATISIFABLE+.
By adding literals to $P$ and recursively  calling itself, the DPLL function searches for whether the formula is satisfiable.
The DPLL algorithm in defined in figure \ref{impl.DPLL} as presented in \cite{dixon2004automating}:
\begin{figure}[h]
\begin{center}
\begin{alltt}
function DPLL(\(F, P\))
   P = unit-propagate(\(F, P\))
   if \(P\) is not consistent:
       then return UNSATISIFABLE;
   if \(P\) is a solution to \(F\):
       then return SATISFIABLE;
   \(l\) = decide\((P)\);
   if DPLL\((F, P \cup \{l\})\)
       return SATISFIABLE
   else
       return DPLL\((F, P \cup \{\neg l\})\);
\end{alltt}
  \caption{Recursive DPLL algorithm}
  \label{impl.DPLL}
\end{center}
\end{figure}

The first \verb+if+ branch returns \verb+UNSATISIFABLE+ if the partial assignment $P$ is inconsistent.
The second \verb+if+ branch determines if the partial assignment $P$ is a solution to $F$, this would end the search by returning \verb+SATISFIABLE+.
The \verb+unit-propagation+ function adds literals that to $P$ and the \verb+decide+ function returns a literal to be added to $P$, both described in the following sections. 

\subsubsection{Unit Propagation}
The first part of the DPLL algorithm calls the \verb+unit-propagation+ function.
This function uses the clauses in the formula and the partial assignment and adds literals to $P$ that must be included if it is to be a partial solution.

A clause $c$ is called unit, given a partial assignment $P$, if $c$ is not satisfied by $P$ and $P$ contains all but one of its literals negations.
For example, a clause $\{a,b,c\}$ is unit if the partial assignment contains $\neg b$ and $\neg c$ but not $a$ or $\neg a$.
A unit literal is then defined as the literal in a unit clause whose negation is not in the partial assignment.
In the above example, the literal $a$ is described as the unit literal.

For a formula to be satisfiable for a given partial assignment, each unit literal must be included in the partial assignment.
For example, given a formula $\{c\}$, where $c = {a,b}$;
given the assignment $\{\neg a\}$ the clause $c$ is unit and literal $b$ is unit.
If $\neg b$ where in the partial assignment, $c$ would not be satisfied by the partial assignment,
therefore $b$ must be in the partial assignment.

The process of unit propagation is further defined in figure \ref{impl.propagation}.
\begin{figure}[htp]
\begin{center}
\begin{alltt}
unit-propagate(\(F, P\)):
    while P is consistent and there exists a \(c \in F\) that given \(P\) is unit:
        \(l\) = unit literal in \(c\)
        \(P\) = \(P \cup \{l\}\)
\end{alltt}
  \caption{Pseudo code of Unit Propagation}
  \label{impl.propagation}
\end{center}
\end{figure}

\subsubsection{Literal Order}
The function \verb+decide+ returns a literal who is not in the partial assignment, nor whose negation is in the partial assignment.
That is, if $l =$ \verb+decide+$(P)$, then $\{l\} \not \in P$ and $\{\neg l\} \not \in P$.
This literal is then added to the partial assignment as an assumption that it might be in the final solution.
This assumption is checked to be correct or not by recursively calling the DPLL function again with the new partial assignment.

If the \verb+decide+ function selects literals that are in the final solution then the search for whether the formula is satisfiable is quickly resolved.
However, if it returns literals that are not in the final solution, the search can be extensive and costly.
Therefore, the efficacy of this selection has a great impact on the overall efficiency of the DPLL algorithm.

\subsection{Advancements in SAT Solvers}
Though the DPLL algorithm is the basis of most modern SAT solvers, the actual implementations have been significantly altered to increase efficiency.
Some changes, including the use of conflict learning, backjumping, and watched literals, are briefly described here.
This section should give a broad overview of the techniques used in current SAT solvers, 
in order to show that their application to problems like component evolution is justified. 

\subsubsection{Conflict Learning and Backjumping}
Conflict learning \citep{stallman1976} is a technique to cache previously tried sets of assignments in order to stop re-solving the same sub-problems.
This is accomplished through remembering what clauses where unit to add literals to the partial assignment, 
these clauses are also known as the reasons for a literal to be in the partial assignment.
This process works by identifying a variable both inferred to be true and false,
then creating a new clause, known as the learnt clause, which stops that inconsistency being reached again.
This learnt clause is derived by disjoining the two reason clauses after removing both the references to the inconsistent variable.
This process is defined in figure \ref{impl.clauselearning}. 

\begin{figure}[htp]
\begin{center}
$\begin{array}{c}
\{a_1,\ldots,a_k, l\} \\
\{b_1,\ldots,b_m,\neg l\}\\
\hline
\{a_1,\ldots,a_k, b_1,\ldots,b_m \}
\end{array}$
  \caption{Clause Learning Definition}
  \label{impl.clauselearning}
\end{center}
\end{figure}

For example, if the reason for the infered literal $a$ is clause $\{a, b\}$, and the reason for $\neg a$ is clause $\{\neg a, c\}$,
then the learnt clause derived is $\{b,c\}$, and added to the formula.

Backjumping \citep{Gaschnig1979} is the technique which determines how far to up the search tree to backtrack when a conflict is found.
The higher up the tree the technique ``jumps'' to, the greater reduction of the search space.
This algorithm typically depends on the learnt clause, where the place that is jumped to is when the clause becomes unit. 

More advanced methods of conflict learning occur by minimising the size of the learnt clauses, as presented in \cite{sorensson2009}.
This research describes search methods that use other reason clauses to find smaller more succinct leanrt clauses.
The smaller the clause, the more of the search tree is pruned and the more levels are backjumped through the search.

\subsubsection{Watched Literals}
As noted by studies into the efficiency of DPLL-based SAT solvers \citep{dixon2004automating}, unit propagation is where the bulk of the computation occurs.
Attempts to increase the efficiency of this task was initially to find better heuristics \cite{JamesMCrawford1996} for the literal order, to encourage cascades of unit propagation.
These attempts were shown to work well on random SAT problems but be less efficient for large structured problems \citep{dixon2004automating}.

It was noted that within unit propagation most of the time was spent on identifying the unit clauses.
The naive approach to unit propagation was to examine every clause, and then every literal in the clause to find if it is unit or not.
A more efficient approach was proposed using watched literals \citep{Madigan2001}, where instead of having the clauses examined, 
the clauses maintain an index of the necessary literals and notify the algorithm when they become unit.
This ``don't call us, we will call you'' concept makes the efficiency of the unit propagation function less dependent on the amount of clauses in the formula.

Advances on watched literals have occurred through algorithms to maintain the index of literals, like that presented in \cite{Moskewicz2001}.
Such algorithms enable larger formulae to be solved without necessarily increasing the time to solve them.

\subsection{Pseudo-Boolean Extension of SAT Solvers}
%%%Optimisation of SAT solvers is typically done through extending their possible constraints to include Psuedo Boolean inequalities
A typical extension of the DPLL algorithm is through adding the ability to handle pseudo-Boolean (PB) constraints \citep{dixon2004automating}.
These constraints consist of a linear inequality over Boolean variables.
Through this extension the method of pseudo-Boolean optimisation can be used to define criteria and optimise the evolution of component systems.

\subsubsection{Pseudo-Boolean Representation}
A pseudo-Boolean function takes a set of Boolean literals and returns a real number, i.e. $f:B^n \rightarrow \mathbb{R}$ where $B = \{0,1\}$.
A pseudo-Boolean constraint is then an inequality relating the function to a number, e.g. $f(x) \leq 3$.

This is a broad definition of these constraints.
To be efficiently implemented the pseudo-Boolean representation has been restricted to integers and a linear equation.
That is,
\begin{defs}
A pseudo-Boolean function is a linear function over Boolean literals $x$ of the form

$f(x) = a_1x_1 + b_1\neg x_1 + a_2x_2 + b_2\neg x_2 \ldots +  a_nx_n + b_n\neg x_n$

where $x_i \in \{0,1\}$ and fixed integers $a_i$ and $b_i$.
\end{defs}
For example, the pseudo-Boolean function $f(x) = 1x_1 + 0\neg x_1 + 0x_2 + 3\neg x_2$, when taking the input $\langle x_1, \neg x_2 \rangle$ will return $4$.
This function can be simplified by removing all members whose coefficient is $0$, e.g. $f(x) = 1x_1 + 3\neg x_2$.
It can also be represented by a set of pairs where each pair a literal and a coefficient, e.g. $f(x) = \{ (1,x_1), (3,\neg x_2) \}$.

This function can become a constraint such that:
\begin{defs}
A pseudo-Boolean constraint is a linear inequality over Boolean literals $x$ and pseudo-Boolean function $f$ of the form

$f(x) \geq k$ , $f(x) > k$, $f(x) \leq k$ or $f(x) < k$

where $k$ is a fixed integer
\end{defs}

For example, the pseudo-Boolean function $f(x) = 1x_1 + 3\neg x_2$ can be used to create the constraint $f(x) > 2$.
This constraint will be satisfied by the literals $\langle x_1, \neg x_2 \rangle$ as $f(x) = 4$, but not by the literals $\langle x_1,  x_2 \rangle$ as $f(x) = 2$.
A pseudo-Boolean constraint can be therefore be represented by a pseudo-Boolean function $f$, an inequality and an integer.

Such constraints, can be translated into the standard CNF, but the original pseudo-Boolean representation has been shown to be exponentially more concise \citep{dixon2004automating}.
Also given the proper amendments to unit propagation and other algorithms (some of which is described in \cite{Sheini2006}), 
it can be faster to find solutions to problems represented in pseudo-Boolean constraints rather than their translated SAT constraints \citep{dixon2004automating}.
Both of these reasons give ample justification to use the pseudo-Boolean extension to DPLL when mapping the component evolution problem to a set of constraints. 

\subsection{MiniSat and SAT4J}
MiniSAT presented in \cite{een2003}, is a simple SAT solver implementation written in C, and designed for speed and extensibility.
It uses the DPLL based conflict driven algorithm as discussed above.
This solver has become popular and is the basis of many other SAT solvers due to its open source distribution.
This has also lead to a track in the 2011 SAT competitions\footnote{http://www.satcompetition.org/2011/} that deals with only altering MiniSAT to increase performance.
This means that MiniSAT has been repeatedly validated for performance by third parties across many difference SAT problems. 

SAT4J \citep{le2010sat4j} is a Java re-implementation, and extension, of MiniSAT in the Java programming language.
The extensions SAT4J makes to MniSAT include the ability to find resolve pseudo-Boolean constraints.
SAT4J was developed in order to quickly test combinations of advancements in SAT solving technology.
This goal has created an easily modifiable and transparent implementation, able to be used in various circumstances.

\section{GJSolver}
%%%My Implementation, cut the fat, straight forward
Through the course of this research an implementation grew out of the need to have a modifiable base to experiment with component dependency resolution.
Other implementations of CDR where often component model specific, or involved difficult (or impossible) to modify code.
The CDR implementation created through this research is known as GJSolver, 
and is designed to solve CDR problems represented in CUDF with optimisation descriptions in Mancoosi format.

\subsection{Requirements of GJSolver}

Given the context under which GJSolver was developed, the set of requirements that it was designed according to are briefly listed here:
\begin{enumerate}
  \item \textbf{Mancoosi International Competition Ready}: The MISC gives a set of standards to solve CUDF problems with criteria defined using the Mancoosi optimisation format.
  A goal of the GJSolver is to be entered in the MISC in order to be compared against other solvers, and be validated to show the implementation is correct.
  Following these standards ensures that the solver can be entered, and then competing can show the relative speed of the GJSolver implementation.
  This will ensure that GJSolver is a valid and efficient implementation. 
  \item \textbf{Anytime Algorithm}: Return a solution, even if it is not the optimal solution, within a predefined amount of time. 
  CDR problems can be large and complex, finding the ``best'' solution can be difficult. 
  This requirement ensures that the algorithm will return a solution in a practical time frame.
  \item \textbf{Easily Creatable Criteria}: The ability to quickly implement and test criteria in a manner that enables experimentation, will increase the speed of research.
\end{enumerate}

The implementation details in the GJSolver that relate to these requirements will be discussed in the 

\subsubsection{Mancoosi International Solver Competition}
Given a goal of the GJSolver implementation is to compete in the MISC, the interface and standards defined for this competition must be followed.
How the entered solvers are executed, what environment they are executed in, and the output required are all important aspects to the development of GJSolver.

%%%They are executed on the command line
The way in which the entered solvers are executed must be a standard allowing for the automation of the majority of the competition.
The entered solvers should be able to be executed on the command line with three arguments, \verb+cudfin+, \verb+cudfout+ and \verb+criteria+.
These arguments are defined as:
\begin{itemize}
  \item \verb+cudfin+: is a relative path to a CUDF document (as specified in section \ref{formal.cudf}) that describes the problem to be solver.
  \item \verb+cudfout+: is a relative path to a non-existent file which is created by the solver to output the solution
  \item \verb+criteria+: is a Mancoosi optimisation format (as described in section \ref{formal.mancoosioptimisationformat}) that describes the 
\end{itemize}
The format of the output to the file whose relative path is given in the \verb+cudfout+ argument is a sub set of CUDF.
This output is only the package name, version and installed properties of the package description stanzas of only the packages that are installed in the new system are required.
This removes the preamble and request stanza, also all superfluous package information to simplify and limit the size of the output.

%%%The environment POSIX, with 5minutes 1GB of memory
The environment in which the solver is executed is a virtual machine running a GNU/Linux system in a x86 architecture with 1GB of memory (RAM).
It contains a Java runtime environment, allowing the use of Java as a primary language.
The time in which the solver is allowed to run is 5 minutes, after this is will return a result of ABORT at which time the solver will be forcibly executed.
This time limit either ensures that your algorithm returns a result quickly or is an anytime algorithm, where it can be return a result when interrupted during the search.
The latter is the option that has been selected for GJSolver, and will be discussed in the next section.

\subsubsection{Anytime Algorithm}
As component system evolution is NP-Hard, the time required to find an optimal solution can be impractical.
For this reason, GJSolver will be implemented with an anytime algorithm at its core.
An anytime algorithm can return a valid solution to a problem if it is interrupted before it ends.
This interruption could be a user or some other stimuli from another source.
In the case of GJSolver, the interruption will be caused by an internal timer dedicated to ensure that the algorithm does not run in time exceeding the 5 minute deadline.

Such anytime algorithms therefore create a trade-off between time and optimality, where the more time that is set aside for the algorithm to run the more optimal the solution. 
They can also create inconsistent results, as any input may, on different runs, return many different possible solutions.
This means that if it is necessary to interrupt the algorithm, it is possible to return non-optimal solutions.
However, this is only if interuption is necessary, as GJSolver will encounter a wide range of difficult problems, 
it is difficult, or impossible, to judge beforehand if a problem will require the algorithm to be interrupted. 

\subsubsection{Expendable Criteria}
The ability to test and experiment with a wide range of possible criteria to use in CDR is a goal of this research.
This requirement is one which must be defined
To enable this, GJSolver was developed in a modular manner, in which the criteria can be extended, modified, and tested when solving problems quickly.

\subsection{Implementation Decisions}
%%%First decision we base our solver on Eclipse P2
The first decision that was made in the development of the GJSolver, was to model GJSolver on another successful CDR solver, Eclipse P2 \citep{le_berre_dependency_2009,leBerre2010}.

%%%The advantages to this decision
There are many advantages to basing GJSolver on P2; firstly P2 had already been modified to enter MISC, so had various tools (like a CUDF parser) that reduced the work significantly.
The developers of Eclipse P2, are also the developers of the internal SAT solver SAT4J,
This may of lead to difficulties in making changes as the dependencies between the two may have become intertwined.
However, due to the requirement that P2 is deployed inside of the Eclipse framework, P2 and SAT4J are separated into OSGi bundles for easy reuse.
Most importantly of all, both SAT4J and P2 are open source making internal investigation of issues, and modification possible reducing risk and increasing reuse.

GJSolver however is not just a modification of Eclipse P2, there where some core problems that would make such an effort impractical.
Firstly, Eclipse P2 is a solver for the OSGi/Eclipse component models, and has much superfluous code and platform specific details.
The way in which P2 solves CUDF problems, is to first change them into OSGi problems through an internal data structure that does not resemble CUDF.
This additional layer of abstraction makes error checking, and solution checking a difficult process as the problem no longer directly resembles the original description.
The implementation of criteria in Eclipse P2 is also difficult to alter as again the system is built for optimising OSGi specific CDR problems.

Once this decision was made, this constrained the implementation of GJSolver to being Java based, 
using SAT4J as a core and implementing a pseudo-Boolean optimisation algorithm.
The conversion from CUDF into SAT and PB constraints for use by SAT4J is firstly discussed.
Lastly the method of pseudo-Boolean optimisation is discussed as the chosen method to find optimal solutions to component evolution.

\subsubsection{CUDF Mapping}
In chapter \ref{formal} the abstract definition of the problem was described using four types of constraints, keep, dependency, conflict and request.
The mapping of a CUDF document to a set containing such instances was also described, where the request constraints where also extended.

The mapping from these four constraints to the SAT and pseudo-Boolean constraints is described here.

Given the definitions above, where a formula $F$ is a set of clauses, a clause is a set of literals, and a literal is either a variable or its negation;
The set of variables are then defined as the set of component $C$, therefore the literals are components or their negation.

The mapping to this formalism from the keep, dependency, conflict constraints are: 
\begin{itemize}
  \item keep constraints of the form $a_1\vee \ldots \vee a_n$  can be represented as a clause $\{a_1,\ldots,a_n\}$
  \item dependency constraints of the form $a \rightarrow c_1 \vee \ldots \vee c_n$ can be represented as the clause $\{\neg a, c_1, \ldots, c_n\}$
  \item conflict constraints of the form $a \rightarrow \neg c$ can be represented as a clause $\{\neg a, \neg c\}$
\end{itemize}

The request constraints, which were extended in the CUDF mapping, can be mapped as:
\begin{itemize}
      \item install constraints of the form $a_1 \vee \ldots \vee a_n$  can be represented as a clause $\{a_1,\ldots,a_n\}$
      \item remove constraints of the form $\neg a$ can be represented as a clause $\{\neg a\}$
      \item update constraints have two clause types; one $\neg a$ can be represented as a clause $\{\neg a\}$;
      the second $a_1 + \ldots + a_i = 1$ can be represented by the pseudo-Boolean clause with literals $\langle a_1,\ldots,a_i\rangle$, coefficients $\langle 1_1,\ldots,1_i \rangle$
      and integer $1$, and the clause $\{a_1,\ldots,a_i\}$.
\end{itemize}

The update constraints of the form $a_1 + \ldots + a_i = 1$ accomplish the requirement of updated request to have exactly one version of a set of versions installed in the system.
This constraint could be mapped to SAT constraints in CNF efficiently as described in \cite{silva2007}.
However, as the requirement to use a pseudo-Boolean solver has already been defined, the most direct method is to map it to a PB constraint.

\subsubsection{Pseudo-Boolean Optimisation}
%%%Map psuedo boolean optimisation to formal optimisation definition
The method that Eclipse P2 uses to optimise the component evolution problem is through a method called pseudo-Boolean optimisation (PBO).
PBO involves trying to assign the Boolean variables values in such a way to either minimise or maximise a pseudo-Boolean function.

This optimisation method can be shown to be consistent and mapped to the framework defined in chapter \ref{formal}.
Two complete lattices are defined to either minimise or maximise natural numbers; $L_{min} = \langle \mathbb{N}, \geq \rangle$ and $L_{max} = \langle \mathbb{N}, \leq \rangle$.
Both these lattices are totally orders sets, so trivially shown to have a join, meet and be partially ordered.
Given the set of variables $V$, these lattices are used to form either a minimising or maximising  ranking system;
$RS_{min} = \langle L_{min}, \{F,T\}, V \rangle$ and $RS_{max} = \langle L_{max}, \{F,T\}, V \rangle$. 

A solution $\gamma$ in this ranking system can be mapped to a set of literals $l$
where all variables in the system are mapped to a positive literal if they return $T$ or a negative literal otherwise.
That is, $l = [v \mid v \in V \wedge \gamma(v) = T] \cup [\neg v \mid v \in V \wedge \gamma(v) = F]$

The ranking function ($Rank$) for either of these ranking systems must be defined to be a pseudo-Boolean function.
However, a different pseudo-Boolean function exists for each previous solution, as during search for a new system the previous system does not change.
This means that $Rank(l_1,l_2)$ will equal a pseudo-Boolean function given a previous solution, i.e. $f_{l_1}(l_2)$.
This function can be represented by a set of pairs of coefficients to literals as described in previous sections.

For example, the criteria to minimise the amount of components added to the system can be defined in such a manner.
The function $Rank^{added}(l_1,l_2)$ can be defined to return the size of the set of components that are a negative literal in $l_1$ and a positive literal in $l_2$,
i.e. $Rank^{added}(l_1,l_2) = |[x \mid x \in \mathbb{C} \wedge \neg x \in l_1 \wedge x \in l_2]|$.
Given the restrictions on PBO, this function must be able to be able to be defined as pseudo-Boolean function, otherwise it is unable to be used.
This function $f_{l_1}(l_2)$ is created by counting all literals that are negative in the previous system, that are positive in the proposed solution,
i.e. $f_{l_1}(l_2) = [(1, x) \mid x \in \mathbb{C} \wedge \neg x \in l_1]$.
This function can then be used to search for a solution.

The way in which such pseudo-Boolean equations are minimised through the use of pseudo-Boolean constraints is discussed in the following sections. 

\subsection{Iterative Strengthening}
The algorithm known as iterative strengthening presented in \cite{calistri1994iterative} and \cite{le2010sat4j}, 
describes an anytime algorithm that iteratively finds better solutions.
This is done by first finding a solution, then iteratively adding constraints to the formula that ensure the next solution found will be better than the previous.
This continues until either the strengthened formula is found to be unsatisfiable, or the algorithm is interrupted, at which point the best solution currently found is returned. 
This algorithm is defined in figure \ref{impl.strength}.

\begin{figure}[htp]
\begin{center}
\begin{alltt}
find-solution(\(F\),strengthen):
    \(answer\) = DPLL(\(F\))
    if \(answer\) = UNSAT:
        return UNSAT
    do:
        \(model\) = \(answer\)
        \(constraints\) = strengthen(\(model\))
        \(F\) = \(F \cup constraints\)
        \(answer\) = DPLL(\(F\))
    while not interrupted() and \(answer\) != UNSAT
    return \(model\) 
\end{alltt}
  \caption{Pseudo code of Iterative Strengthening Algorithm}
  \label{impl.strength}
\end{center}
\end{figure}

The first aspect to note in this algorithm is that the \verb+DPLL+ function has been altered slightly.
This function now takes the formula $F$ as a parameter and return a solution, $answer$, that satisfies $F$, 
or if the formula is unsatisfiable then it returns the flag \verb+UNSAT+.

The first action in this algorithm is to check if the formula is satisfiable, by assigning the output of the \verb+DPLL+ function to the variable $answer$.
If $answer$ is the flag \verb+UNSAT+, then the algorithm stops and returns that the formula is unsatisfiable.

The main loop of this function first assigns the contents of the variable $answer$ to the variable $model$.

The function \verb+strengthen+ is then called on $model$, to generate a set of constraints that when added to the formula ensure that only solutions more optimal are satisfiable.
Therefore, given some function $d$ is measures how optimal a solution is, $model_1$ satisfies $F$, and $constraints = $ \verb+strengthen+$(model_1)$;
any $model_2$ that satisfies $F \cup constraints$ must be of the relation $d(model_1) < d(model_2)$.
That is, that given some measurement of optimal, adding the constraints ensure that only more optimal solutions satisfy the formula with the strengthened constraints. 

These constraints are then added to the formula, and another $answer$ is found using \verb+DPLL+.
This function is the mechanism to find the optimal solution given some criteria.
As such, the criteria must be able to be represented in the formula, as to create constraints that will find more optimal solutions.

To ensure that this algorithm will end, and not just have \verb+DPLL+ repeatedly return the same answer to the formula, 
the found $model$ must not satisfy the formula with the strengthened constraints.
If this is the case, the number of answers that \verb+DPLL+ can return will decrease by at least one per iteration.

This main loop will continue until the function \verb+interrupted+ returns true, which may happen given some external cause (like a user stopping the algorithm or a timer running out),
or the strengthened formula is found to be unsatisfiable.
At this point the previously best solution discovered stored in the variable $model$ will be returned.

\subsubsection{Lexicographic Optimisation}
%%%Basic iterative strengthing can be modified towards handling lexicographically ordered crtiera, and allowing this criteria to effect DPLL's efficiency
The basic iterative strengthening algorithm, presented in figure \ref{impl.strength}, can be modified to handle lexicographically related criteria, so as to solve component evolution problems.
By adding an additional outer loop, including other necessary alterations, the original algorithm has been modified to handle lexicographical criteria.

The original iterative strengthening algorithm with the lexicographical extension has been implemented in GJSolver. 
This altered algorithm is presented in figure \ref{impl.lexstrength}.

\begin{figure}[htp]
\begin{center}
\begin{alltt}
find-lex-solution(\(F\),criteria):
	\(F\) = \(F \cup \) criteria.initial-constraints()
    \(answer\) = DPLL(\(F\))
    if \(answer\) = UNSAT:
        return UNSAT
    do:
        current-criterion = criteria.pop()
        do:
            \(model\) = \(answer\)
            \(constraints\) = current-criterion.strengthen(\(model\))
            \(F\) = \(F \cup constraints\)
            \(answer\) = DPLL(\(F\))
        while not interrupted() and \(answer\) != UNSAT
        \(F\) = \(F \backslash constraints\)
        \(F\) = \(F \cup \) current-criterion.lock(\(model\))
        \(answer\) = \(model\)
    while not interrupted() and criteria.size() != 0
    return \(model\) 
\end{alltt}
  \caption{Pseudo code of the Lexicographic Iterative Strengthening Algorithm}
  \label{impl.lexstrength}
\end{center}
\end{figure}

The first alteration to the original algorithm is that this version takes two parameters, firstly the formula denoted as $F$;
secondly  a stack of criteria, where the top of the stack is the criteria most lexicographically important.

The first action of this algorithm is to include initial criteria constraints,
these constraints may define new literals that can be used in other criteria functions.
These constraints are just a union of sets of constraints given by individual criteria. 

The next action of this algorithm is to check whether the formula is satisfiable,
and if it is not, return unsatisfied.

A new outer loop is defined, that loops over the criteria until either there is none left or it is interrupted.
This loop find optimal solutions for each criteria then moves to the next.
The first action is to $\verb+pop+$ off the currently most important criteria and store it in the variable \verb+current-criterion+.

Then the inner loop based from original algorithms main loop is then defined.
This main difference between this and the originals inner loop is the \verb+strengthen+ function is now defined per criteria from the function \verb+current-criterion.strengthen+.
This means the \verb+strengthen+ function changes throughout the outer loop, as it optimises for different criteria.
As this inner loop iterates more optimal solutions are found, until no their exist no more to be found.

When the \verb+current-criterion+ has strengthened the formula to make it unsatisfiable, the constraints that made it unsatisfiable are removed.
This is necessary to continue the search for more optimal solutions judged by the next criteria.
The formula then has constraints from the function \verb+current-criterion.lock+ added to it, this ensures no worse solution will be found on the next iteration.

These loops will iterate until either all the criteria have been optimised, or they are interupted.
The result is a set of literals that satisfy the constraints in the variable $model$.
These literals can be easily mapped to a set of components that are non-negative literlas in the model.
That is, the resulting system of components is equal to $[c \mid c \in \mathbb{C}$ and $c \in model]$.

\subsubsection{Criteria}
%%%This algorithm shows the use of some methods that criteria must implement
This new algorithm allows us to describe the necessary requirements a criteria must define in order to be used.
Each criteria must define the functions \verb+strengthen+ and \verb+lock+ to be used in this algorithm.
Further more, through altering the \verb+decide+, or literal order, function in the DPLL algorithm, each criteria could make the search for optimal solutions more efficient.
This will help guide the search to near optimal solutions, which will lower the upper bound of the number of loops necessary to find the most optimal solution.

%%%Initial constraints
Some criteria also require the ability to add constraints to the formula before the outer loop starts,
this is implemented in the function \verb+criteria.initial-constraints+.
These constraints are typically used to measure quantities that have no direct variable available.
The typical case for this is that of component names, where one criteria states that minimise the removed component names from the solution.
There is no variable for a component name, therefore one must be added to the formula for the amount of component names to be measured and used in optimisation.
For instance, if component $\langle a,2\rangle$ was removed and component $\langle a,3 \rangle$, the component name $a$ still exists in the solution therefore no change has been made.
Further description of how initial constraints are used is given in chapter \ref{strategies}. 

\subsubsection{Formal Criteria}
Each criterion in the stack \verb+criteria+ contains a pseudo-Boolean equation that either requires minimisation or maximisation.
This equation, represented by a list of literals and coefficients, is used to define the \verb+strengthen+ function.

The ranking function \verb+criteria+.$f$ is the pseudo-Boolean function that defines the criteria.
This function is created through the $Rank$ function of the criteria, which takes the previous solution to build the pseudo-Boolean function that defines the criteria (as described above).

Given a criterion's pseudo-Boolean function $f$,
the function \verb+strengthen+ the criterion is defined to return a pseudo-Boolean constraint such that:
\begin{itemize}
  \item if the function is being minimised then the returned constraint is $\langle f,<,f(l_2) \rangle$
  \item if the function is being maximised then the returned constraint is $\langle f,>,f(l_2) \rangle$
\end{itemize}
The function \verb+lock+ is similarly defined except the inequalities $>$ and $<$ are replaced respectively with $\geq$ and $\leq$.

\subsection{Drawbacks of this Optimisation approach}
%%%There are a few drawbacks to this mapping;
Given this algorithm and these requirements, there are a few drawbacks to this approach.
These include, the optimisation criteria must be linear and real numbers cannot be represented with integer pseudo-Boolean constraints.

%%%The optimisation must be represented linearly
As noted in \cite{le_berre_dependency_2009} and \cite{leBerre2010} there is no easy solution to extending a SAT solver to handle non-linear constraints.
Given the requirement that GJSolver uses a SAT solver as its core, there is no practical way using non-linear constraints for optimisation.
This stops the definition of some criteria, however this also keeps the criteria practical and efficiently solvable.   

%%%Real numbers must be truncated to fit the integer representation. 
The pseudo-Boolean constraints use only integer coefficients as it allows the inequality to be easily converted from $\leq$ to $>$ and so on.
This stops the use of real numbers in the definition of criteria, and thus the representation of different values, like percentages.
The most direct solution to this is to multiply any real number by a large scale and truncate the prodcut to an integer representation.
Scaling a value in such a manner, may remove a small amount of resolution of the value, though it does allow for such criteria to use real values numbers.

%%%These are necessary to keep the implementation practical
These simplifications are seen as necessary in order to keep the problem manageable.
Including all aspects of our formal optimisation framework would be practically impossible, and limit the efficiency of finding any solutions.

\subsection{Verification}
\label{impl.verif}
%%%We entered this solver into two Mancoosi MISC competitions
The process of verification of the GJSolver implementation was through entering it into the Mancoosi International Solver Competition, whose requirements are described above.
This process was taken twice, firstly in a MISC Live event, which is an interim competition held during the year;
secondly at the MISC 2011 event, 
which is the main competition, whose results are announced at the Workshop on Logics for Component Configuration\footnote{http://www.pps.jussieu.fr/~treinen/lococo/2011/}.

\subsubsection{Tracks and Scoring}
As MISC Live and MISC are competitions to compare solvers entered by different developers and researchers,
criteria to select the ``best'' solver must be defined through a scoring system.

Each competition is broken down into three possible tracks, each defined by the criteria used to solve the set of problems.
The first basic track, is 'paranoid', the second more advanced track is 'trendy', and third track is 'user'.
Both 'paranoid' and 'trendy' have pre-defined criteria in set lexicographic order for the solvers to use, 
however the 'user' track uses pre-defined set of criteria in different combinations to execute solvers.
This means that 'paranoid' and 'trendy' can have solutions tailored to their specific criteria that is required, where the 'user' track cannot.
The specific criteria and how they are defined, is discussed in the next chapter.

For each track, a set of solvers is entered.
Each track has a set of problems to solve, and all participating solvers are executed to return solutions to these problems.
Each solution is scored and all are summed to get a final score of a solver for a given track, where the solver with the lowest amount of points is victor.

The way in which a solution is scored is by first giving it one of three classes; a real solution; no solution; a incorrect solution.
A real solution is any solution that is correct; no solution occurs when a solver finished without output, this can happen because of error, timeout, or there not being a satisfiable solution;
an incorrect solution is the worst class, as it can cause an incorrect system to be created.

If $m$ is the number of solvers entered into the competition,
a real solution is given $1$ point if it is the best solution returned by any solver, and $1$ plus how ever many solvers found better solutions.
For example, if solver $s1$ found a solution where solvers $s2,s3,s4$ found a more optimal solution given the criteria of the track, then $s1$ gets $4$ points for that solution.
If no solution is returned then $2\times m$ are the points given, and if an incorrect solution is returned then $3 \times m$ points are given.

If more than one solver has the same amount of points at the end of a track, then the time it took for them to find each solution is summed and the solver that took the least time wins.
This is also a lexicographical order of points, where it is infinitely better to return good solutions, than to return solutions quickly.

\subsubsection{MISC Live}
%%%In the first competition we had only partially implemented much of the functionality, so we did not expect great results.
The first competitions, MISC Live, was entered without GJSolver being fully implemented.
Therefore, the only track that was possible to enter was the basic track 'paranoid'.
The results for this track\footnote{http://mancoosi.org/misc-live/20101126/paranoid/} where promising, though there where some clearly necessary improvements.
Due to the competition openly distributing the solutions and the output from the entered solvers, these problems could be analysed and solved easily.

\subsubsection{MISC}
The main verification of GJSolver was through the MISC 2011 event.
In this event GJSolver was entered into all tracks, where the 'paranoid' track had a total of 5 solvers, the 'trendy' track had a total of 6 solvers, 
and the 'user' track had a total of 4 solvers.
Each track was also entered by the solver which GJSolver is based on, Eclipse P2, and another very efficient solver aspuncud.

The scores and the times for each of the track compared to that from Eclipse P2 and aspuncud in table \ref{impl.misc2011}.
\begin{table}
\begin{tabular}{| l | c | c | c | c |}\hline
Track & \# of Problems & GJSolver(score:time) & P2(score:time) & aspuncud(score:time)\\ \hline
paranoid & 129 & (190 : 5,294) & (181 : 4,646) & (147 : 1,035) \\ \hline
trendy & 129 & (197 : 13,073) & (232 : 13,435) & (151 : 1,767) \\ \hline
user & 400 & (656 : 73,522) & (1392 : 87,956) & (1215 : 39,905) \\ \hline
\end{tabular}
\caption{Results from MISC 2011}
\label{impl.misc2011}
\end{table}

The total winners for each track where for both 'paranoid' and 'trendy' aspuncud won, and for the 'user' track GJSolver won.

\subsubsection{Analysis}
The main analysis of the results from the MISC competition is accomplished through comparing them to the previously stated requirements that
GJSolver should be \textbf{Mancoosi International Competition Ready}, should implement an \textbf{Anytime Algorithm} and should be \textbf{Easily Creatable Criteria}.

The results from MISC show that GJSolver was ready for its entrance, during the competition it had very consistent results.
These results allowed it to compete with the other solvers, and even win the 'user' track.

The implementation of the anytime algorithm was shown through the many hard problems required to be solved.
For example, many of the problems in the 'trendy' track required a lengthy search, and timed out requiring the return of a solution before the optimal was found.
Looking at many of these timed out problems, reveals that many of them returned optimal solutions regardless.
This leads to the presumption that the finding of the optimal solution is not the time consuming part of the search, but the proof that there are no better solution remaining.

The criteria that were defined in to enter this competition were wide ranging, and will be specifically discussed in chapter \ref{strategies}.
It may be said that the reason for GJSolvers win of the 'user' track could be put down to the easily defined and tuned criteria that allowed the quick testing of different heuristics.


%\section{Other Methods}
%This \cite{Stuckenholz2007} study looks at using boolean optimisation with branch and bound as a solution, as does \cite{Jenson2010a}.
%\subsection{Integer Programming}
%Discussion of this method as the best MISC solver uses this, it has a very complex implementation
%\subsection{SMT Solvers}
%SMT Solver, a slightly higher logic than SAT uses; it has to broad a definition when SAT suffices
%\subsection{Constraint Solver}
%We could just use Prolog, like SMT I think it is too broad when there are good SAT solvers


\section{Summary}
In this chapter first Boolean satisfiability solvers were introduced, 
describing the DPLL algorithm and its extensions through pseudo-Boolean constraints as a means to solve component evolution problems.
Then the implementation produced through this research, GJSolver, was introduced with the modified iterative strengthening algorithm to find optimal solutions.
Finally, the description of the MISC competition and GJSolver's results through it were given to show the verification of it as a suitable implementation to solve CUDF problems.
