\chapter{Implementation}
\label{implementation}
\epigraph{What I cannot create, I do not understand.}
{\textit{Richard Feynman, 1988}}

%%%CDR Has been shown to be NP-Complete to find a solution and NP-Hard to find an optimial one.
As presented in the previous chapter, the problem of evolving a component system was shown to be NP-Complete, and the problem of finding an optimal evolution was shown to be NP-Hard.
Therefore, to automate this complex evolution process it will require an efficient algorithm and implementation to not only find satisfiable solutions, but also optimal ones. 

%%%The efficiency of the implementation is important as the problem can combinatorially grow into a hard problem 
The efficiency of the implementations is necessary for many instances of CDR problems.
The goal of many component models is to have large amounts of components with support creating multiple versions of each component.
This directly makes the complexity of evolution grow in a combinatorial manner, making any ad-hoc or overly simple CDR implementation quickly overwhelmed.
The CDR algorithm should then be implemented to be robust and fast.

%%%Formally it resembles the SAT problem, which already has efficient implementations, and has been used to solve this problem
The formal representation of this problem was presented as a set of constraints that must all be satisfied in order for a component relationships and the user request to be fulfilled.
This structure closely resembles the Boolean Satisfiability Problem (SAT), which has fast, robust solver implementations that can be used.
This has been noticed by other researchers \citep{leberre2008,Mancinelli2006} 
and the use of SAT solvers to implement CDR in both academia \citep{abate2011} and industry \citep{leBerre2010} has become common.

%%%Finding the optimial solutions requires extensions to SAT solvers
The finding of an optimal solution though is difficult when using a ``pure'' SAT solver, as it cannot easily represent constraints relating to criteria. 
However, through extending the SAT solver implementations to also handle other types of constraints, such as pseudo Boolean constraints, optimisation becomes significantly easier.

%%%In this chapter\ldots
In this chapter the algorithms and implementation of CDR using SAT solvers extended with pseudo Boolean constraints, are discussed.
First the Davis-Putnam-Logemann-Loveland algorithm \citep{Davis1960, davis1962machine} and its extensions, which are the basis for many current SAT solvers, are described.
This will give a basic understanding of the internal workings of current SAT solvers.
this is given as a comparison of a current technology used by CDR implementations.
Then the description of this research's CDR implementation, GJSolver, is given.
The specific reason for the necessary changes, the differences from other solutions, and the advantages to using it.
This also describes the process in which it was validated and compared against other solvers in the MISC 2011 competition.

\section{Boolean Satisfiability Solvers}
\label{impl.SAT}
Boolean satisfiability (SAT) is the problem of determining if the variables in a Boolean equation can be assigned in such a way that the equation returns true.
This problem was the first identified NP-Complete problem, meaning that no known algorithm exists that efficiently solves all instances of SAT problems.
It has been used previously in this thesis as a means in chapter \ref{formal} as a means of showing component evolution as being NP-Complete.
This also means that many proofs of NP-Completeness is through the reduction to SAT, and therefore many difficult problems have been expressed in the terms of a Boolean equation.
The benefit of this is that algorithms and implementations of programs that solve SAT problems have received significant attention and research to provide solutions to these various problems.
Taking advantage of these algorithms will help with the creation of a CDR implementation that can robustly evolve a component system.

%%%SAT solvers are used to solve many problems, creating a community dedicated to creating fast implenmenations
SAT solvers have been used in various domains to tackle problems such as electronic design automation \citep{Marques-Silva2000}, 
model verification \citep{dennis2006}, and, of course, component system evolution \citep{leBerre2010}.
These uses have caused a great need to create efficient solvers, and this has spawned a community dedicated to creating efficient implementation.
This community, creates and improves their solvers, 
and as a means of validating their creations, regularly compare them against one another in a series of competitions \footnote{http://www.satcompetition.org/}.
Using a competition for progress not only encourages the improvement of solvers for a particular problem set, but also general improvements to the algorithms and heuristics used. 

%%%In this section a common implementation of SAT solvers is discussed, the DPLL algorithm
In this section, first a brief definition of the SAT problem is described, this defines much of the terminology in this domain used in the description of algorithms and implementations. 
Then a short description of the DPLL algorithm is given, this algorithm is the basis of most current SAT solvers and therefore is required knowledge to discuss current implementations.
A common variant of the DPLL algorithm in current SAT algorithms is the alteration to become conflict driven,
this alteration is presented and discussed.
The extension of this algorithm with Pseudo-Boolean constraints is then described,
as this extension allows for more complicated optimisation problems to be represented.
Finally, other methods of solving SAT problems are briefly discussed to give a broader description of the domain. 

\subsection{The SAT Problem}
As described above, the goal of a SAT algorithm is to find whether a set of variables in a Boolean equation can be assigned values in such a way as to make the equation equal true.
A common representation of such a formula is in Conjunctive Normal Form (CNF), this is defined as a conjunction of clauses, 
where each clause is a conjunction of literals, e.g. $(a \vee b) \wedge (\neg b \vee c)$.

This CNF representation of a SAT problem can be defined as such:
\begin{defs}
An instance of a SAT problem is a set of Boolean variables $V$ and a formula which is a set of clauses $F$.
Each clause in $F$ is a set of literals, where a literal is a variable $v$ or its negation $\neg v$.
A set of literals is said to be consistent if for any variable $v$, the set does not contain both $v$ and its negation $\neg v$.
A solution to a SAT formula $F$ is a consistent set of literals $P$, such that for every clause in $c$ in $F$ there exists a literal in $c$ that is also in $P$.
If there exists a $P$ that is a solution of a SAT formula $F$ the problem is said to be satisfiable, otherwise it is unsatisfiable. 
\end{defs}

That is, a SAT problem can have the variable $\{a,b,c\}$ and the formula $\{c_1,c_2\}$, where clauses $c_1 = {a,b}$ and $c_2 = {\neg b, c}$.
A solution for this problem could be $\{a,\neg b,c\}$ as $a \in c_1$ and $\neg b \in c_2$.
However $\{a, \neg b, b\}$ is not because it is not consistent, and $\{a, b, \neg c\}$ is not because it does not contain a literal in $c_2$.  

This definition also follows some classical logic rules, such as literals and their negation can be defined as $\neg \neg v = v$.
This rule can be extended to sets of literals where given a set of literals $P$, $\neg P = [\neg v \mid v \in P]$.

\subsection{Davis-Putnam-Logemann-Loveland algorithm for SAT Solvers}
%%%A successful algorithm for solving SAT problems is the DPLL algorithm, here we describe it in overview
The Davis-Putnam-Logemann-Loveland (DPLL) algorithm \citep{Davis1960, davis1962machine} for solving SAT problems is a complete (meaning it will find a solution if one exists), 
backtracking-based search algorithm for SAT problems represented in conjunctive normal form (CNF).

It runs by assuming a variable from the Boolean formula is true by adding it to the set of literals $P$ then calling itself to find if this new assignment is a solution.
It first takes this new set of literals and calculates literals that are inferred, in a process called unit-propagation. 
If this new set of literals contains a contradiction then the assumption was incorrect and it returns false, and negates the assumption to continue searching.
However, if this assumption leads to a solution it then returns that it is satisfiable, as it has found a solution that satisfies the formula.

The DPLL algorithm in defined in figure \ref{impl.DPLL} as presented in \cite{dixon2004automating}:
\begin{figure}[h]
\begin{center}
\begin{alltt}
function DPLL(\(F, P\))
   P = unit-propagate(\(F, P\))
   if \(P\) contains a contradiction:
       then return UNSATISIFABLE;
   if \(P\) is a solution to \(F\):
       then return SATISFIABLE;
   \(l\) = decide\((P)\);
   if DPLL\((F, P \cup \{l\})\)
       return SATISFIABLE
   else
       return DPLL\((F, P \cup \{\neg l\})\);
\end{alltt}
  \caption{Recursive DPLL algorithm}
  \label{impl.DPLL}
\end{center}
\end{figure}

The first \verb+if+ branch of the DPLL algorithm finds if partial assignment $P$ is consistent, if it is not it returns \verb+UNSATISIFABLE+.
The second \verb+if+ branch determines if the assignment $P$ is a solution to $F$, this would end the search by returning \verb+SATISFIABLE+.
The \verb+unit-propagation+ function and the \verb+decide+ function are both described in the following sections. 

\subsubsection{Unit Propagation}
The first part of this function is the call to infer new literals given the current assignment, this is known as unit-propagation.
Given a possible assignment, a clause is called unit if the assignment does not contain one literal, and contains all of its other literals negation.
That is, given an assignment $P$ and a clause $c$ if $|c \backslash \neg P| = 1$, then $c$ is unit.
A unit literal is then defined given a unit clause as the literal $l$ where $ c \backslash \neg P = \{l\}$.

This notion of ``unit'' allows a literal to be inferred to be in the solution given a current assignment.
For instance, given a formula $\{c_1,c_2\}$, where $c_1 = {a,b}$ and $c_2 = {\neg b, c}$, and the assignment $\{\neg a\}$;
clause $c_1$ is unit and literal $b$ is unit as $c_1 \backslash \neg \{\neg a\} = c_1 \backslash\{ a\} = \{b\}$.
Through unit propagation, $b$ can be inferred to be in the solution, as it must be to make $c_1$ true.
Therefore the assignment is extended to $\{\neg a, b\}$.
This new assignment then makes $c_2$ unit, which through unit propagation infers the assignment be $\{\neg a, b, c\}$, and so on.

The process of unit propagation is further defined in figure \ref{impl.propagation}
\begin{figure}[htp]
\begin{center}
\begin{alltt}
unit-propagate(\(F, P\)):
    while P is consistent and there exists a \(c \in F\) that given \(P\) is unit:
        \(l\) = unit literal in \(c\)
        \(P\) = \(P \cup \{l\}\)
\end{alltt}
  \caption{Pseudo code of Unit Propagation}
  \label{impl.propagation}
\end{center}
\end{figure}

\subsubsection{Literal Order}
The function \verb+decide+ returns a literal that is not unit, nor whose negation is unit, i.e. given $l =$ \verb+decide+$(F)$, $\{l\} \not \in F$ and $\{\neg l\} \not \in F$.
This literal is then added to the formula and checked for satisfiability.
If the formula is not satisfiable, then the negation is added to the formula and checked for satisfiability.  
The order in which the variables from the formula are selected will have a great impact on the speed at which the algorithm finds the formula to be satisfiable or not.

\subsection{Advancements in SAT Solvers}
Though the DPLL algorithm is the basis of most modern SAT solvers, the actual implementations have been altered to increase efficiency.
Some changes are briefly described here, these include the use of conflict learning, backjumping, and watched literals.
This section should give a broad overview of the techniques used in current SAT solvers, 
in order to show that their application to problems like component evolution is justified. 

\subsubsection{Conflict Learning and Backjumping}
Conflict learning \citep{stallman1976} is a technique to cache previously tried sets of assignments in order to stop re-solving the same sub-problems.
This is accomplished through remembering the clauses which inferred literals through unit propagation, these clauses are also known as reasons.
These reasons are then used to derive a clause which prunes exactly the branches which created the conflict.
This process basically works by identifying the inconsistent variable (a variable both inferred to be true and false) 
then creating a new clause from the clauses that is the reason for them being inferred.
This new clause is derived by disjoining the two reason clauses and removing both the references to the inconsistent variable.
This process is defined in figure \ref{impl.clauselearning}. 

\begin{figure}[htp]
\begin{center}
$\begin{array}{c}
\{a_1,\ldots,a_k, l\} \\
\{b_1,\ldots,b_m,\neg l\}\\
\hline
\{a_1,\ldots,a_k, b_1,\ldots,b_m \}
\end{array}$
  \caption{Clause Learning Definition}
  \label{impl.clauselearning}
\end{center}
\end{figure}

For example, if the reason for the infered literal $a$ is clause $\{a, b\}$, and the reason for $\neg a$ is clause $\{\neg a, c\}$,
then a new clause derived is $\{b,c\}$.
This new clause is then added to the formula, which then stops the process from stepping through the branches which caused this exact conflict to occur again.

Backjumping \citep{Gaschnig1979} is the technique which determines how far to up the search tree to backtrack when a conflict is found.
The higher up the tree that the technique ``jumps'' up the greater reduction of the search space.
This algorithm typically depends on the clause created through conflict learning in modern solvers.
This uses the derived learnt clause to determine the point in the search at which can be jumped to.
The search is backtracked to the level where the learnt clause becomes unit, this can significantly improve the performance when solving problems.

More advanced methods of conflict learning occur by minimising the size of the learn clauses, as presented in \cite{sorensson2009}.
This research describes search methods that use other reason clauses to find smaller more succinct conflict clauses.
The smaller the clause the more of the search tree is pruned and the more levels are backjumped through the search.
Therefore, significant improvements to the efficiency of modern SAT solvers is gained through the use of such methods.


\subsubsection{Watched Literals}
As noted by studies into the efficiency of DPLL-based SAT solvers \citep{dixon2004automating}, unit propagation is where the bulk of the computation occurs.
Attempts to increase the efficiency of this task was initially to find better heuristics \cite{JamesMCrawford1996} for the literal order, to encourage cascades of unit propagation.
These attempts were shown to work well on random SAT problems but be less efficient for large structured problems \citep{dixon2004automating}.

It was noted that within unit propagation most of the time was spent on identifying the unit clauses.
The naive approach to this solution was to examine every clause, and then every literal in the clause to find if it is unit or not.
The idea of watched literals \citep{Madigan2001} was added so that instead of having the clauses examined, 
the clauses maintain an index of the necessary literals that when added to the partial solution the clause becomes unit.
This ``don't call us, we will call you'' function makes the function of finding unit clauses less dependent on the amount of clauses in the formula.

Advances on watched literals have occurred through algorithms to maintain the index of literals, like that presented in \cite{Moskewicz2001}.
Such algorithms enable larger formulae to be solved without necessary increasing the time to solve them.

\subsection{Pseudo-Boolean Extension of SAT Solvers}
%%%Optimisation of SAT solvers is typically done through extending their possible constraints to include Psuedo Boolean inequalities
This DPLL algorithm can be adapted to solve problems with constraints that are not represented as CNF clauses.
Through extending DPLL to be able to handle such constraints, problems can be more easily expressed and solved.
A typical extension is through using pseudo-Boolean (PB) constraints \citep{dixon2004automating}, these constraints consist of a linear inequality over Boolean variables.
Through this extension the method of pseudo-Boolean optimisation can be used to define criteria and optimise the evolution of component systems.

\subsubsection{Pseudo-Boolean Representation}
A pseudo-Boolean function is a function that takes a set of Boolean literals and returns a real number, i.e. $f:B^n \rightarrow \mathbb{R}$ where $B = \{0,1\}$.
A psuedo-Boolean constraint is then an inequality relating the function to a number, e.g. $f(x) \leq k$.

This is a broad definition of these concepts, to be efficiently included into the DPLL algorithm the pseudo-Boolean representation has been restricted to a linear equation.
That is,
\begin{defs}
A pseudo-Boolean equation is a linear function over Boolean literals $x_i$ of the form

$\sum a_i x_i$

where $x_i \in \{0,1\}$ and fixed integers $a_i$ and $k$.
\end{defs}

That is, a pseudo-Boolean equation is a tuple formed by a list of literals $\langle x_1,\ldots,x_i \rangle$, 
a list of integers $\langle a_1,\ldots,a_i \rangle$.
If the literal $x_i$ resolves to true then in the constraint it is mapped to $1$, otherwise  if $x_i$ resolves to false it is resolved to $0$.
The product of all the mapped literals with their integer counterpart is then summed, resulting in a natural number. 

A pseudo-Boolean constraint is then defined as:
\begin{defs}
A pseudo-Boolean constraint is a linear inequality over Boolean literals $x_i$ of the form

$\sum a_i x_i \geq k$ , $\sum a_i x_i > k$, $\sum a_i x_i \leq k$ or $\sum a_i x_i < k$

where $x_i \in \{0,1\}$ and fixed integers $a_i$ and $k$.
\end{defs}

A pseudo-Boolean constraint is an equation with an inequality (either $\geq$,$>$,$\leq$, or $<$) and an integer $k$.

For example, a pseudo-Boolean constraint with a list of literals $\langle a, \neg b , c\rangle$, with integers $\langle 1, 2, 3\rangle$, inequality $\geq$, and $k$ value of $3$,
can be denoted as the inequality $1a + 2 \neg b + 3c \geq 3$.
For an assignment $\{a , \neg b, \neg c\}$, this equation would resolve to $1 + 2 + 0 \geq 3$, which is true as this assignment satisfies the inequality.

Such constraints, can be translated into the standard CNF, but the original pseudo-Boolean representation has been shown to be exponentially more concise \citep{dixon2004automating}.
Also given the proper amendments to unit propagation and other algorithms (some of which is described in \cite{Sheini2006}), 
it can be faster to find solutions to problems represented in pseudo-Boolean constraints rather than their translated SAT constraints \citep{dixon2004automating}.
Both of these reasons give ample justification to use the pseudo-Boolean extension to DPLL when mapping the component evolution problem to a set of constraints. 

\subsection{MiniSat and SAT4J}
MiniSAT presented in \cite{een2003}, is a simple SAT solver implementation written in C, and designed for speed and extensibility.
It uses the DPLL based conflict driven algorithm as discussed above.
This solver has become popular and is the basis of many other SAT solvers due to its open source distribution.
This has also lead to a track in the 2011 SAT competitions\footnote{http://www.satcompetition.org/2011/} that deals with only altering MiniSAT to increase performance.
This means that MiniSAT has been repeatedly validated for performance by third parties across many difference SAT problems. 

SAT4J \citep{le2010sat4j} is a Java re-implementation, and extension, of MiniSAT in the Java programming language.
The extensions SAT4J makes to MniSAT include the ability to find solutions to pseudo-Boolean constraints.
SAT4J was developed in order to quickly test combinations of advancements in SAT solving technology.
This goal has created an easily modifiable and transparent implementation, able to be used in various circumstances.

\section{GJSolver}
%%%My Implementation, cut the fat, straight forward
Through the course of this research an implementation grew out of the need to have a modifiable base to experiment with component dependency resolution.
Other implementations of CDR where often component model specific, or involved difficult (or impossible) to modify code.
The CDR implementation created through this research is known as GJSolver, 
and is designed to solve CDR problems represented in CUDF with optimisation descriptions in Mancoosi format.

\subsection{Requirements of GJSolver}

Given the context under which GJSolver was developed, the set of requirements that it was designed according to are briefly listed here:
\begin{enumerate}
  \item \textbf{Mancoosi International Competition Ready}: The MISC gives a set of standards to solve CUDF problems with criteria defined using the Mancoosi optimisation format.
  A goal of the GJSolver is to be entered in the MISC in order to be compared against other solvers, and be validated to show the implementation is correct.
  Following these standards ensures that the solver can be entered, and then competing can show the relative speed of the GJSolver implementation.
  This will ensure that GJSolver is a valid and efficient implementation. 
  \item \textbf{Anytime Algorithm}: Return a solution, even if it is not the optimal solution, within a predefined amount of time. 
  CDR problems can be large and complex, finding the ``best'' solution can be difficult. 
  This requirement ensures that the algorithm will return a solution in a practical time frame.
  \item \textbf{Easily Creatable Criteria}: The ability to quickly implement and test criteria in a manner that enables experimentation, will increase the speed of research.
\end{enumerate}

The implementation details in the GJSolver that relate to these requirements will be discussed in the 

\subsubsection{Mancoosi International Solver Competition}
Given a goal of the GJSolver implementation is to compete in the MISC, the interface and standards defined for this competition must be followed.
How the entered solvers are executed, what environment they are executed in, and the output required are all important aspects to the development of GJSolver.

%%%They are executed on the command line
The way in which the entered solvers are executed must be a standard allowing for the automation of the majority of the competition.
The entered solvers should be able to be executed on the command line with three arguments, \verb+cudfin+, \verb+cudfout+ and \verb+criteria+.
These arguments are defined as:
\begin{itemize}
  \item \verb+cudfin+: is a relative path to a CUDF document (as specified in section \ref{formal.cudf}) that describes the problem to be solver.
  \item \verb+cudfout+: is a relative path to a non-existent file which is created by the solver to output the solution
  \item \verb+criteria+: is a Mancoosi optimisation format (as described in section \ref{formal.mancoosioptimisationformat}) that describes the 
\end{itemize}
The format of the output to the file whose relative path is given in the \verb+cudfout+ argument is a sub set of CUDF.
This output is only the package name, version and installed properties of the package description stanzas of only the packages that are installed in the new system are required.
This removes the preamble and request stanza, also all superfluous package information to simplify and limit the size of the output.

%%%The environment POSIX, with 5minutes 1GB of memory
The environment in which the solver is executed is a virtual machine running a GNU/Linux system in a x86 architecture with 1GB of memory (RAM).
It contains a Java runtime environment, allowing the use of Java as a primary language.
The time in which the solver is allowed to run is 5 minutes, after this is will return a result of ABORT at which time the solver will be forcibly executed.
This time limit either ensures that your algorithm returns a result quickly or is an anytime algorithm, where it can be return a result when interrupted during the search.
The latter is the option that has been selected for GJSolver, and will be discussed in the next section.

\subsubsection{Anytime Algorithm}
As component system evolution is NP-Hard, the time required to find an optimal solution can be impractical.
For this reason, GJSolver will be implemented with an anytime algorithm at its core.
An anytime algorithm can return a valid solution to a problem if it is interrupted before it ends.
This interruption could be a user or some other stimuli from another source.
In the case of GJSolver, the interruption will be caused by an internal timer dedicated to ensure that the algorithm does not run in time exceeding the 5 minute deadline.

Such anytime algorithms therefore create a trade-off between time and optimality, where the more time that is set aside for the algorithm to run the more optimal the solution. 
They can also create inconsistent results, as any input may, on different runs, return many different possible solutions.
This means that if it is necessary to interrupt the algorithm, it is possible to return non-optimal solutions.
However, this is only if interuption is necessary, as GJSolver will encounter a wide range of difficult problems, 
it is difficult, or impossible, to judge beforehand if a problem will require the algorithm to be interrupted. 

\subsubsection{Expendable Criteria}
The ability to test and experiment with a wide range of possible criteria to use in CDR is a goal of this research.
This requirement is one which must be defined
To enable this, GJSolver was developed in a modular manner, in which the criteria can be extended, modified, and tested when solving problems quickly.

\subsection{Implementation Decisions}
%%%First decision we base our solver on Eclipse P2
The first decision that was made in the development of the GJSolver, was to model GJSolver on another successful CDR solver, Eclipse P2 \citep{le_berre_dependency_2009,leBerre2010}.

%%%The advantages to this decision
There are many advantages to basing GJSolver on P2; firstly P2 had already been modified to enter MISC, so had various tools (like a CUDF parser) that reduced the work significantly.
The developers of Eclipse P2, are also the developers of the internal SAT solver SAT4J,
This may of lead to difficulties in making changes as the dependencies between the two may have become intertwined.
However, due to the requirement that P2 is deployed inside of the Eclipse framework, P2 and SAT4J are separated into OSGi bundles for easy reuse.
Most importantly of all, both SAT4J and P2 are open source making internal investigation of issues, and modification possible reducing risk and increasing reuse.

GJSolver however is not just a modification of Eclipse P2, there where some core problems that would make such an effort impractical.
Firstly, Eclipse P2 is a solver for the OSGi/Eclipse component models, and has much superfluous code and platform specific details.
The way in which P2 solves CUDF problems, is to first change them into OSGi problems through an internal data structure that does not resemble CUDF.
This additional layer of abstraction makes error checking, and solution checking a difficult process as the problem no longer directly resembles the original description.
The implementation of criteria in Eclipse P2 is also difficult to alter as again the system is built for optimising OSGi specific CDR problems.

Once this decision was made, this constrained the implementation of GJSolver to being Java based, 
using SAT4J as a core and implementing a pseudo-Boolean optimisation algorithm.
The conversion from CUDF into SAT and PB constraints for use by SAT4J is firstly discussed.
Lastly the algorithm used to implement the pseudo-Boolean as this is the part of the problem that is most important to implement.

\subsubsection{CUDF Mapping}
In chapter \ref{formal} the abstract definition of the problem was described using four types of constraints, keep, dependency, conflict and request.
The mapping of a CUDF document to a set containing such instances was also described, where the request constraints where also extended.

The mapping from these four constraints to the SAT and pseudo-Boolean formula as defined in this chapter is described here.

Given the definitions above, where a formula $F$ is a set of clauses, a clause is a set of literals, and a literal is either a variable or its negation;
The set of variables are then defined as the set of component $C$, therefore the literals are components or their negation.

The mapping to this formalism from the keep, dependency, conflict constraints are: 
\begin{itemize}
  \item keep constraints of the form $a_1\vee \ldots \vee a_n$  can be represented as a clause $\{a_1,\ldots,a_n\}$
  \item dependency constraints of the form $a \rightarrow c_1 \vee \ldots \vee c_n$ can be represented as the clause $\{\neg a, c_1, \ldots, c_n\}$
  \item conflict constraints of the form $a \rightarrow \neg c$ can be represented as a clause $\{\neg a, \neg c\}$
\end{itemize}

The request constraints, which were extended in the CUDF mapping, can be mapped as:
\begin{itemize}
      \item install constraints of the form $a_1 \vee \ldots \vee a_n$  can be represented as a clause $\{a_1,\ldots,a_n\}$
      \item remove constraints of the form $\neg a$ can be represented as a clause $\{\neg a\}$
      \item update constraints have two clause types; one $\neg a$ can be represented as a clause $\{\neg a\}$;
      the second $a_1 + \ldots + a_i = 1$ can be represented by the pseudo-Boolean clause with literals $\langle a_1,\ldots,a_i\rangle$, coefficients $\langle 1_1,\ldots,1_i \rangle$
      and integer $1$, and the clause $\{a_1,\ldots,a_i\}$.
\end{itemize}

The update constraints of the form $a_1 + \ldots + a_i = 1$ accomplish the requirement of updated request to have exactly one version of a set of versions installed in the system.
This constraint could be mapped to SAT constraints in CNF efficiently as described in \cite{silva2007}.
However, as the requirement to use a pseudo-Boolean solver has already been defined, the most direct method is to map it to a PB constraint.

\subsubsection{Pseudo-Boolean Optimisation}
%%%Map psuedo boolean optimisation to formal optimisation definition
The method that 
The core reason for the inclusion of pseudo-Boolean constraints is to define component evolution as a pseudo-Boolean optimisation problem.
This optimisation involves trying to assign the Boolean variables values in such a way to either minimise or maximise a pseudo-Boolean equation.

This optimisation method can be shown to be consistent and mapped to the framework defined in chapter \ref{formal}.
Two complete lattices are defined to either minimise or maximise natural numbers; $L_{min} = \langle \mathbb{N}, \geq \rangle$ and $L_{max} = \langle \mathbb{N}, \leq \rangle$.
Both these lattices are totally orders sets, so trivially shown to have a join, meet and be partially ordered.
Given the set of variables $V$, these lattices are used to form either a minimising or maximising  ranking system;
$RS_{min} = \langle L_{min}, \{F,T\}, V \rangle$ and $RS_{max} = \langle L_{max}, \{F,T\}, V \rangle$. 

A solution $\gamma$ in this ranking system can be mapped to a set of literals $l$
where all variables in the system are mapped to a positive literal is they return $T$ or a negative literal otherwise.
That is, $l = [v \mid v \in V \wedge \gamma(v) = T] \cup [\neg v \mid v \in V \wedge \gamma(v) = F]$

The ranking functions of these ranking systems are defined to be either maximising ($Rank^{max}$) or minimising ($Rank^{min}$) a pseudo-Boolean equation.
The ranking function takes two solutions mapped to literals, to create a function that 
The list of literals and set of

These functions are defined to take two sets of literals, the previous solution and the proposed model and return a natural number to be either minimised or maximised.
These ranking functions are then defined as pseudo-Boolean equations by defining a list of literals and a list of natural number coefficients.
That is the function $Rank(l_1,l_2)$ must define a set literals $l$ and a set of coefficients $a$ to define a pseudo-Boolean equation. 

The function \verb+strengthen+ of the criteria then returns a pseudo-Boolean constraint, a tuple of literals, coefficients, inequality and natural number such that:
\begin{itemize}
  \item if the criteria function is $Rank^min$ with literals $l$ and coefficients $a$ then the returned constraint is $\langle l,a,<,Rank(l_1,l_2)\rangle$
  \item if the criteria function is $Rank^max$ with literals $l$ and coefficients $a$ then the returned constraint is $\langle l,a,>,Rank(l_1,l_2)\rangle$
\end{itemize}
The function \verb+lock+ is similarly defined except the inequalities $>$ and $<$ are replaced respectively with $\geq$ and $\leq$.

Therefor to define any criteria, only the intention to maximise of minimise a list of literals and list of natural numbers that form a pseudo-Boolean equation is necessary. 

\subsection{Iterative Strengthening}
The algorithm known as iterative strengthening presented in \cite{calistri1994iterative} and \cite{le2010sat4j}, 
describes an anytime algorithm that iteratively finds better solutions.
This is done by first finding a solution, then iteratively adding constraints to the formula that ensure the next solution found will be better than the previous.
This continues until either the strengthened formula is found to be unsatisfiable, or the algorithm is interrupted, at which point the best solution currently found is returned. 
This algorithm is defined in figure \ref{impl.strength}.

\begin{figure}[htp]
\begin{center}
\begin{alltt}
find-solution(\(F\),strengthen):
    \(answer\) = DPLL(\(F\))
    if \(answer\) = UNSAT:
        return UNSAT
    do:
        \(model\) = \(answer\)
        \(constraints\) = strengthen(\(model\))
        \(F\) = \(F \cup constraints\)
        \(answer\) = DPLL(\(F\))
    while not interrupted() and \(answer\) != UNSAT
    return \(model\) 
\end{alltt}
  \caption{Pseudo code of Iterative Strengthening Algorithm}
  \label{impl.strength}
\end{center}
\end{figure}

The first aspect to note in this algorithm is that the \verb+DPLL+ function has been altered slightly.
This function now takes the formula $F$ as a parameter and return a solution, $answer$, that satisfies $F$, 
or if the formula is unsatisfiable then it returns the flag \verb+UNSAT+.

The first action in this algorithm is to check if the formula is satisfiable, by assigning the output of the \verb+DPLL+ function to the variable $answer$.
If $answer$ is the flag \verb+UNSAT+, then the algorithm stops and returns that the formula is unsatisfiable.

The main loop of this function first assigns the contents of the variable $answer$ to the variable $model$.

The function \verb+strengthen+ is then called on $model$, to generate a set of constraints that when added to the formula ensure that only solutions more optimal are satisfiable.
Therefore, given some function $d$ is measures how optimal a solution is, $model_1$ satisfies $F$, and $constraints = $ \verb+strengthen+$(model_1)$;
any $model_2$ that satisfies $F \cup constraints$ must be of the relation $d(model_1) < d(model_2)$.
That is, that given some measurement of optimal, adding the constraints ensure that only more optimal solutions satisfy the formula with the strengthened constraints. 

These constraints are then added to the formula, and another $answer$ is found using \verb+DPLL+.
This function is the mechanism to find the optimal solution given some criteria.
As such, the criteria must be able to be represented in the formula, as to create constraints that will find more optimal solutions.

To ensure that this algorithm will end, and not just have \verb+DPLL+ repeatedly return the same answer to the formula, 
the found $model$ must not satisfy the formula with the strengthened constraints.
If this is the case, the number of answers that \verb+DPLL+ can return will decrease by at least one per iteration.

This main loop will continue until the function \verb+interrupted+ returns true, which may happen given some external cause (like a user stopping the algorithm or a timer running out),
or the strengthened formula is found to be unsatisfiable.
At this point the previously best solution discovered stored in the variable $model$ will be returned.

\subsubsection{Lexicographic Optimisation}
%%%Basic iterative strengthing can be modified towards handling lexicographically ordered crtiera, and allowing this criteria to effect DPLL's efficiency
The basic iterative strengthening algorithm, presented in figure \ref{impl.strength}, can be modified to handle lexicographically related criteria, so as to solve component evolution problems.
By adding an additional outer loop, including other necessary alterations, the original algorithm has been modified to handle lexicographical criteria.

The original iterative strengthening algorithm with the lexicographical extension has been implemented in GJSolver. 
This altered algorithm is presented in figure \ref{impl.lexstrength}.

\begin{figure}[htp]
\begin{center}
\begin{alltt}
find-lex-solution(\(F\),criteria):
	\(F\) = \(F \cup \) criteria.initial-constraints()
    \(answer\) = DPLL(\(F\))
    if \(answer\) = UNSAT:
        return UNSAT
    do:
        current-criterion = criteria.pop()
        do:
            \(model\) = \(answer\)
            \(constraints\) = current-criterion.strengthen(\(model\))
            \(F\) = \(F \cup constraints\)
            \(answer\) = DPLL(\(F\))
        while not interrupted() and \(answer\) != UNSAT
        \(F\) = \(F \backslash constraints\)
        \(F\) = \(F \cup \) current-criterion.lock(\(model\))
        \(answer\) = \(model\)
    while not interrupted() and criteria.size() != 0
    return \(model\) 
\end{alltt}
  \caption{Pseudo code of the Lexicographic Iterative Strengthening Algorithm}
  \label{impl.lexstrength}
\end{center}
\end{figure}

The first alteration to the original algorithm is that this version takes two parameters, firstly the formula denoted as $F$;
secondly  a stack of criteria, where the top of the stack is the criteria most lexicographically important.

The first action of this algorithm is to include initial criteria constraints,
these constraints may define new literals that can be used in other criteria functions.
These constraints are just a union of sets of constraints given by individual criteria. 

The next action of this algorithm is to check whether the formula is satisfiable,
and if it is not, return unsatisfied.

A new outer loop is defined, that loops over the criteria until either there is none left or it is interrupted.
This loop find optimal solutions for each criteria then moves to the next.
The first action is to $\verb+pop+$ off the currently most important criteria and store it in the variable \verb+current-criterion+.

Then the inner loop based from original algorithms main loop is then defined.
This main difference between this and the originals inner loop is the \verb+strengthen+ function is now defined per criteria from the function \verb+current-criterion.strengthen+.
This means the \verb+strengthen+ function changes throughout the outer loop, as it optimises for different criteria.
As this inner loop iterates more optimal solutions are found, until no their exist no more to be found.

When the \verb+current-criterion+ has strengthened the formula to make it unsatisfiable, the constraints that made it unsatisfiable are removed.
This is necessary to continue the search for more optimal solutions judged by the next criteria.
The formula then has constraints from the function \verb+current-criterion.lock+ added to it, this ensures no worse solution will be found on the next iteration.

These loops will iterate until either all the criteria have been optimised, or they are interupted.
The result is a set of literals that satisfy the constraints in the variable $model$.
These literals can be easily mapped to a set of components that are non-negative literlas in the model.
That is, the resulting system of components is equal to $[c \mid c \in \mathbb{C}$ and $c \in model]$.

\subsubsection{Criteria}
%%%This algorithm shows the use of some methods that criteria must implement
This new algorithm allows us to describe the necessary requirements a criteria must define in order to be used.
Each criteria must define the functions \verb+strengthen+ and \verb+lock+ to be used in this algorithm.
Further more, through altering the \verb+decide+, or literal order, function in the DPLL algorithm, each criteria could make the search for optimal solutions more efficient.
This will help guide the search to near optimal solutions, which will lower the upper bound of the number of loops necessary to find the most optimal solution.

%%%Initial constraints
Some criteria also require the ability to add constraints to the formula before the outer loop starts,
this is implemented in the function \verb+criteria.initial-constraints+.
These constraints are typically used to measure quantities that have no direct variable available.
The typical case for this is that of component names, where one criteria states that minimise the removed component names from the solution.
There is no variable for a component name, therefore one must be added to the formula for the amount of component names to be measured and used in optimisation.
For instance, if component $\langle a,2\rangle$ was removed and component $\langle a,3 \rangle$, the component name $a$ still exists in the solution therefore no change has been made.
Further description of how initial constraints are used is given in chapter \ref{strategies}. 

\subsubsection{Formal Criteria}
\label{impl.formalcrit}
%TODO

\subsection{Drawbacks of this Optimisation approach}
%%%There are a few drawbacks to this mapping;
Given this algorithm and these requirements, there are a few drawbacks to this approach.
These include, the optimisation criteria must be linear and real numbers cannot be represented with integer pseudo-Boolean constraints.

%%%The optimisation must be represented linearly
As noted in \cite{le_berre_dependency_2009} and \cite{leBerre2010} there is no easy solution to extending a SAT solver to handle non-linear constraints.
Given the requirement that GJSolver uses a SAT solver as its core, there is no practical way using non-linear constraints for optimisation.
This stops the definition of some criteria, however this also keeps the criteria practical and efficiently solvable.   

%%%Real numbers must be truncated to fit the integer representation. 
The psuedo-Boolean constraints use only integer coefficients as it allows the inequality to be easily converted from $\leq$ to $>$ and so on.
This stops the use of real numbers in the definition of criteria, and thus the representation of different values, like percentages.
The most direct solution to this is to multiply any real number by a large scale and truncate the prodcut to an integer representation.
Scaling a value in such a manner, may remove a small amount of resolution of the value, though it does allow for such criteria to use real values numbers.

%%These are necessary to keep the implementation practical
These simplifications are seen as necessary in order to keep the problem manageable.
Including all aspects of our formal optimisation framework would be practically impossible, and limit the efficiency of finding any solutions.

\subsection{Verification}
%%%We entered this solver into two Mancoosi MISC competitions
The process of verification of the GJSolver implementation was through entering it into the Mancoosi International Solver Competition, whose requirements are described above.
This process was taken twice, firstly in a MISC Live event, which is an interim competition held during the year;
secondly at the MISC 2011 event, 
which is the main competition, whose results are announced at the Workshop on Logics for Component Configuration\footnote{http://www.pps.jussieu.fr/~treinen/lococo/2011/}.

\subsubsection{Tracks and Scoring}
As MISC Live and MISC are competitions to compare solvers entered by different developers and researchers,
criteria to select the ``best'' solver must be defined through a scoring system.

Each competition is broken down into three possible tracks, each defined by the criteria used to solve the set of problems.
The first basic track, is 'paranoid', the second more advanced track is 'trendy', and third track is 'user'.
Both 'paranoid' and 'trendy' have pre-defined criteria in set lexicographic order for the solvers to use, 
however the 'user' track uses pre-defined set of criteria in different combinations to execute solvers.
This means that 'paranoid' and 'trendy' can have solutions tailored to their specific criteria that is required, where the 'user' track cannot.
The specific criteria and how they are defined, is discussed in the next chapter.

For each track, a set of solvers is entered.
Each track has a set of problems to solve, and all participating solvers are executed to return solutions to these problems.
Each solution is scored and all are summed to get a final score of a solver for a given track, where the solver with the lowest amount of points is victor.

The way in which a solution is scored is by first giving it one of three classes; a real solution; no solution; a incorrect solution.
A real solution is any solution that is correct; no solution occurs when a solver finished without output, this can happen because of error, timeout, or there not being a satisfiable solution;
an incorrect solution is the worst class, as it can cause an incorrect system to be created.

If $m$ is the number of solvers entered into the competition,
a real solution is given $1$ point if it is the best solution returned by any solver, and $1$ plus how ever many solvers found better solutions.
For example, if solver $s1$ found a solution where solvers $s2,s3,s4$ found a more optimal solution given the criteria of the track, then $s1$ gets $4$ points for that solution.
If no solution is returned then $2\times m$ are the points given, and if an incorrect solution is returned then $3 \times m$ points are given.

If more than one solver has the same amount of points at the end of a track, then the time it took for them to find each solution is summed and the solver that took the least time wins.
This is also a lexicographical order of points, where it is infinitely better to return good solutions, than to return solutions quickly.

\subsubsection{MISC Live}
%%%In the first competition we had only partially implemented much of the functionality, so we did not expect great results.
The first competitions, MISC Live, was entered without GJSolver being fully implemented.
Therefore, the only track that was possible to enter was the basic track 'paranoid'.
The results for this track\footnote{http://mancoosi.org/misc-live/20101126/paranoid/} where promising, though there where some clearly necessary improvements.
Due to the competition openly distributing the solutions and the output from the entered solvers, these problems could be analysed and solved easily.

\subsubsection{MISC}
The main verification of GJSolver was through the MISC 2011 event.
In this event GJSolver was entered into all tracks, where the 'paranoid' track had a total of 5 solvers, the 'trendy' track had a total of 6 solvers, 
and the 'user' track had a total of 4 solvers.
Each track was also entered by the solver which GJSolver is based on, Eclipse P2, and another very efficient solver aspuncud.

The scores and the times for each of the track compared to that from Eclipse P2 and aspuncud in table \ref{impl.misc2011}.
\begin{table}
\begin{tabular}{| l | c | c | c | c |}\hline
Track & \# of Problems & GJSolver(score:time) & P2(score:time) & aspuncud(score:time)\\ \hline
paranoid & 129 & (190 : 5,294) & (181 : 4,646) & (147 : 1,035) \\ \hline
trendy & 129 & (197 : 13,073) & (232 : 13,435) & (151 : 1,767) \\ \hline
user & 400 & (656 : 73,522) & (1392 : 87,956) & (1215 : 39,905) \\ \hline
\end{tabular}
\caption{Results from MISC 2011}
\label{impl.misc2011}
\end{table}

The total winners for each track where for both 'paranoid' and 'trendy' aspuncud won, and for the 'user' track GJSolver won.

\subsubsection{Analysis}
The main analysis of the results from the MISC competition is accomplished through comparing them to the previously stated requirements that
GJSolver should be \textbf{Mancoosi International Competition Ready}, should implement an \textbf{Anytime Algorithm} and should be \textbf{Easily Creatable Criteria}.

The results from MISC show that GJSolver was ready for its entrance, during the competition it had very consistent results.
These results allowed it to compete with the other solvers, and even win the 'user' track.

The implementation of the anytime algorithm was shown through the many hard problems required to be solved.
For example, many of the problems in the 'trendy' track required a lengthy search, and timed out requiring the return of a solution before the optimal was found.
Looking at many of these timed out problems, reveals that many of them returned optimal solutions regardless.
This leads to the presumption that the finding of the optimal solution is not the time consuming part of the search, but the proof that there are no better solution remaining.

The criteria that were defined in to enter this competition were wide ranging, and will be specifically discussed in chapter \ref{strategies}.
It may be said that the reason for GJSolvers win of the 'user' track could be put down to the easily defined and tuned criteria that allowed the quick testing of different heuristics.


%\section{Other Methods}
%This \cite{Stuckenholz2007} study looks at using boolean optimisation with branch and bound as a solution, as does \cite{Jenson2010a}.
%\subsection{Integer Programming}
%Discussion of this method as the best MISC solver uses this, it has a very complex implementation
%\subsection{SMT Solvers}
%SMT Solver, a slightly higher logic than SAT uses; it has to broad a definition when SAT suffices
%\subsection{Constraint Solver}
%We could just use Prolog, like SMT I think it is too broad when there are good SAT solvers


\section{Summary}
In this chapter first Boolean satisfiability solvers were introduced, 
describing the DPLL algorithm and its extensions through pseudo-Boolean constraints as a means to solve component evolution problems.
Then the implementation produced through this research, GJSolver, was introduced with the modified iterative strengthening algorithm to find optimal solutions.
Finally, the description of the MISC competition and GJSolver's results through it were given to show the verification of it as a suitable implementation to solve CUDF problems.
