\chapter{Simulation}
\label{simulation}
\epigraph{A model is a physical, mathematical, or logical representation of a system entity, phenomenon, or process. 
A simulation is the implementation of a model over time. 
A simulation brings a model to life and shows how a particular object or phenomenon will behave.}
{\textit{Systems Engineering Fundamentals. Defense Acquisition University Press, 2001}}

To study the CSE process is the main purpose of this thesis.
This study could be in the form of either looking at real systems and users, studying a system in a controlled environment or simulating the environment in an abstract manner.
Each of these options is discussed and weighed for their costs and benefits.
The simulation method is selected as it has the best attributes required for this research.

The methodology outlined by \cite{Law2005} is used to create a valid and credible simulation.
The basic artifact of this methodology is a ``conceptual model'', which is a simplified abstraction of reality.
A conceptual model can be used to create a CUDF* documents that can then be simulated to study the CSE process.
These relationships are described in figure \ref{sim.modeldiagram}.



\begin{figure}[htp]
\begin{center}
\digraph[scale=.5]{simmodeldiagram}{
rankdir=BT;
CMS[label=<<TABLE BORDER="0" CELLBORDER="1" CELLSPACING="5"><TR><TD WIDTH="150">CUDF*</TD></TR></TABLE>> shape=none];
subgraph {
	rank=same;
	CMI[label=<<TABLE BORDER="0" CELLBORDER="1" CELLSPACING="5"><TR><TD WIDTH="150">:CUDF*</TD></TR></TABLE>> shape=none];
	ConMI[label=<<TABLE BORDER="0" CELLBORDER="1" CELLSPACING="5"><TR><TD WIDTH="150">:Conceptual Model</TD></TR></TABLE>> shape=none];
}
ConM[label=<<TABLE BORDER="0" CELLBORDER="1" CELLSPACING="5"><TR><TD WIDTH="150">Conceptual Model</TD></TR></TABLE>> shape=none];
CMI -> CMS [ label="instantiates"];
ConMI -> ConM [ label="instantiates"];
ConMI -> CMI [ label="creates"];
}
  \caption[labelInTOC]{figureCaption}
  \label{sim.modeldiagram}
\end{center}
\end{figure}



\section{The study of Component System Evolution}
To study the CSE process, one must examine and investigate how a component system evolves in response to a request to change. 
This requires access to many component systems state before an after a change, and the requests that instigate the change.

To obtain this information three methods could be used:
\begin{enumerate}
  \item field data could be collected from real systems evolving ``in the wild''.
  \item a controlled environment could be created, in which systems are monitored as users are asked to request changes.
  \item a simulation of a systems evolution could be run, where variables of the simulation are altered to represent different environments and situations.
\end{enumerate} 

In this section the positive and negative aspects of each of these approaches are discussed,
and the reasons for the selection of the simulation method for this research are presented.

\subsection{Using Real Users and Systems}
Studying real systems that are evolved by real users is one method of studying CSE.
Users could participate by installing monitoring software in their component systems that collects data.
This data could then be used to study the process by which their systems evolve.

This approach  has significant drawbacks:
\begin{itemize}
  \item it will take long periods of time to find meaningful results.
  \item the monitoring of user's actions is an invasion of their privacy, which may limit the pool of willing participants.
  \item finding and convincing enough users to participate to generate useful results will require significant effort.
\end{itemize}

\subsection{Users in a Controlled Environment}
To increase the efficiency of collecting results and to eliminate privacy issues of users, a controlled environment may be created.
This environment is a component system where the parameters of its evolution could be controlled and monitored.
A user would then be asked to evolve the system to approximate their real actions, the resulting systems could then be studied.

Such a controlled environment could very quickly simulate  many interactions that would of occurred over a long period.  
It also removes the necessity user trust as the user is altering a system that is not theirs.
However, it will also produce less valid results, as it is still an approximation of a real system, and the users know this.
Any controlled environment would alter how the user interacts with their controlled system, making the results less reliable.

However, the largest problem with this method is the effort required to conduct the experiment on enough users to find significant results.
The core difficulty with this method is finding and convincing enough users to participate, and then monitoring and supporting them while they are in the controlled environment.

\subsection{Simulating the Problem}
CSE can be studied by simulating the users actions and the component system using reasonable approximations.
A simulation is a surrogate of the real system, 
such that it represents the core aspects of the reality while simplifying and abstracting away unnecessary detail.
The goal of a simulation is not to represent every aspect of the real world, 
but to make a ``close enough'' approximation so that the conclusions drawn from it are valid while minimising complexity.

When analysing the results and forming conclusions from a simulation, 
the assumptions and abstractions made, must be taken into account.
Therefore, the majority of the effort of creating a simulation is making sure the results from the simulation are valid.

\subsection{Why Simulate?}
The advantages of simulating are the speed at which ideas can be tested and evaluated;
the cost to test and get results; and the control over variables and configuration of the environment.

The core steps to generate new knowledge is the iterative process of creating a hypothesis, then testing its accuracy.
As the speed of this process increases, the greater amount of knowledge and meaningful information that can be generated. 
Using real users, or real systems, requires long periods of waiting and preparation where little or no progress is made.
However, if the actions and environment can be modeled and executed computationally, these times of no progress are eliminated.

%%%Cost
The cost of testing a hypothesis can be measured in the amount of time it takes to define an experiment and to collect the results.
Using real users requires significant amount of time to collect and convince users to participate.
However, with a simulation the main effort is ensuring that the returned results are valid.
Once the simulations validity is established, it takes very little time to test a hypothesis.

%%%Control
The control over the variables in a simulation allows for testing extreme and normal situations.
It also allows for the sensitivity analysis of different variables, finding how important they are to the simulated system.

%%%Final words on why we simulate
A simulation of CSE is selected in this research as it has a cost/benefit ratio that is desirable,
and provides an acceptable level of accuracy to draw meaningful conclusions.

\section{Methodology}
%%%We use the methodology from `` Build a Valid and Credible Simulation''
The core hurdle in creating a simulation is validating the returned are similar to reality.
To create a valid and credible simulation the methodology that \citep{Law2005} outlines is followed.
This methodology is a guideline for defining the study, collecting information, creating and validating models, and running the simulation.
In this section the methodology is described and aligned to this research's objectives. 

\subsection{Validation and Credibility}
%%%Why do we use this methodology and how is it relevant?
This methodology was created after the observation that validation was often ``attempted after the simulation models had already been developed'' \citep{Law2005}.
That is even if validation was attempted, it may only occurred if there was money and time left at the end of the project.
However, such simulations, that are not validated, can produce erroneous information that leads to bad, possibly costly decisions being made.
This reduces the credibility of the simulation to be used in future as a tool.

A simulation is an abstraction and simplification of reality, often created as using an actual system can be disruptive, not cost-effective, or simply impossible.
In this context,

\begin{quotation}
``\textit{Validation} is the process of determining whether a simulation is an accurate representation of the system, for the particular object of the study.'' \citep{Law2005}
\end{quotation}

The latter part is an important aspect of validation, as the accuracy of the simulation is directly dependent on the problem and questions the study addresses.
Therefore, the definition of the problem will directly lead to the modelling and scope of the simulation.

%%%What is credible
A simulation, and by extension its results, have \textit{credibility} if key stakeholders accept them as ``correct''.
A credible model is not necessarily valid, and vice-versa, as it involves the input of a person who decides if the goals of the simulation have been obtained.
Credibility of a simulation is then only attainable if the key personale from the project understand and are involved directly with the project.

The simulation produced by this study to identify the effect of different strategies on the evolution of component systems must be validated to produce meaningful results,
and must be credible for these results to be trusted.

\subsection{Seven Step Method}
The methodology presented in \citep{Law2005} has a seven step approach to creating a valid and credible simulation.
These steps are; formulating the problem; collecting information and data to construct a conceptual model; validating the conceptual model;
implementing (programming) the model; validating the programmed model; designing, conducting and analysing experiments; and documenting and presenting the simulation results.

\subsubsection{Step 1: Formulate the problem}
The first step is to formulate the problem as clearly as possible, this is usually done with core stakeholders in a ``kick-off meeting''.
The core artifacts from this step are the overall objectives of the study, specific questions wanting to be answered, scope of the study,
 and different configurations of the simulation with the measures used to evaluate their performance. 

\subsubsection{Step 2: Collect information and data to construct conceptual model}
The conceptual model is a description of how the simulation and system work relative to the problems earlier defined.
It is the most important artifact of the simulation.
It should be high level enough to be understood by the core stakeholders but detailed enough to be reused in future simulations.
It is created through interviews with subject matter experts and collecting relevant data like results from similar exiting systems.
Problems like the data not being representative of the model, not being in the appropriate format or type, and containing errors must be handled before use.

The conceptual model also contains all of the variables that can be configured including their documented assumptions. 
It is defined to the level of detail with respect to project objectives, performance measures, data availability, computer constraints, and resource constraints.

\subsubsection{Step 3: Conceptual model validation}
The conceptual model is the most important aspect of the simulation, thus its validation must be thorough.
The core method used to validate this model, is to discuss it with core-stakeholders and subject matter experts.
This provides feedback as to the direction of the conceptual model, ensuring that it will answer the questions posed in the study.

\subsubsection{Step 4: Implement the conceptual model}
The implementation of the conceptual model must also be executed and documented in a way that allows others to replicate and repeat the process.
The artifacts created during this process must be verified to work correctly, this can be accomplished through test-cases and debugging \citep{Pressman1992}.

\subsubsection{Step 5: Validate implementation}
There is no completely definitive approach to validating the simulation,
however, the most definitive test of a simulations validity is established by closely looking at the outputted results compared to that from an actual system \citep{Law2005}.

This is done through:
\begin{itemize}
  \item \textbf{Results validation: } a comparable system is used to create results and compared with the results from the simulation for validation.
  \item \textbf{Face Validation: } experts are given output of the simulation model and checked to see if it is consistent with how they perceive the system should operate.
\end{itemize}

Further validation of the implementation can be accomplished with sensitivity analysis,
which is performed on the simulation to find the factors with the greatest impact on the performance and results.

\subsubsection{Step 6: Design, conduct and analyse experiments}
For each of the experiments that are run, the time and number of independent runs must be defined.
If the results are inconclusive, or other aspects of interest have arisen, it may be necessary to run additional experiments.

\subsubsection{Step 7: Document and present results}
This step involves the presentation of the conceptual model, simulation, and results to the core-stakeholders.
This presentation is critical for the future re-use of the model, as it should promote credibility through describing the validation process.

\subsection{Differences in methodology}
A core goal of creating this simulation study is to produce valid and credible results.
However, this specific methodology has been created for large scale industrial projects with substantial resources.
As this study is smaller in scale and resources some of the procedures recommended in this methodology have been restricted and some removed.

The most significant difference between this study and the described methodology is the clear definition between decision-maker and simulation designer.
In larger projects a simulation designer is contracted by a decision-maker who has a problem to solve or question to answer.
In this project both these people are the same person, therefore meetings between them are not required.

Other people in the project including subject-matter experts, core-stakeholders, simulation analysts consist of survey participants and project supervisors.
This is because the limits of the projects resources excludes the employment of experts for validation.
This may reduce the validity of the end model, but these restrictions have been made only when necessary,
and done so in a manner that attempts to minimise negative effects.

\section{Conceptual Model}
In this section the ``conceptual model'' of the component system evolution process is presented.
The c the conceptual model is defined to represent \textbf{realistic} component system evolutions.

This relation to reality is the core difference between the conceptual model and the \modelname model.
For example, \modelname could describe a situation where a user may requests to upgrade their system every minute for a year.
This situation is clearly unrealistic, and not able to be represented by the conceptual model.

In this section, the conceptual models of the user and repository are described.
How an instance of the conceptual model can be used to create a realistic component system evolution, described by a CUDF* document, is also described.

\subsection{Conceptual User}
The important aspects to the conceptual user are the initial component system, the user requests to be made to the system, and the times at which these requests are made. 

Installing a component system is the first action a user takes during the CSE process.
This initial system is represented in the conceptual model as a CUDF* document.

In CUDF* three different actions are defined that a user can request to alter their system; installing, removing or upgrading a component.
However, as shown in the user survey presented in chapter \ref{strategies} the two main actions of users are the installation of a component, and the upgrading of the entire system.

The times at which the user requests these changes is also important.
For example, a user will typically not request to upgrade their system every minute of the day as this would be seen as superfluous.
Reasonably, if an action occurs its max

%%%We have limited it to two actions, install and update, as they are the core actions a user executes
The variables that make up the conceptual user are: 
\begin{itemize}
  \item the initial system as a CUDF* document
  \item the probability a user selects to update the system per day
  \item the probability a user selects to install a component per day
  \item the probability a component is selected to be installed
  \item the MOF criteria used to select an optimal system for an update request
  \item the MOF criteria used to select an optimal system for an install request
  \item The number of days they exist
\end{itemize}

The initial system the user installs is also modelled here, as this is the starting point of any evolution.

\subsubsection{Install}
A user typically requests to install a component to extend the functionality of their component system.
How often does a user requests to install a package, and what packages is the user likely to request to install are interesting questions. 
simulating, the information of what component a user will install and when will a user select to install it are needed. 

%%%What probability a package will be selected to be installed
Determining the probability a user will select any component to be installed is difficult given the enormous amount of factors this relies on.
The users job, location, current tasks, previously installed software, favourite colour and numerous other aspects can determine what the user will select to install.
Given that all this information is impractical to simulate, this problem is abstracted into the form of two questions;
\begin{itemize}
  \item What components may a user select to install?
  \item How likely would a user select to install any of these?
\end{itemize}
This is represented by a weighted list of components, where the weights represent popularity;
e.g. component ``A'' has a 10\% chance of being selected as the component to install, or component ``B'' has a 5\% chance.

%%%The core problem with this is correlation between packages and user
The core problem with this weighting of components is that it ignores the correlation between probability a component will be installed. 
Two or more components may complement their functionality, thus making their installation together more likely.
This is impossible to represent with a weighted list, however this type of information is very difficult to calculate.
It would require the analysis of a significant amount of systems, which would probably cost more than the benefits gained.

The next question is what is the probability that a user will install a component, and how many components?
This is represented by a probability for the amount of components that a user may install on a day,
e.g. they have a 80\% chance of installing nothing, a 15\% chance of installing one component.

\subsubsection{Initial System}
%%%The initial system to start from is important, typically there are many to choose from
The starting system will effect how the system is evolved.
Therefore, selecting an initial system to start evolving is an important issue.

%%%What initial systems are there to chose from
Many component frameworks will release various configurations of components that satisfy many use cases.
This is analogous to software product lines \citep{clements2001software}, which increases the re-use of components and satisfy many different users with minimal effort.
The Eclipse framework offers more than ten types of initial Eclipse installs for different users.

The Ubuntu distribution offers three different systems for use, server, desktop and alternative.
Each of these also includes the choice of either amd64 or i386 chipsets.

The selection of the initial system can depend on many aspects of the simulation, which should be considered when defining the questions to be asked.


\subsection{Repository Model}
The set of components that exist at a given time.

\subsubsection{Time Range}
%%%The range of time in which to look at
The range of time over which the simulation is run will determine the repositories that are required.
It is also important as it should be long enough to draw conclusions from,
but as the simulation can take considerable resources to execute, too long and it may make it impractical to execute all the iterations necessary.
Other external aspects such as policy changes in the way in which a repository is run, or release cycles of the component system, 
must be considered when defining the time range.
These may have an effect on the results therefore should be considered in the conclusions drawn.

\subsubsection{Variables}

%%%this model contains a record of packages in repository over time
This model then contains one set of information:
\begin{enumerate}
  \item A daily record of components stored in the repository
  \item A time frame, start and finish, over which the simulation is run
\end{enumerate}

\subsection{Configuration}
The simulation configuration is a set of variables in which the simulation can be altered through.
To treat these variables as a set of inputs, their format and constraints must be defined.

The format for the initial system and criteria have been defined in previously chapters.
The initial system and the repository information will be store in the CUDF format, as described in chapter \ref{background}. 
The criteria to update and install will be defined in our modified MANCOOSI format defined in chapter \ref{criteria}.

Some inputs are trivially defined;
the update probability is a number between 0 and 1;
the time frame is defined as the dates of the days (given in seconds since the epoch at Jan. 1st 1970) between a given start date and the number of days that the simulation should run for.

These final inputs, the probability distributions of whether a component will be selected to be installed and how many components a user will select to install on a given day,
are represented using a set of pairs, each pair containing the action and the probability the action will be taken.



\subsection{Processes}
These processes are attempts to abstract and simplify the reality of the component system evolution.
The process that simulates the evolution of a component system takes a configuration and first generates a set of ``user actions" a user may take.
These user actions are then iteratively applied to the initial system though generating a CUDF problem,
then passing it to the resolver with the appropriate criteria to find the resulting solution.

The results of this process is then a set of systems that are created through the user actions.
These will be different along the dimensions of the configuration.



\paragraph{Initialisation}
%%%Initialize the algorithm
The \verb+generateUserActions+ algorithm first initializes three variables, \verb+userActions+, \verb+keeps+ and \verb+days+.
The \verb+userActions+ list is returned at the end of the algorithm.

The variable \verb+keeps+ is an initially empty list, that is contains all the components the user has previously selected to install.
This list is later translated into a ``keep: package'' property of the installed component with defined semantics in the CUDF specification.
This ensures that after a component has been selected to be installed by a user a component is not later removed by an action, like an update.




\paragraph{Differences to Reality}
The core differences from this model of a user to a real user are life cycle actions and correlation between actions.

A user when first installing a system will likely have components that they require that are not in the default system, and therefore when starting will install a lot of components.
This type of reasoning leads to life cycle actions, actions that are caused by events in the life cycle of the system.
Another example of this may be that at the end of each month a user decides to look through what they have installed and remove unneeded packages.
These actions add an extra dimension to the user model and process and will clearly change some results.
However, it has been opted out of the simulation as a precise set of possible actions and associated parameters are difficult to define.
Also as stated in their name they occur infrequently, if at all, and therefore it has been decided that they may not be significant.  

One action may also impact the occurrence of another action, so that they are correlated to happen together.
For example, a user may select to always update before they install, or to only remove a component directly after it has been installed.
The complexity that can be introduced by these correlations could make this simulation significantly more complex.
As these values may be different for each type of user and is probably more related to the person than the strategy they employ these have been deemed 



\section{Simulation Validation}
%%%Validation of this simulation, what needs to be validated/why it should be validated
The conceptual model presented is a simplified abstraction of the reality in which users evolve component systems.
It describes the variables that effect component system evolution as a configuration,
and the processors used to execute the simulation given a configuration.

%%%What if it is wrong
If some significant aspect of the system was missed, or if some aspect was incorrectly defined, the simulation may produce results that are incomprehensible,
or worse, misunderstood.
Therefore, the validation of these artifacts is essential to move forward. 
This validation was accomplished though regular stakeholder meetings, and an online survey with subject matter experts (as described in chapter \ref{strategies}).

\subsection{Stakeholder Meetings}
%%%Weekly meetings with stakeholders (i.e. supervisors)
As described in the methodology; one effort to validate these artifacts is done through meetings and a structured walk-through with the core stakeholders.
In this simulation the core stakeholders are the project researcher and supervisors.
These are the people who are asking the questions and are also impacted by the outcome, therefore they are directly effected by the validity of the results.
Meetings where held at regular intervals to ensure the projects progress and direction where correct.

\subsection{Subject Matter Expert Survey}
%%%Results from what else should be asked, install stuff not from repository, installs break
The survey described in chapter \ref{strategies}, 
was conducted at a point in the project when the conceptual model was just being developed, so had considerable impact on these artifacts.
The questions asked in this survey helped gauge the necessity and frequency of user actions,
so that only the most important aspects of the problem can be selected to be simulated.
It also filled in gaps of what was missing from the survey and model, giving direction for exploration.

\subsubsection{Frequency of User Actions}
%%%How often do users do these actions
The more frequently a user selects an action to evolve their system, the more important it is to the evolution of their system.
The information gained from the survey provides confidence that not including the action to remove a component and abstract the requirement multiple repositories, 
would not damage the validity of the results.
It also helped us define and represent the update and install actions in the configuration.

The remove action can be ignored as it seemed many of the users do not use this frequently.
When they do select to remove a component it is usually directly after installing it, if the selection to install a component was seen as a mistake.
Although it is clearly an important function to be included when evolving a system, the assumption is made that it is unnecessary for this simulation.

This survey also clearly shows that the main actions of a user is to update their system,
with the installation of a component the second most used action.
This survey also showed that the update occurs at more regular intervals than the install components.
Therefore, only the update and install user actions were included in this simulation and it also defined their representation. 

\subsubsection{Component Fault Exploration}
Another aspect of this problem that was mentioned by the subject matter experts through the survey was that a system may break during a change.
This typically then requires a reversion of the system components to a previously stable state.
Simulating this effect was ultimately deemed outside the scope of the project as it is seen as a rare occurrence with modern systems.
However, before it was eliminated, it was explored for possible inclusion.

The core problem with including simulated faults in a component systems evolution, is that each component has a different likelihood of causing a fault.
Different properties of a component like development process or complexity can impact this value.
Therefore, the function to calculate the likelihood of a component failing could rely on many different properties.

Instead of creating a function that tried to calculate the probability of failure per component,
an effort to measure it was attempted through a small study of component bug reports was attempted.

The feedback generated when a component causes a fault should ideally be a bug report, for the Ubuntu distribution these are filed on the project hosting server LaunchPad.
By using the Launchpad API to extract bug information, the amount of bugs per component in the system was able to be measured.
It was initially assumed that the number of systems that a component was installed on would increase the number of bug reports generated,
as more users means more eyes and systems to find bugs.
The Ubuntu Debian popularity contest was used then to see if this relationship existed the graph in figure \ref{bugsvspop} was created.

\begin{figure}[htp]
\begin{center}
  \includegraphics[width=\textwidth]{simulationpics/bugsvspopularity}
  \caption[Bugs v.s. Popularity]{A plot of the bugs a package has compared to its popularity, with notable outliers labeled}
  \label{bugsvspop}
\end{center}
\end{figure}

The first thing to note is that there seems to be very little relationship between the two variables, other than the package with the most bug reports is also one of the most used components.
The second thing to note is that the packages with the largest amount of bugs are ``apt'' and ``aptitude'' the two most popular package managers.
This could be because those packages are very buggy, or it could be because problems caused by apt, e.g. trying and failing to install a faulty package, may be reported as a problem with apt.

The last thing to notice is that there are many less popular components that have many bug reports.
When identifying the purposes of these packages many are used by developers, e.g. emacs21 is a popular text editor to program in.
The reason for their increased amount of bug reports may be that the users have prior experience and appreciation for the bug reports and the maintenance process, so file more bugs.

Measuring a components likely hood of failure using bug reports is then hypothesised to be impractical if not impossible,
as a component purpose and a components users may affect the results of the best measurement method available.
Other methods of finding a components likelihood of failure have not been further explored since this variable was eliminated from the configuration.
Though, it is expected that this is an intractable problem that is likely impossible to simulate accurately.

\subsection{Further Validation}
The assignment of the configuration variables is a different stage in the validation of this simulation.
Clearly if you create a configuration that is completely unrealistic, the saying ``garbage in, garbage out'' applies to the results.
However, this is not a concern when validating the conceptual model, or the abstract processes.
Further discussion of the validation of the assignment of the configuration variables is in chapter \ref{ubunutsimulation}.

\section{Summary}
{}In this chapter possible options were discussed for studying various strategies employed when evolving component systems.
{}Simulation, through the methodology \citep{Law2005} describes, was selected, and the steps involved were described.
{}The central artifact of this methodology, the conceptual model, was broken down into models of the user, repository and solver, and the processes of simulation.
{}These models were validated through regular meetings with the core stakeholders, and a survey conducted on subject matter experts.
{}In the next chapter the configuration of the simulation is further defined, and the questions about component system evolution are attempted to be answered.
