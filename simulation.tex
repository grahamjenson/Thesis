\chapter{Simulation}
\label{simulation}
{}To evaluate different strategies when evolving a component system, the environment in which these evolutions occur must be studied.
{}This study could be in the form of either looking at real systems and users, studying a system in a controlled environment or simulating the environment in an abstract manner.
{}Each of these options is discussed and weighed for their costs and benefits.
{}The simulation method is selected as it has the best attributes required for this research.

{}The methodology used to create the simulation is then discussed.
{}This methodlodology from \cite{Law2005} is an important element of this research as the validity of the simulation directly impacts the credibility of the results.   
{}The basic artifact of this methodology is a ``conceptual model'', this is an simplified abstraction of reality.
{}It describes the individual sub-models of the user, repository, and resolver; the variables in the configuration of the simulation;
{}and the processes in which these variables are used to simulate reality.
{}This chapter is finished with the discussion of the validation process of this conceptual model.

\section{Problem Definition}
The problem addressed in this thesis is outlines by our thesis question,

``What effect does using component resolution with different strategies have on system evolution?''.

Component resolution, evolution strategies, and component system evolution have been reviewed in previous chapters, and are summarized as:
\begin{itemize}
  \item \textbf{Component Resolution} is a function that takes a set of component constraints and returns an optimal system given a set of criteria
  \item An \textbf{Evolution Strategy} is the plan of action employed by a user through component resolution to evolve a system to attain the systems intents
  \item \textbf{Component System Evolution} is the process by which a component system is changed for maintenance or extension 
\end{itemize}

%%%What is the effect being measured to answer the question
Measuring the ``effect'' on the evolution of a component system when using different strategies is the result of the question. 
Though, what is this ``effect''?
An ``effect'' is a consequence caused by the action of using component dependency resolution to evolve a system.

%%%The most important effects are related to the intent of the strategy
The effects that are most closely studied are those that are a measurement of how effectively a strategy resolves its intents.
For example, if a server administrator uses a particular strategy in order to be up to date with minimum change.
The measurement of how up to date their system is per change made is an important effect to be measured.

%%%There is also side effect, what unintended consequences occur because of the use of a strategy
Other effects of the strategy are also important to measure.
The unintended consequences, side effects, of using a strategy can be important as well.
A given strategy may fulfill all its intents, though make the system worse through some other means.
For example, the server administrators strategy may keep a system up to date with little change, 
though it may increase internal complexity of the system making it unstable. 

\section{How to answer our thesis question}
%%%What we need to have to answer our question? A time line of systems created using different strategies.
To study the effects of different strategies on component system evolution requires the study of the systems and how they change over a period of time given the strategy employed. 
Each system along this evolutionary timeline can be studied and analysed for the effects of the strategy.
For a given strategy multiple different timelines must be collected, to eliminate the effects from the choices of the user and not the strategy.
These strategies can then be compared by the effects that are caused to the system.

%%%How to get this timeline of systems
To get this timeline of systems, real systems and users could be used and actual field data collected;
a controlled environment could be created, where users employ strategies on a system that is monitored;
or a simulation of a systems evolution is created, where different aspects are modelled and processed by an algorithm to generate the systems.
In this section we discuss the positive and negative aspects of each of these approaches,
and present the reasons for the selection of the simulation method for this research.

\subsection{Using Real Users and Systems}
%%%The pros and cons of using real systems with real users
Using real systems with real users would clearly produce real and valid data to analyse evolution strategies.
Users could be recruited to employ different strategies on their actual systems, while software monitoring their system would collect data.
Resolvers with strategy specific criteria would also have to be developed and distributed to the users.
After a period of time the information of the systems evolution could be retrieved and analysed.

%%%Now the cons %How about incomparible users/ if they use different systems from differntn starting points will haveto be handeled
This approach will create very accurate research, as the data would be real and the validity easily accessed. 
However, it has significant drawbacks,
it will take significant periods of time to create meaningful results, for each day of results a day has to pass.
The most difficult hurdle will be finding users that will trust an experimental resolver to alter their real system.   
As even the slightest error from a resolver can cause a system to become unstable, a user will likely not trust a resolver without a significant amount of validation.

%%%Users trust is difficult to gain
To gain the trust of possible users the experimental resolvers would have to be thoroughly tested through a managed development cycle and be well maintained.
Moving a package through this cycle can be itself a massive undertaking lasting several months. 
For example, the development of Eclipse Plug-ins involves a component going through four phases 
Proposal, Incubation, Mature, and Top-Level\footnote{http://www.eclipse.org/projects/dev\_process/development\_process\_2010.php};
similar to Debian's process of moving through the phases Unstable, Testing, Frozen, and finally stable\footnote{http://en.wikipedia.org/wiki/File:Debian-package-cycl.svg}.
After it has been through this cycle, maintenance of the resolver will likely be required.
This shows how much effort must be applied to earn a users trust, and this effort will likely outweigh the actual benefits.

\subsection{Users in a Controlled Environment}
%%%The pros and cons of using real users with pseduo systems
To remove the necessity for a users trust, and to increase the efficiency of collecting results, 
a study of users interacting with a system in a monitored and controlled environment could be executed.
Each user could be given different strategies and different tasks to perform on their system to approximate real interactions.

This study could compress a years worth of interactions with a system into an hour, and produce results from real users. 
This research also removes the necessity of user trust as the user is altering a system that is not theirs.
However, it will also produce less valid results, as it is still an approximation of a real system, and the users know this.
Any controlled environment would alter how the user interacts with their controlled system, making the results less reliable.
For example, a user typically researches a package before selecting installing it because it would have real implications, 
in a controlled environment these implications are removed so the selection would still be of questionable validity.

However, the largest problem with this method is the effort required to conduct the experiment on enough users to find significant results.
Each user must be found and organised into a location for them to be monitored using a system.
Given the amount of necessary users required, and the resources available to organise such an effort is a very limiting factor.
For example, given we want to test 10 different strategies 30 times each to find statistically significant results, 
and each subject taking upwards of an hour to complete the study.
This would be 300 hours of effort, not including the organisation and preparation going into the study.

\subsection{Simulating the Problem}
%%%Instead we can simulate, with an approximation of the real world.\\ 
%%%When drawing conclusions the accuracy of this approximation must be considered, therefore significant effort has gone into data collection and validation.
A simulation is a surrogate of the real system, 
such that it represents the core aspects of the reality it represents while simplifying and abstracting away unnecessary detail.
This is accomplished through modelling the most important parts of the system, 
and computationally creating and measuring the results. 
The goal of a simulation is not to represent every aspect of the real world, 
but to make a ``close enough'' approximation so that the conclusions drawn from it are valid while minimising complexity.

When analysing the results and forming conclusions from a simulation, 
the assumptions and abstractions made, must be taken into account.
Therefore, the majority of the effort when creating a simulation is defining the problem in such a way that the reality is simplified, but not too simple.

\subsubsection{Why Simulate?}
%%%What is good about the simulation over the other two methods, speed at which results are generated, the cost to get the results, the control over the variables in the simulation.
Simulation is superior method when compared to either using real users and systems, or using a controlled environment in the speed at which hypotheses can be tested and evaluated;
the cost to test and get results; and the control over variables and configuration of the environment.

%%%Speed at which results are generated
As the speed increases at which a hypothesis can go from being an idea on paper to being tested in a real or simulated environment, 
increases the amount of meaningful information that can be generated. 
When an idea is proposed as a possible solution to a problem, the quicker that idea can be eliminated or encouraged allows new ideas to be formed off of the original.
These ideas themselves can then be tested, to see if they are suitable solutions.
Using real users, or real systems, requires long periods of waiting and preparation.
If the actions and environment can be modeled and executed computationally these times of no progress are eliminated.
Through simulation an idea can be tested almost immediately by simultaneously testing multiple strategies and measuring outcomes.
The same cannot be said for using real users or system, where the turn around could be days or months to generate the same results.  

%%%Cost
The cost of getting the neccessary results can be measured in the amount of time it would take in organising, measuring and wasted time, for the the results to be collected.
Using a real user and system would involve an enormous amount of time validating and distributing the solvers (as discussed earlier), 
and the controlled environment would require many hours of organisation and planning to gather the necessary users, and then measure their interactions with the system.
However, with a simulation the main effort is ensuring that the models are accurate enough to gather meaningful results.
This time in validating the simulation and associated models is only necessary once, 
i.e. if more results are needed the simulation will not need to be re-validated.
The other two methodologies do not have this property, as for each user tested and measured will be a duplicate effort. 

%%%Control
The control over the results that a simulation returns far exceeds the other methods as models can be altered and tested where a real user cannot. 
This control allows the testing of extreme situations, the sensitivity analysis of different variables,
and the generation of possibly optimal but 'out of the box' ideas. 
For instance, measuring the difference between the update cycles that users have can be tested through altering the times between updates, then running the simulation.
If real users where used, the amount of users necessary to be found would be enormous to produce any significant results in this area.

%%%Final words on why we simulate
A simulation was used as it provides us with a cost/benefit ratio that is desirable, while potentially allowing an appropriate level of accuracy to draw meaningful conclusions.
The main effort is then to create the simulation with valid models that will give us the desired accuracy.

\section{Methodology}
%%%We use the methodology from `` Build a Valid and Credible Simulation''
{}The core hurdle in using a simulation is validating the results and conclusions, and assuring their credibility.
{}This places validity and credibility of a simulation and the results it provides at a high priority.
{}To create such a simulation the methodology outlined by \cite{Law2005} in ``How to build valid and credible simulation models'' is followed.
{}This methodology is a guideline for defining the study, collecting information, creating and validating models, and running the simulation.
{}In this section we introduce the methodology with its goals and how they are aligned to ours, 
{}discuss the steps that it states should be taken to create a valid simulation, and how the simulation will be created and what it will produce for this study.

\subsection{Validation and Credibility}
%%%Why do we use this methodology and how is it relevant?
This methodology was created after the observation that validation was often ``attempted after the simulation models had already been developed''.
That is even if validation was attempted, as it may of only occurred if there was money and time left at the end of the project.
However, such simulations, that are not validated, can produce erroneous information that leads to bad, possibly costly decisions being made.
This reduces the credibility of the simulation to be used in future as a tool.

%%%What is a valid simulation?
A simulation is an abstraction and simplification of reality, often created as using an actual system can be disruptive, not cost-effective, or simply impossible.
In this context,

``\textit{Validation} is the process of determining whether a simulation is an accurate representation of the system, for the particular object of the study.''

The later part is an important aspect of validation, as the accuracy of the simulation is directly dependent on the problem and questions the study addresses.
A simulation that is 99\% accurate may cost significantly more to produce than one that is 90\%, but provide little additional benefit given the specific information it is required to create.
Therefore, the definition of the problem will directly lead to the modelling and scope of the simulation.

%%%What is credible
A simulation, and by extension its results, have \textit{credibility} if key stakeholders accept them as ``correct''.
A credible model is not necessarily valid, and vice-versa, as it involves the input of a person who decides if the goals of the simulation have been obtained.
Credibility of a simulation is then only attainable if the key personale from the project understand and are involved directly with the project.

The simulation produced by this study to identify the effect of different strategies on the evolution of component systems must be validated to produce meaningful results,
and must be credible for these results to be trusted.

\subsection{7 Step Method}
This methodology has a 7 step approach to creating a valid and credible simulation.
These steps are; formulating the problem; collecting information and data to construct a conceptual model; validating the conceptual model;
implementing (programming) the model; validating the programmed model; designing, conducting and analysing experiments; and documenting and presenting the simulation results.

\subsubsection{Step 1: Formulate the problem}
The first step is to formulate the problem as clearly as possible, this is usually done with core stakeholders in a ``kick-off meeting''.
The core artifacts from this step are the overall objectives of the study, specific questions wanting to be answered, scope of the study,
 and different configurations of the simulation with the measures used to evaluate their performance. 

\subsubsection{Step 2: Collect information and Data to Construct Conceptual Model}
The conceptual model is a description of how the simulation and system work, relative to the problems earlier defined.
It is the most important artifact of the simulation, as it should be high level enough to be understood by the core stakeholders
and be reused in future simulations.
It is created through interviews with subject matter experts, collecting data like procedures and results from similar exiting systems, and other sources of relevant data.
Problems like the data not being representative of the model, not being in the appropriate format or type, and containing errors must be handled before use.

The conceptual model also contains all of the parameters including the documented assumptions. 
It is defined to the level of detail with respect to project objectives, performance measures, data availability, computer constraints, and resource constraints.

\subsubsection{Step 3: Conceptual model Validation}
The conceptual model is the most important aspect of the simulation, thus its validation will be thorough.
The core method used to validate this model, is to discuss it with core-stakeholders and subject matter experts.
This provides feedback as to the direction of the conceptual model, ensuring that it will answer the questions posed in the study.

\subsubsection{Step 4: Implement the Models}
The implementation of the simulation models must also be executed and documented in a way that allows other to replicate and repeat the process.
The artifacts created during this process must be verified to work correctly, this can be accomplished through test-cases and debugging.

\subsubsection{Step 5: Validate Implementation}
There is no completely definitive approach to validating the simulation,
however, the most definitive test of a simulations validity is established by closely looking at the outputted results compared to that from an actual system \cite{Law2005}.

This is done through:
\begin{itemize}
  \item \textbf{Results validation: } a comparable system is used to create results and compared with the results from the simulation for validation.
  \item \textbf{Face Validation: } experts are given output of the simulation model and checked to see if it is consistent with how they percieve the system should operate.
\end{itemize}

Further validation of the implementation can be accomplished with sensitivity analysis,
which is performed on the simulation to find the factors with the greatest impact on the performance and results.

\subsubsection{Step 6: Design, Conduct and Analyse Experiments}
For each of the experiments that are run, decisions about the time given to them and the number of independent runs to return statistically significant results must be made.
At the end of this step, after the results have been analysed a decision must be made if additional experiments are to be run.
As either the results are inconclusive, or other aspects of interest have arisen.

\subsubsection{Step 7: Document and Present Results}
This step involves the presentation of the model to the core-stakeholders in a manner that describes the concept model, the simulation and the results.
This step is critical for the future re-use of the model, as is the detailed description of the validation process to promote credibility.


\subsection{Differences in methodology}
To produce a valid and credible simulation study is the goal of using this methodology.
However, this methodology has been created for industrial projects with of a larger scale; for instance the U.S.A. Department of Defence.
As this study is smaller in scale in time and effort that many other simulation projects, some of the procedures recommended in this method have been restricted and some removed.
The most significant difference to the methodology is the clear definition between decision-maker and simulation designer.
In this project these all refer to a single person, therefore meetings between these people are superfluous.
Other people in the project including subject-matter experts, core-stakeholders, simulation analysts consist of survey participants and
project supervisors, because the limits of the projects resources excludes employment of experts.
This may reduce the validity of the end model, but these restrictions have been made when only necessary,
and done so in a manner that attempts to minimise negative effects.


\section{Conceptual Model}
The methodology to create a simulation is used, and in this section we present the first artifact the ``conceptual model''.
This models includes the description of the sub-models, the user model, the resolver model, and the repository model.
This includes their variables and configuration in order to define the simulation.

The conceptual model also includes the processes that simulate the real life evolution of a component system.
These processes are used to generate user actions and then take these actions to be processed into CUDF problems
to be solved by a component dependency resolver with a strategies necessary criteria.

\subsection{Sub-Models}
The conceptual model is broken down into different models of reality, the user, the resolver, and the repository.
These three aspects have been chosen as they have the most impact on component system evolution.

The way in which the user interacts with the resolver, how the resolver finds the optimal solution, and the set of components that are used in order to do so
are represented and discussed here.


\subsubsection{User Model}
The user model involves many aspects, including the life-cycle of specific actions, the probability of actions being taken, and also the parameters of an action.
A user can typically execute many different types of actions, install, remove or upgrade a component, upgrade the entire system,
download a source for a component, change the status of a package, automatically remove package that are not necessary, or even format the system and start again.
In our abstraction of reality the user has two actions with the resolver, either to install a new package or to try upgrade the entire system.
These two action were selected as they represent the core interactions the user performs on the system.
The initial system that is 

%%%Update
Probably the most executed action by the user is the request to update the system.
This involves looking at the repository and trying to find newer versions of currently installed packages to replace the current versions.
This is done either because newer versions have increased functionality or have fixed bugs,
however can become quite complex as newer versions of component may conflict with other packages, therefore are not able to be installed.
The update action can happen at regular intervals as it is typically automated or part of a users routine.
These intervals can be quite quick, daily for many users, 
and they can be over an extended period of time, 
as with server administrators not wanting to change a potentially critical system.
In this abstraction, the core variable for a users update action is the cycle in which it occurs,
e.g. every 3 days the user updates.

%%%Install
Installation if the request a user performs to extend the functionality of the system they are using.
A user selects a package with the required functionality they desire, usually to complete a task,
then the resolver installs a system that allows that package to be used.
Determining the probability a user will select a component to be installed is difficult given the enormous amount of factors this relies on.
The users job, location, current tasks, previously installed software, favourite colour\ldots all may effect when and what package a user selects to install.
Given that all this information is impractical to simulate, we abstract this problem into the form of two questions;
what components may a user select to install and how likely would a user select to install these.
This is represented by a weighted list of components, where the weights represent popularity,
e.g. package ``A'' has a 10\% chance of being selected as the package to install, package ``B'' has a 5\% change \ldots

The next question is what is the probability that a user will install a package, and how many packages?
This is represented by a probability for the amount of packages that a user may install on a day,
e.g. they have a 80\% chance of installing nothing, a 15\% chance of installing one pace

%%%The initial system to start from
The initial system the user has installed before the component simulation of evolution commences can have a direct effect on the results.
Many systems will release different configurations of components that use the same repository, as this is a typical use of components to create systems with different use cases.
The Eclipse framework offers more than ten types of initial Eclipse installs for different users.
The Ubuntu system offers at least three different (server, desktop, alternative) each for either amd64 or i386 chipsets.
The selection of the initial system can depend on many aspects of the simulation, 
though selecting the most popular installed system may result in broader results. 

A user model is then represented by 4 pieces of information,
\begin{enumerate}
  \item  The cycle at which a user selects to update a system
  \item The probability a package will be selected to be installed
  \item The probability a user will select to install a package per day
  \item The initial system
\end{enumerate}


\subsubsection{Resolver Model}
The solver selected to resolve the package dependencies is an important aspect of this simulation.
Throughout this thesis we have discussed the definition, implementation and extension of a component dependency resolver.
The core aspect we have focused on is the variable criteria that can be used to find different optimal solutions given the requirements of a system.
Given the possible actions a user can take in this user model, update or install, each have different objectives in the system,
they also require different criteria to optimise for.
While the update action tries to upgrade the entire system, 
the install action may require the newest version of a package to be installed but would typically not update the entire system to do it. 

The solver model then contains this information:
\begin{enumerate}
  \item The criteria to optimise for when the user updates the system
  \item The criteria to optimise for when the user installs a package
\end{enumerate}

\subsubsection{Repository Model}
The repository model is an abstraction of the server which contains and distributes the packages to the client to be installed.
The repository is a key component in the study of component system evolution, 
as the user actions, of update and install, are dependent on the repository and the solver model must consider all packages in the repository when finding the optimum system.
This model is the most complex of the three, however it can be simplified by removing the distribution concern.
It is a very difficult model to create and is recommended that real data be used,
and assumptions also have to be made to maintain the consistency of this model.

%%%It is very complex and we need daily data
A repository of components contains all of the information of the component interaction, and component versions.
What is in the repository at any given time is then dependent on the contained components life cycles,
as each of these is a complex entity in itself, the state of the entire repository is difficult to predict.
This complexity that is created over time in the repository is an aspect that clearly effects the evolution of a component system.
Therefore, this simulation takes into account the date, by identifying the set of components that exit in the repository on a given date.

%%%Distrubution details are ignored
As the distribution, and aspects like the protocol or network used to transfer the packages, are not core issues of component system evolution,
they will not be simulated.
This is then the assumption that the repository is always accessible by the solver with no faults at all times during the simulation.
This is clearly not the case in reality, as servers and connections can be intermittent,
however, removing this aspect greatly reduces the complexity of the model without a negative impact on accuracy. 

%%%This accuracy is difficult to create
A repository built through using algorithm would be difficult to validate as being an accurate representation of an actual repository.
The amount of factors that would have to be considered to define an accurate representation, the algorithm to build such a repository,
and the method in which you can validate it's accuracy,
are all difficult problems, and outside the scope of this study. 
Therefore, in this simulation it is recommended that a data from a real repository is used.
This in itself is a difficult task to accomplish, especially as daily information on a repositories state means that accurate records have to of been kept on a artifact that is constantly in flux.

%%%The range of time in which to look at
The range of time over which the simulation is run will determine the repositories that are required.
It is also important as it should be long enough to draw conclusions from,
but as the simulation can take considerable resources to execute, too long and it may make it impractical to execute all the iterations necessary.
Other external aspects such as policy changes in the way in which a repository is run, or release cycles of the component system, 
may have an effect on the internal properties of the repository.
This must also be considered when defining the time range, if not excluded then understood when analysing results.
Selecting of the initial system also relies on the time frame that was selected,
as all the components in the system must be ensured to also be in the repository.

%%%You can never remove a component from the repository
This model has two core assumptions that must be mentioned; a repository may never remove a package, and a package version is static and may not change.
This is clearly not a constraint that exist in reality as when releasing a newer version, many repositories (e.g. Eclipse) will remove out dated packages,
in some cases the repository will only contain the most uptodate packages.
Although rare, the changing of a package version may occur in some repositories, here though it is a strict constraint.
These assumptions are to ensure the consistency of the repository and it's packags to enable their use by the solver, 
as the repository in this simulation stores all information about the packages.
Having all packages are always in the repository ensures a case where a package is in a system but not in the repository is impossible,
and making sure a package version never changes ensures that information between the system and repository is synchonised.  

%%%this model contains a record of packages in repository over time
This model then contains one set of information:
\begin{enumerate}
  \item A daily record of package versions and their dependencies stored in a repository
  \item A time frame, start and finish, over which the simulation is run
\end{enumerate}

\subsubsection{Configuration}
%%%A configuration is the set of parameters of the simulation. What are these parameters which we must consider.
A configuration is the set of variables which define the parameters of the simulation.
How these variables are defined are derived from what goals of the specific simulation.
What the invariants are and how the variables are altered depends on the questions being asked, and the approach to answering them.

In this conceptual model there have been defined 8 different dimensions to alter this simulation by:
\begin{enumerate}
  \item The cycle at which a user selects to update a system
  \item The probability a package will be selected to be installed
  \item The probability a user will select to install a package per day
  \item The initial system
  \item The criteria to optimise for when the user updates the system
  \item The criteria to optimise for when the user installs a package
  \item A daily record of package versions and their dependencies stored in a repository
  \item A time frame, start and finish, over which the simulation is run
\end{enumerate}

These variables can be altered in many different ways to answer important and relevant questions to do with component evolution.


\subsection{Processes}
These processes are attempts to abstract and simplify the reality of the component system evolution.
The process that simulates the evolution of a component system takes a configuration and first generates a set of actions a pseudo user may take.
These user actions are then iteratively applied to the initial system though generating CUDF problems that are solved using the resolver given its criteria.
The results of this process is then a set of systems that are created through the user actions.
 
The results from the execution of this process will be different along the dimensions of the configuration.
Each configuration may create a different set of user actions, and using different criteria or repositories, will create different systems.

\subsection{Input Configuration}
The simulation configuration is a set of variables in which we can alter the simulation through.
To treat these variables as a set of inputs, their format and constraints must be defined.

The format for the initial system and criteria have been defined in previously chapters.
The initial system and the repository information will be store in the CUDF format, as described in chapter \ref{background}. 
The criteria to update and install will be defined in our modified MANCOOSI format defined in chapter \ref{criteria}.

Some inputs are trivially defined;
the update cycle is an integer representing days between updates;
the time frame is defined as the dates of the days (given in seconds since the epoch at Jan. 1st 1970) between a given start date and the number of days that the simulation should run for.

These final inputs, the probability distributions of whether a package will be selected to be installed and how many packages a user will select to install on a given day,
are represented using a set of pairs, each pair containing the action and the probability the action will be taken.

These variables are formally defined as:
\begin{itemize}
  \item the initial system as $IS$ which is a CUDF object.
  \item the update criteria $UCRIT$ and install criteria $ICRIT$ as strings in the format of given in chapter \ref{criteria}
  \item the update cycle as $UC$ as an integer
  \item the dates between the start date and the start date plus the number of days as $DATES$ is a list of integers of seconds since the epoch.
  \item the repository function $R(d)$ which returns a CUDF object for the given date $d$
  \item the probability a package will be selected to be installed is a set of pairs $WP \in C \times \mathbb{R}$ which maps a package to a real number between $0$ and $1$ 
  and whose sum for all real values is 1, i.e. $\sum_{\forall p,r \in P}r = 1$,
  e.g. a set $WP = \{ (a,.8) , (b,.2)\}$ means there is an 80\% change a will be slected and a 20\% change b will be selected
  \item the probability a user will install a package per day is represented as a set of pairs of non negative integers to real numbers $UI \in \mathbb{N} \times \mathbb{R}$,
  integer is the number of packages and real number is the probability that amount of packages will be installed, 
  e.g.  a set $UI = \{ (0,.8) , (1,.15), (2,.5)\}$ means the user has a 80\% chance of installing 0 packages, a 15\% chance of installing 1 package and a
  5\% chance of installing 2 packages.
\end{itemize} 

\subsection{Algorithms}
The algorithms of this simulation are executed in two stages; 
first ``user actions" are generated, these are an aggregation of some configuration information into a set of actions the user will take over the specified time period.
These users actions are then combined with the remaining configuration and processed by iteratively applying them to an initial system by producing a CUDF problem,
then passing it to the resolver with the appropriate criteria to find the resulting solution.

\subsubsection{User Actions}
%%%Generate user scripts
A user action is defined as a tuple with four values,
\begin{enumerate}
  \item The time an action is taken in seconds since the epoch
  \item The list of packages to install which can be empty
  \item The list of package that have been previously installed (the keep list)
  \item A Boolean value representing whether to update or not
\end{enumerate}

User actions are organised into a list ordered by the time the action occurs.
This is generated through the algorithm presented as pseudo code in figure \ref{generateuser}

\begin{figure}[htp]
\begin{center}
\begin{alltt}
\textbf{INPUTS}:
DATES
UI
WP
UC

\textbf{OUTPUTS}
UserActions

function generateUserActions: 
    UserActions = []
    keeps = []
    days = 0
    for d in DATES:
        UserAction = (0, [], [], false) 
        //the date(0), install(1), keep(2) and update(3) tuple
        
        //What is the date
        UserAction[0] = d
        
        //What packages to keep
        if keeps.length > 0:
            UserAction[2] = keeps 
            
        //What packages to install
        installs = []
        for i = 1 to randomSelect(UI):
            //If 0 is selected then this is never
            package = randomSelect(WP)
            WP.remove(package)
            installs.append(package)
            keeps.append(package)
            
        if installs.length > 0:
            UserAction[1] = installs 
        
        //Update or not
        if days mod UC == 0:
            UserAction[2] = true
        days++
        UserActions.append(UserAction)
        
    return UserActions
\end{alltt}
\caption[generateUser script]{Pseudo code of the generate user script which}
\label{generateuser}
\end{center}
\end{figure}



%%%Initialize the algorithm
The \verb+generateUserActions+ algorithm first initializes three variables, \verb+UserActions+,\verb+keeps+ and \verb+days+.
The \verb+UserActions+ variable is the list of user actions of the user, this is returned at the end of the algorithm.

The variable \verb+keeps+ is a list, initialised to empty, that contains a list of all the packages the user has previously selected to install.
This list is later translated into a ``keep: package'' property of the installed package with defined semantics in the CUDF specification.
This ensures that after a package has been selected to be installed by a user a package is not later removed by an action, e.g. an update.

The \verb+days+ variable keeps a count of the days in order to maintain the update cycle. 

%%%Describe the Installation and keep
For each iteration, the keep list is first assigned, then the packages to install are assigned, finally to update or not to update is decided.

The keep list is first assigned as it is essentially a list of previously selected packages to install, and does not contain the list of packages currently selected to be installed.

The code which builds the list of packages that are to be installed, is initialized by first randomly selecting a value from the $UI$ set for the number of components to be installed.
If the length selected is 0 then this loop will not execute, and \verb+installs+ will remain empty.
If 1 or greater is selected, then that amount of randomly selected packages is added to the \verb+installs+ list by randomly selecting them from the $WP$ set.
Once a package has been found it is removed from the $WP$ set to stop it being selected again.
The selected packages are also added to the \verb+keeps+ list to ensure that in future it is kept in the system.
 
The function \verb+randomSelect+ requires definition and discussion.
This function takes a set of pairs, where the second item in the pair is the probability that item will be selected,
and randomly selects and returns an item given its probability.
As both the probability a package is selected and the probability of the amount of package a user will install are defined in the same way this function is reusable.
In the instance where the package is selected, to ensure that the same package is not selected repeatedly for install it is removed from the list of possible packages
with the operation \verb+WP.remove+.
This function then must recalculate the distribution so that the sum of all probabilities equals 1 again.

%%%Describe the Update
The update action is included if the day modulus the update cycle $UC$ is equal to 0.
This makes the update action occur at regular intervals during the time period, as if it was scripted or part of a user routine.

%%%Describe the output
The output of this algorithm is then the list of \verb+UserActions+, this can be used to simulate what a user will do to their system.

\subsubsection{Execution}
%%%Run the simulation
The simulation execution takes the initial system defined in the configuration, and iteratively executes the user actions previously created, to create a timeline of systems.
Each iteration is broken into two parts, the update and the installation, as these are the core actions the user can take and have a different set of criteria to optimise for.
This iteration is described in the figure \ref{executeSimulation}.

\begin{figure}[htp]
\begin{center}
\begin{alltt}
\textbf{INPUT}:
UserActions
IS
UCRIT
ICRIT
R

function runSimulation:
    system = IS
    for time,installs,keeps,update in UserActions:
        if update:
            cudfFile = generateCUDF([],keeps,true,R(time),system)        
            outputCUDF  = time + "-update.cudf"
            executeSolver(cudfFile, outputCUDF, UCRIT)
            if not FAIL(outputCUDF):
                system = outputCUDF
        if installs.length > 0:
            cudfFile = generateCUDF(installs,keeps,false,R(time),system)        
            outputCUDF  = time + ".cudf"
            executeSolver(cudfFile, outputCUDF, ICRIT)
            if not FAIL(outputCUDF):
                system = outputCUDF
            
\end{alltt}
\caption[Execute Simulation]{Pseudo code describing the run simulation algorithm}
\label{executeSimulation}
\end{center}
\end{figure}

%%%Undefined function generateCUDF, executeSolver and FAIL
There are three additional undefined functions in this script, \verb+FAIL+, \verb+executeSolver+ and \verb+generateCUDF+.

%%%Describing the FAIL function
The \verb+FAIL+ function takes a location of a CUDF file and returns true if it is incorrect and false otherwise.
This is used in this situation to check if the solver has produced an output to a user action.
If it has not produced a correct output then the previous system is used, 
this is similar to how a typical system will act if an action is unable to be accomplished it will be rolled back to the previous version.
It also allows the continuing simulation without disruption, instead of stopping as soon as a user action is unable to be executed.
This check does not check semantically that the CUDF file conforms to the constraints necessary, only that it has an output syntactically correct.

%%%Describing the execute solver function
The \verb+executeSolver+ function calls the resolver to be executed on \verb+cudfFile+ and the output to be written to \verb+outputCUDF+, 
returning a solution that is optimised for the provided criteria (\verb+UCRIT+ or \verb+ICRIT+).
This function is where all the results are generated for the simulation, it creates four files;
the \verb+outputCUDF+ file, an error and log report, a save of the standard out, and file that contains the time and average memory usage of the process.
These files can be used to solve problems and fix bugs, or analyse the results of the simulation.

%%%Describing the generate CUDF function
The \verb+generateCUDF+ function is used to create an executable CUDF problem given a user action, a system, and a repository.
It does this by creating a new CUDF file with all of the components from the repository included, 
and changing the status of the components that are already installed in the system to be installed in the new file.
It also changes the keep status of the components selected in the keeps list to \verb+package+, this will ensure that a version of this package is in the resulting system.
It also generates the request for the CUDF from the selected packages to install in the installs, 
and from the Boolean of whether to update or not.
The algorithm to do this is presented in the figure \ref{generateCUDF}.

\begin{figure}[htp]
\begin{center}
\begin{alltt}
\textbf{INPUT}:
installs
keeps
update
repository
system


function generateCUDF:
    newCUDF = new CUDF()
    for component in repository:
        if component in system:
            component.installed = true
            if component.name in keeps:
                component.keep = ``package''
        newCUDF.add(component)
    
    request = ``''
    if update:
        request += ``upgrade: *'' + endline
        
    if installs.length > 0:
        request += ``install: ''  + installs + endline
    
\end{alltt}
\caption[Generate CUDF Pseudo Code]{Pseudo code describing the generate CUDF algorithm}
\label{generateCUDF}
\end{center}
\end{figure}


\subsubsection{The Output}
The ultimate output of this simulation will be a series of CUDF files that represent the system for each date over the time period.
These files are accompanied with logs, that describe errors and the time it took to complete the operation of calculating that system.
The CUDF files can then be processed to find the necessary data in order to answer questions about the simulation.

\section{Simulation Validation}
%%%Validation of this simulation, what needs to be validated/why it should be validated
The conceptual model presented is a simplified abstraction of the reality in which users evolve component systems.
It describes the variables that effect component system evolution presented a configuration,
and the processors used to execute the simulation given a configuration.
If some significant aspect of the system was missed, or if some aspect was incorrectly defined, the simulation may produce results that are incomprehensible,
or worse, misunderstood.
Therefore, the validation of these artifacts is essential to move forward. 
This validation was accomplished though regular stakeholder meetings, and an online survey with subject matter experts.

\subsection{Stakeholder Meetings}
%%%Weekly meetings with stakeholders (i.e. supervisors)
As described in the methodology, one validation of these artifacts is done through meetings and a structured walk-through with the core stakeholders.
In this simulation the core stakeholders are the project researcher and supervisors.
These are the people who are asking the question and are impacted by the outcome, therefore directly effected by the validity of the results.
Meetings where held at regular intervals to ensure the projects progress and direction where correct.

\subsection{Subject Matter Expert Survey}
%%%The survey used to validate and refine the model
An online survey was also completed, further described in chapter \ref{ubunutsimulation} with questions and results presented in appendix \ref{survey}.
It was conducted at a point in the project when the conceptual model was just being developed, so had considerable impact on these artifacts.
This survey was completed by over 55 subject matter experts, who answered questions that assigned values to the configuration variables.

%%%Results from what else should be asked, install stuff not from repository, installs break
The two most important questions asked in this survey for the validation of this conceptual model are 
``What frequency are your typical interactions?'' and ``What other questions should have been asked?''.
The first question allowed us to gauge the necessary user actions and how they are represented,
and the second allowed the expert to input what they thought was missing from the survey, and what should be asked when discussing this problem.

%%%This showed that people interact mostly at regular intervals as an update 
What frequency the user interacts with the resolver to evolve their system is important to find the most significant actions to include in the simulation.
Due to this interaction, the user action to remove packages was ignored, as it seemed many of the users do not use this frequently.
When they do select to remove a package it is often part of a more complex action, which includes installing a package and then rejecting its installation.

This survey clearly shows that users update their systems at regular intervals, and install packages frequently as well.
This was used to select these two actions as the most important to component system evolution. 

%%%Install stuff not from the repository
What aspects of the survey these experts thought was missing fell into mostly two groups,
installing packages that are not in the core repository and what happens if a system breaks during a change.

A few experts noted that they sometimes install obscure or untested components that are not included in the core repository.
These components may change the systems structure through having dependencies on, or be depended on by components in the main repository.
This is not a borderline case, as many component systems, including Ubuntu and Eclipse, use the idea of multiple repositories that users ``subscribe'' to in order to find and get packages.
As an abstraction, this model has selected to only have one repository that all users can access components from.
This is because the management of multiple repositories was seen as outside the scope of the problem.
Also, aggregating the most popular repositories into a single set was significantly more simple, and could be done without losing much information.  

If multiple repositories where to be included in the simulation, where each user can access a different a subset of repositories, necessary alterations to the model could be made.
These would include an extra step being added to the ``run simulation'' script, where the all the repositories at a given date are selected, and to the generate CUDF script where
it aggregates these repositories into a single CUDF file for the problem to be solved.

\subsection{Component Fault Exploration}
Another aspect of this problem that was mentioned is that a system may break during a change, requiring a reversion to the previous stable system state.
Simulating this effect was ultimately deemed outside the scope of the project as it is seen as a rare occurrence with modern systems.
However, before it was eliminated, it was explored for possible inclusion.

The core problem with including simulated faults in a component systems evolution, is that each component has a different likelihood of causing a fault.
Different properties of a component like development process or complexity can impact this value.
Therefore, the function to calculate the likelihood of a component failing could rely on many different properties.

Instead of creating a function that tried to calculate the probability of failure per component,
an effort to measure it was attempted through a small study of component bug reports was attempted.

The feedback generated when a component causes a fault should ideally be a bug report, for the Ubuntu distribution these are filed on the project hosting server LaunchPad.
By using the Launchpad API to extract bug information, we where able to count the amount of bugs per package in the system.
It was initially assumed that the number of systems that a component was installed on would increase the number of bug reports generated,
as more users means more eyes and systems to find bugs.
The Ubuntu Debian popularity contest was used then to  
Therefore to see if this relationship existed the graph in figure \ref{bugsvspop} was created.

\begin{figure}[htp]
\begin{center}
  \includegraphics[width=\textwidth]{simulationpics/bugsvspopularity}
  \caption[Bugs v.s. Popularity]{A plot of the bugs a package has compared to its popularity, with notable outliers labeled}
  \label{bugsvspop}
\end{center}
\end{figure}

The first thing to note is that there seems to be very little relationship between the two variables, except the package with the most bug reports is also one of the most used components.
The second thing to note is that the packages with the largest amount of bugs are ``apt'' and ``aptitude'' the two most popular package manages.
This could be because those packages are very buggy, or it could be because problems caused by apt, e.g. trying and failing to install a faulty package, may be reported as a problem with apt. 
The last thing to notice is that there are many less popular components that have many bug reports.
When identifying the purposes of these packages many are used by developers, e.g. emacs21 is a popular text editor to develop in.
The reason for their increased amount of bug reports may be that the users have prior experience and appreciation for the bug reports and the maintenance process, so file more bugs.

Measuring a components likely hood of failure using bug reports is then hypothesised to be not possible,
as a component purpose and a components users may affect the results.
Other methods of finding a components likelihood of failure have not been further explored since this variable was eliminated from the configuration.
Though, we expect this is an intractable problem that is likely impossible to simulate accurately.

\subsection{Further Validation}
The assignment of the configuration variables is a different stage in the validation of this simulation.
Clearly if you create a configuration that is completely unrealistic, the saying ``garbage in, garbage out'' applies to the results.
However, this is not a concern when validating the conceptual model, or the abstract processes.
Further discussion of the validation of the assignment of the configuration variables is in chapter \ref{ubunutsimulation}.

\section{Summary}
{}This chapter has described the abstract definition of the proposed simulation used to study component system evolution.
{}The methodology from Law \cite{Law2005} was used to guide us to a valid and credible simulation.
{}From this methodology we defined a conceptual model of the user, repository and solver and described the processes used to simulate a systems evolution over a time period.
{}It was validated through regular meetings with the core stakeholders, and a survey conducted on subject matter experts.
{}This simulation can, and will, be used to draw meaningful conclusions on component system evolution.
