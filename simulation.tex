\chapter{Simulation}
\label{simulation}
{}To evaluate different strategies when evolving a component systems, the environment in which these evolutions occur is simulated.
{}Through modelling then simulating the different aspects of a system; the user-interaction, the repository, and the resolver;
{}conclusions can be drawn on the benefits and draw backs of a particular evolution strategy.

{}The motivations for simulating and the methodology used to create the simulation are presented in this chapter.
{}The result of this chapter is a simulation that can be used to predict how a component system will evolve given different variables of the user and the strategy they use.

\section{Why Simulate?}
%%%Why are we using a simulation instead of real systems, or a controlled environment
To evaluate an evolution strategy some options include;
using a set of actual systems with real users could be accessed to collect field data;
creating a controlled environment in which users are given tasks and monitor the outcomes;
simulating a systems evolution through creating models and altering environment variables to study their effects.
In this section we discuss the positive and negative aspects of each of these approaches,
and present the reasons for the selection of the simulation methodology for this research.

%%%The pros and cons of using real systems with real users
Using real systems with real users would clearly produce real and valid data to be studied and analysed to find the best evolution strategies.
Solvers could be developed and distributed to different users, each solver using different criteria 
and as each user has different behaviours (amount they install, when they update)
these results could be analysed to find strategies that produce good systems. 

%%%Now the cons
This approach will create very accurate research, however it has some significant drawbacks,
it will take significant periods of time to create meaningful results, for each day of results you have to wait a day.
Finding the way a particular strategy effects a system over an entire year, will mean waiting a year.
More significantly, finding users that will trust an experimental system to alter their real system would be extremely difficult.   
A user will likely not trust a newly released resolver, as their system is important to them and even the slightest error can cause
a system to become unstable.

%%%Users trust is difficult to gain
To gain the trust of possible users the experimental resolvers would have to be thoroughly tested through a repositories development cycle and be well maintained.
Moving a package through this cycle can be itself a massive undertaking lasting several months. 
For example Eclipse Plug-ins it involves going a component going through four phases 
Proposal, Incubation, Mature, and Top-Level\footnote{http://www.eclipse.org/projects/dev\_process/development\_process\_2010.php};
similar to Debian's process of moving through the phases Unstable, Testing, Frozen, and finally stable\footnote{http://en.wikipedia.org/wiki/File:Debian-package-cycl.svg}.
After it has been through this cycle, maintenance of the resolver is still required; 
for instance the Ubuntu package ``apt'' has released about 2 changes a month\footnote{http://changelogs.ubuntu.com/} to it or related packages since its initial release.
This shows how much effort must be applied to earn a users trust, and this effort will likely outweigh the actual benefits.

%%%The pros and cons of using real users with pseduo systems
A study of users interacting with a system in a monitored and controlled environment could be executed.
Each user could be given different strategies, e.g. solver criteria, and different tasks to perform on their system to approximate real interactions.
This study could compress a years worth of interactions with a system into an hour, and produce results from real users. 
Compared to the previous type of study using real users with real systems,
this research removes the necessity of user trust as the user is altering a system that is not theirs,
and results can be gained in hours not years. 
However, it will also produce less valid results, as it is still an approximation of a real system, and the users know this.
Any controlled environment would alter how the user interacts with their controlled system, making the results less reliable.
For example, a user typically researches a package before selecting installing it because it would have real implications, 
in a controlled environment these implications are removed so the selection would still be of questionable validity.

The largest problem with this method is again finding enough users.
With the previous method it was trust that would limit the user count, here it is the effort to find and monitor the amount of necessary users required.
For example, given we want to test 10 different strategies with 5 different user types, each combination must contain multiple different test subjects to find statistically significant results, 
and each subject taking upwards of an hour to complete the study. 
This would be a significant effort to organise users, monitor interactions and collect results, so much so that that it may be logistically impractical when compared to the benefits.

%%%Instead we can simulate, with an approximation of the real world.\\ 
%%%When drawing conclusions the accuracy of this approximation must be considered, therefore significant effort has gone into data collection and validation.
A simulation is a surrogate of the real system, such that it represents the core aspects of a component systems evolution.
This is accomplished through modelling the most important parts of the system, 
and computationally creating and measuring the results. 
As the simulation is only an approximation of the real world, 
the accuracy to which it actually represents the real world is not 100\%.
The goal is then to make it a ``close enough'' approximation so that the conclusions drawn from it are valid in the real world.
So when analysing the results and forming conclusions, we must take into account the assumptions we have made creating the simulation to determine 
the conclusions overall accuracy.
Therefore, the majority of the effort when creating a simulation is gathering and using valid information when making assumptions.

%%%What is good about the simulation over the other two methods, speed at which results are generated, the cost to get the results, the control over the variables in the simulation.
Simulation is superior to either using real users and real systems, or using a controlled environment in the speed at which hypotheses can be tested and evaluated,
the cost to test and get results, and the control over variables and configuration of the environment.

%%%Speed at which results are generated
As the speed increases at which a hypothesis can go from being an idea on paper to being tested in a real or simulated environment, increased the amount of meaningful information can be generated. 
When an idea is proposed as a possible solution to a problem, the quicker that idea can be eliminated or encouraged allows new ideas to be formed off of the original.
These ideas themselves can then be tested, to see if they are suitable solutions.
The faster this process is, the ideas can be tested, the better solutions and more information is generated.
Using real users, or real systems, requires long periods of waiting, where if the actions and environment can be modeled and executed computationally there is little waiting.
Through simulation an idea can be tested almost immediately by simultaneously evolving multiple simulated systems, where using a real user, 
it may take days or months to generate the same amount of results.  

%%%Cost
The cost of using any of these methods can be measured in the amount of time it would take in organising, measuring and wasted time, for the the results to be collected.
Using a real user and system would involve an enormous amount of time validating and distributing the solvers (as discussed earlier), 
and the controlled environment would require many hours of organisation and planning to gather the necessary users, and then measure their interactions with the system.
However, with a simulation the main effort is ensuring that the models are accurate enough to gather meaningful results.
This time in validating the simulation and associated models is only necessary once, 
i.e. if more results are needed the simulation will not need to be re-validated.
The other two methodologies do not have this property, as for each user tested and measured will be a duplicate effort. 

%%%Control
The control over the results that a simulation returns far exceeds the other methods as models can be altered and tested where a real user cannot. 
This control allows the testing of extreme situations, the sensitivity analysis of different variables,
and the generation of possibly optimal but 'out of the box' ideas. 
For instance, the testing of the amount of days a user waits between updates can be tested through altering the times between updates, then running the simulation.
If real users where used, the amount of users necessary to be collected would be enourmous to produce any significant results in this area.

%%%Final words on why we simulate
A simulation was used as it provides us with a cost/benefit ratio that is desirable, while potentially allowing an appropriate level of accuracy to draw meaningful conclusions.
The main effort is then to create the simulation with valid models that will give us the desired accuracy.

\section{Methodology}
%%%We use the methodology from `` Build a Valid and Credible Simulation''
{}To create a model that is credible and valid the methodology outlined by \cite{Law2005} in ``How to build valid and credible simulation models'' is followed.
{}This methodology walks through the steps from collecting information, creating and validating models, and running the simulation.
{}This method is used as it is applicable to the simulating component evolution, however some points must be changed to accommodate our limits, 
{}to create the necessary artifacts for its execution.  

%%%Why do we use this methodology and how is it relevant? Where else has this methodology been used?
This methodology is

%%%At what points do we differ from the method? Points that are outside the scope of this project.
This methodology can be used for many different scales of project, for instance for motivation the example of the U.S.A. Department of Defence using simulations is given.
As this project is smaller in scale in time and effort that many other simulation projects, some of the artifacts from this method have been restricted and some removed.
One such artifact is meetings with stake holders are reduced to a single survey in order to build our conceptual model of the problem.
This may reduce the validity of the end model, but these restrictions have been made when only necessary and done so in a manner that minimises the negative effects.

%%%What are the artifacts and their goals that are required from this methodology?

\section{Conceptual Model}
{}To create our conceptual model, we must first precisely describe the problem;
{}When evolving a component-based system, different strategies can be employed by the user and the resolver.
{}These strategies have unknown effects over a long period in which the user evolves the system.
{}Through simulating and analysing these effects we can choose a strategy more effectively.
{}There are two parts to these strategies, first how the user interacts with a resolver;
{}second, how the resolver handles these requests.

{}This simulation takes a particular strategy a user employs to evolve a system,
{}and simulates their decision over a period of time to and look at the intervening and resulting systems properties to find good strategies.

Creating a simulation that accurately represents the evolution of a component system so that we can 
analyse the effect different evolution strategies have on a systems properties is the core objective of this simulation.
To this end, we follow the a methodology outlined by, %TODO cite How to Build a Valid and Credible Simulation
This gives us a set of guidelines to follow when preparing a valid simulation including;
specifically defining the problem,
building a conceptual model,
collecting data,

\subsection{Survey analysis}
%%%The questions we want to answer are which actions are routinely performed and at what frequency?
%%%What life cycle specific actions are performed?

%%%Description of the Survey

%%%Results from the Survey

%%%Analysis of these results and how they apply to the user model


\subsection{Strategies}
%%%How do users use different strategies when evolving a system when installing and updating their systems
A user interacts with a resolver to evolve a system by installing, removing or updating packages.
The way in which these interactions occur is usually related to the purpose of the system, and what the type of user.
For instance, a server administrator is less likely to install a component into a system as the system has only one task and if it is working their is no need to change it, 
however a desktop user will more likely install different packages because they use their system for many different tasks.
How a user updates their system is also different along these lines, server admins. will likely only update when absolutely necessary as if it is not broken why fix it.
Yet desktop users will update more frequently as they want the best system they can have, and new features or better performance are usually wanted.

As described in previous chapters, dependency resolvers often try to minimise change and maximise the versions.
These two objectives have been implemented in many different ways, how these interact with each other is the strategy a resolver employs.
For instance, the trendy strategy from the Mancoosi organisation describes the strategy to minimise the removed components above all other criteria,
this is a common strategy because removal of a component is seen as very risky.

\subsection{Consequences}
%%%The core consequences of using a strategy that we are looking at come from the dependencies between the strategy and the resulting metrics of critiera 

%%%Essentially what we want is a table with the strategies laid out in the columns and with how they effect different criteria in rows.

%%%This will show the dependence between the different strategies and what their effects are

%%%We have three control strategies that do not require criteria or user models, one that never updates, another updates daily and updates weekly.

%%%After this we can add criteria with different relationships, and installation strategies and observe the effects.

\subsection{Configuration}
%%%A configuration is the set of parameters of the simulation. What are these parameters which we must consider.
A configuration is the set of variables which define the parameters of the simulation.
How these variables are defined are derived from what goals of the simulation.
As the goal of this simulation is to look at different resolver criteria and user strategies,
these are the core changes that are made.
Given that we want to look at the effects of many different possible ways a system can evolve,
we also change the packages that are selected to be installed by the user.
The variables like, what repositories are used, and over what time period are seen as not being relevant to our eventual analysis,
therefore we do not change them and leave them static.

Given this simulation is looking at the effects of the strategies employed by users and resolvers on the component systems,
we must look at different resolvers, and different user strategies.
What packages the simulated user selects to install and 

%%%The criteria for updating and installation we are testing
The most significant variable in the configuration is the criteria used by the solver to find optimum solutions. 
As there are two separate actions the user can make, updating and installing, each of these can have different criteria associated with them.
This is necessary because the optimum solution for these two actions should be different,
as a user installing will probably not want to update their entire system when trying to install a single package.

%%%We are changing the distributions of user installaion, and the period between updates
The user installs different amount of packages per day, and updates at different intervals. 

%%%We are selecting different packages to install
We are selecting the packages to install that are different.

%%%The repository over which it runs changes over time
Time, We have decided not to 


\section{User Model}
{}The user model involves many aspects, including the life-cycle of specific actions, the probability of actions being taken, and also the parameters of an action.
{}Given the three actions update, install and remove, how often are these actions executed, and what are they executed on.
{}To answer these questions we first look at the analysis of our survey to find the life-cycle of these values, we further refine these values by looking at the submitted user logs.

%%%The user can install a single package or update all packages. Removing a package or other complex actions are rarely used, so they are ignored in this model.
In our abstraction of reality the user has two actions with the resolver, either to install a new package or to try upgrade the entire system.  
In reality the user can do much more fine grained actions, where they can remove packages, 
upgrade individual packages or even create complex queries for the resolver to solve. 
These additional actions, along with being difficult to simulate, are rarely performed by a user, as found with our user survey. %TODO reference
How often a user installs a new package is found through looking at user logs, %TODO ref user logs
and how often a user updates a system is found through our user survey.
These pieces of information allow us to approximately represent real users.


\section{Repository Model}
{}The repository model models the server which serves the packages to the client to be installed.
{}Superfluous aspects of the repository, like the protocol used to transfer the packages, are ignored in this simulation.
{}The model of the repository is complicated by the fact that our simulation takes into account what packages the repository had on a particular date.

%%%We create a repository for each different day

%%%The Ubuntu repository is selected because it has significant amount of users and a significant amount of components.

%%%The initial date is selected as it is after a major release, and the time length is the median system age from the user survey

%%%Analysis of the repository, summary from 

%%%This repository model differs from reality in the fact that we use all components, not just those available in the meta repository (ubuntu manacured)


\subsection{Control User}
{}A user that never installs anything and never updates, has no dependence on any criteria so can be analysed independently.
{}Such a user has interesting consequences for the speed at which their system goes out of date, the change in values of heuristics such as PageRank, HITS and instability,
{}and the 

\subsection{Update Criteria}
{}In the previous chapter two version dependent criteria are presented and defined, one based on the Eclipse P2 concpet to minimise the ``uptodate distance'', 
{}and the second based on Mancoosi version to minimise the amount of packages that are notuptodate. 
{}The implications of using these criteria to update a system are discussed with relation to the order of the criteria, the average change in a system and the comparison of these values.

The order consequences of the order of the crtieria when updating are important given the semantics of the CUDF standard.
The key points when selecting to update a package in a CUDF standard is that there can only be one package and it must be of equal or greater version than the highest installed current version.
This means that if a package has only one installed package then it does not need to change to be valid in the returned solution.
Therefore, the returned system for any update command can be the initial system.
This means that if a criteria, such as changed, or hamming where minimised as the most important  


\section{Strategy Model}


\section{Simulation Validation}
%%%With the methodology there are many pionts at which we validate individual parts/models, and their overall composition.
%%%The main validation method is the inviewing of different stakeholders, this for our method was discussions with supervisors, at conferences, and throught the survey.


\section{Simulation implementation}
%%%Here we describe how our simulation is implemented, specifics that are not previously mentioned, and the variables not yet defined

%%%Preprocessing of the weighted components, removing all packages that are unable to be accomplished (e.g. skype)
%%%When createing the CUDF files it removes install requests that are not in the system (e.g. if chromium was added to the repository in 2010 and it is requestied in 2009, it is just ignored.)

%%%What happens when a install is required that is then not able to be accomplished.

%%%Scripts and such are in the appendix, some code is here though, particularly interesting or critical code.

\section{Conclusions}
%%%Discuss the overall conclusions. Bullet point the points most surprising, and useful for further study.

